<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Smallviller&#39;s Blog</title>
  
  <subtitle>记录点滴成长</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-06-30T07:19:57.107Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Smallviller</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>云服务器配置远程jupyter notebook环境</title>
    <link href="http://example.com/2021/06/30/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/"/>
    <id>http://example.com/2021/06/30/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/</id>
    <published>2021-06-29T17:31:06.698Z</published>
    <updated>2021-06-30T07:19:57.107Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：</p><ol type="1"><li>通过网址随时随地都能打开编程</li><li>配置了适合编程的主题色调</li><li>加入了插件补全功能</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624879339187-e21ea69c-5a13-4f9e-9a78-278e3a86edb6.png#height=365&amp;id=dR0gt&amp;" alt="image.png" /><br />前几天因为折腾自己的服务器环境给搞崩了，数据库出了点问题，所以只能重装系统，导致jupyter notebook又要重装一遍，然后几个月后服务器到期，估计又要重新配一遍环境，就索性写一篇教程，供自己日后和有需要的人参考。</p><h1 id="云服务器选购">云服务器选购</h1><p>首先需要选购一个云服务器，推荐腾讯云或者阿里云，有学生认证的话一年大概100左右，操作系统推荐是用目前主流的两个Linux发行版，ubuntu和cent OS，两个系统在一些安装软件的命令上会有小差异，我这里用的是ubuntu。</p><h1 id="安装anaconda">安装Anaconda</h1><p>在买好云服务器后，就通过ssh连接，就可以用命令行进行操作了，首先第一步是安装anaconda，先要下载anaconda的安装包，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure> 下载好后直接安装： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898431857-7b2a0ec6-2970-4c6f-9af3-07dc336f9a3d.png#height=72&amp;id=DlHXa&amp;" alt="image.png" /><br />会弹出这样一个界面，直接一直回车，然后输入yes继续回车，等待安装完成即可，安装完成会有这样一个界面，就代表安装完成了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898994446-2e991619-5f16-4b5f-bd61-3292b2fc53f7.png#height=126&amp;id=P7Dy3" alt="image.png" /></p><h1 id="配置jupyter-notebook环境">配置jupyter notebook环境</h1><p>接下来就可以配置jupyter notebook环境了，首先需要生成一个配置文件，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure> 因为服务器的安全性，配置远程访问是需要设置一个密码的，输入命令，生成密钥： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure> 输入两次密码，这里就会生成一个密钥放在用户文件夹的.jupyter文件夹下，和刚刚的配置文件路径一样，这两个文件会自动关联起来，在修改配置文件的时候就不需要加跟密钥相关的命令了。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624899427258-a0a09979-4f70-4935-9933-d058534df757.png#height=52&amp;id=i9wwI" alt="image.png" /><br />接下来就可以直接修改刚刚生成的那个配置文件了，使用vim打开，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure> 按键盘的i键进入编辑模式，直接在开头添加以下内容： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&#x27;*&#x27; # 代表任意ip是都可以访问jupyter</span><br><span class="line">c.NotebookApp.notebook_dir=&#x27;/home/ubuntu/jupyter&#x27; # notebook的工作目录，可以自己的实际情况修改，注意要确保目录存在</span><br><span class="line">c.NotebookApp.open_browser = False # 不打开浏览器</span><br><span class="line">c.NotebookApp.port =8888  #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure> 按Esc键退出编辑模式，然后输入:wq保存即可。</p><h1 id="开启远程访问">开启远程访问</h1><p>我们在上一步中指定了端口为8888，也让所有ip都能够访问这个端口了，但是在云服务器中还需要把这个端口开启起来，以腾讯云为例，进入安全组中，添加入站规则，按如下设置，然后在出站规则里点击一键放通，入站规则和出站规则都需要配置好<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624900683679-df0724ff-dcb9-4091-bf70-b9cb926120f2.png#height=205&amp;id=p7ggP" alt="image.png" /><br />接下来就可以将jupyter notebook打开了，不过我们需要能够将notebook一直在后台挂着，所以这里就输入这个命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure> 这里nohup（no hang up）是不挂起的意思，用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行，最后面的<strong>&amp;</strong>是让命令在后台执行，终端退出后命令仍旧执行，&gt; jupyter.log 2&gt;&amp;1是输出日志的意思，把命令的输出和错误都写到jupyter.log这个文件中，方便监控。<br />接下来我们在浏览器中输入：服务器公网ip:端口号，即可访问jupyter，如图所示，再输入刚刚设置的密码就行了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903214506-5212ba31-e39b-497e-bbb4-200892853bb9.png#height=390&amp;id=y6mOt" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903977925-09aa1daf-f1f3-40e2-ad21-8d1f42269d66.png#height=229&amp;id=SPHn7" alt="image.png" /></p><h1 id="装代码补全插件与更换主题">装代码补全插件与更换主题</h1><p>在上一步中，我们已经配置好了一个可以远程访问的jupyter notebook，但是呢，这个notebook的主题是默认的，白色太亮眼不适合编程，而且，默认的jupyter notebook也没有补全代码的功能，所以就通过插件的方式来解决这两个问题。</p><h2 id="补全代码插件">补全代码插件</h2><p>依次执行以下命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions </span><br><span class="line">jupyter contrib nbextension install --user</span><br><span class="line">pip install jupyter_nbextensions_configurator</span><br><span class="line">jupyter nbextensions_configurator enable --user</span><br></pre></td></tr></table></figure> 这样插件就装好了</p><h2 id="更换主题">更换主题</h2><p>首先安装jupyterthemes： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyterthemes</span><br></pre></td></tr></table></figure> jupyterthemes是一个为jupyter notebook设置主题的插件，可以在github上查看他们的使用手册，<br /> 这里推荐自己的一套配置方案，在命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jt -t chesterish -f roboto -fs 12 -ofs 105 -dfs 95 -lineh 150</span><br></pre></td></tr></table></figure><h2 id="重启jupyter-notebook">重启jupyter notebook</h2><p>如果你的notebook在运行中，就需要重启一下才能使得上一步的修改生效，首先找到运行jupyter notebook的进程id，然后杀掉这个进程，再重启就可以了 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ps -aux|grep jupyter</span><br><span class="line">sudo kill -9 进程id</span><br><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure> 可以发现，现在界面已经跟刚才的不一样了：<br /></p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904807402-493d550d-ae8c-4963-82bf-69a99fff310e.png#height=435&amp;id=gz8pT" alt="image.png" /><br /></p><p>然后还需要在Nbextensions中开启下Hinterland，也就是我们的补全插件<br /> <br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624384565647-dab6f490-7909-4501-a903-1cab72df72e0.png?x-oss-process=image%2Fresize%2Cw_1404#height=722&amp;id=AIyRA&amp;originHeight=722&amp;originWidth=1404&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1404" /></p><p>然后就大功告成了，有一个养眼的界面和补全代码的功能，就可以随时随地都能用搭建好的这个环境写一些代码了</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904949851-5191b2d7-4550-423d-b53c-71f609b93a10.png#height=404&amp;id=QiYzW" /></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：&lt;/p&gt;</summary>
    
    
    
    <category term="教程" scheme="http://example.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="jupyter notebook" scheme="http://example.com/tags/jupyter-notebook/"/>
    
    <category term="云服务器" scheme="http://example.com/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>GBDT梯度提升树算法原理</title>
    <link href="http://example.com/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"/>
    <id>http://example.com/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/</id>
    <published>2021-06-16T16:00:00.000Z</published>
    <updated>2021-06-30T07:32:11.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！</p><h1 id="gbdt核心思想">GBDT核心思想</h1><p>GBDT，全称为Gradient Boosting Decision Tree，顾名思义，分为两部分， Gradient Boosting和Decision Tree，具体来说，就是使用<strong>决策树</strong>为基学习器的<strong>梯度提升算法</strong>，因此分别了解下这两部分的原理就可以很好的学习GBDT的思想。</p><h2 id="decision-tree">Decision Tree</h2><p>先说决策树部分，由于在GBDT中，每次迭代都需要用到<strong>负梯度</strong>，求<strong>负梯度</strong>就需要用到连续值，因此GBDT用的决策树都是CART（classification and regression tree）回归树，而不是分类树，CART回归树一般使用的节点分裂准则为平均平方误差MSE，即在节点分裂时穷举每一个特征的每一个阈值，来寻找最优切分特征和最优切分点，衡量的方法是平方误差最小化，这部分算法在很多机器学习的书中都有涉及，这里不详细展开。</p><h2 id="gradient-boosting">Gradient Boosting</h2><p>在说Gradient Boosting之前，先讲下boosting算法，所谓的boosting，就是使用前向分布算法（Forward Stagewise Algorithm）来训练加法模型，这个加法模型表现如下：<br /><span class="math display">\[f_M(x)=\sum_{i=1}^{M}T_(x,\Theta)\\\]</span><br />这里<span class="math inline">\(f_M(x)\)</span>表示的就是经过M次迭代后最终得到的模型，而<span class="math inline">\(T_(x,\Theta)\)</span>代表的就是单颗CART回归树，这里的<span class="math inline">\(\Theta\)</span>代表树的参数，这个模型的意义就是每次训练一个弱学习器<span class="math inline">\(T(x,\Theta)\)</span>，经过M次之后给他们加总起来得到一个强的学习器<span class="math inline">\(f_M(x)\)</span>，那么怎么来训练这个加法模型呢？<br />我们的目标是使得最后的预测误差尽可能的小，但是预测集我们事先是不知道的，我们只能使得训练集的训练误差最小，那么如何使得这种加法模型的训练误差最小？有一个方法就是在每次训练模型的时候都拟合上一个模型预测值与真实值之间的差值<span class="math inline">\(y-\hat{y}\)</span>，也就是残差，然后在把这个模型加到原来的模型中，这样就可以使得更新后的模型的预测误差更小，这种方法也被称为前向分布算法，如果基学习器是线性回归的话就是前向分布回归，关于前向分布回归可以参考我上一篇文章：<a href="https://zhuanlan.zhihu.com/p/369732767">前向分步回归Forward Stagewise Regression原理及Python实现</a>，而这里我们用的基学习器是树模型，这种方法也被称为提升树（boosting tree），具体步骤如下：</p><ol type="1"><li>初始化<span class="math inline">\(f_0(x)=0\)</span></li><li>对于<span class="math inline">\(m=1,2,3...M\)</span><ol type="1"><li>计算残差：<span class="math inline">\(r_{mi}=y_i-f_{m-1}(x),i=1,2,3,..N\)</span></li><li>以残差<span class="math inline">\(r_{mi}\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></li><li>更新<span class="math inline">\(f_m(x)=f_{m-1}(x)+T_m(x,\Theta)\)</span></li></ol></li><li>经过M次迭代后得到最终的提升树模型：<span class="math inline">\(f_{M}(x)=\sum_{i=1}^{M}T(x,\Theta)\)</span></li></ol><p>提升树每次迭代都拟合上一次模型中所没有能够学习到的东西，也就是残差，这使得提升树是肯定比一般决策树要强的，但是呢，这种拟合残差的方式也会存在一定的缺陷，这个先留到后面说，因此，基于提升树的改进算法就诞生了，也就是GBDT。<br />从上述提升树算法中，我们可以发现，boosting的核心思想就是在每次训练模型的时候，都用新的模型来弥补之前模型的不足，使得模型越来越好，这实际上就是一个优化问题，提升树每次迭代的时候优化的就是残差，那有没有存在更好的优化方式呢？当然有，就是将这个模型的“不足”用损失函数的方式来表示，在每次迭代时优化这个损失函数就行了！<br />损失函数可以用<span class="math inline">\(L(y,f(x))\)</span>，这里先用一个通用的形式来表示，具体的损失函数可以是各种各样的，比如回归问题的MSE,MAE，分类问题的交叉熵，然后我们的目标就是使得这个损失函数最小，问题就来了，怎么优化这个损失函数使他变小呢？<br />根据凸优化的理论，梯度的方向是函数值增长最快的方向，那么负梯度的方向自然就是函数值下降最快的方向，所以根据负梯度就可以找到函数值最小的地方，那么，我们就可以通过拟合负梯度的形式来优化损失函数，也就是说，<strong>在每次训练一个新模型的时候，都拟合上一个模型的损失函数的负梯度值，然后把这个新模型加到原来的模型中，这样必然使得更新后的模型的损失函数更小</strong>，这个就是Gradient Boosting了，具体步骤如下：</p><ol type="1"><li>初始化：$f_0(x)=0 $</li><li>对于<span class="math inline">\(m=1,2,3...M\)</span><ol type="1"><li><p>计算负梯度：<span class="math display">\[-g_m(x_i)=-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\\\]</span></p></li><li><p>以负梯度<span class="math inline">\(-g_m(x_i)\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></p></li><li><p>更新<span class="math display">\[f_m(x)=f_{m-1}(x)+\rho T_m(x,\Theta)\]</span></p></li></ol></li><li>经过M次迭代后得到最终的提升树模型：<span class="math display">\[f_{M}(x)=\sum_{i=1}^{M}\rho T(x,\Theta)\]</span></li></ol><p>注意到这里对于每次迭代的决策树都加了一个系数<span class="math inline">\(\rho\)</span>，这个<span class="math inline">\(\rho\)</span>也被称为学习率（learning rate），为什么要加这个学习率呢？是因为如果将学习率设为1，就等于一下子学习全部的不足，会导致过拟合。据说在调参的时候，可以通过设置较小的学习率和较大的迭代次数以得到一个较优的模型，当然，这是以计算效率为代价的。<br /></p><h2 id="gbdt为什么使用负梯度而不是残差">GBDT为什么使用负梯度而不是残差？</h2><p>好了，现在可以说说为什么GBDT要使用负梯度而不是直接使用残差了，这也是我之前比较困惑的点，原因就是残差只是损失函数负梯度的一种特例，而采用负梯度更加通用，可以使用不同的损失函数。<br />为什么说残差只是一种特例呢？我们考虑下面这个损失函数：<br /><span class="math display">\[L(y,f(x))=\frac{(y-f(x))^2}{2}\\\]</span><br />这也就是平均平方误差MSE，然后他的负梯度就是：<br /><span class="math display">\[-[\frac{\partial L(y,f(x))}{\partial f(x)}]=y-f(x)\\\]</span><br />这不就是上面说的残差吗？也就是说，残差就等价于当损失函数采用MSE时的负梯度，而损失函数采用MSE其实会有一些缺点，比如对异常值敏感，所以在实际问题中我们有可能采用MAE或者更为折中的Huber损失函数以避免这一缺陷，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917120462-f7060d81-1511-4423-b70b-7d647b78711d.png#align=left&amp;display=inline&amp;height=368&amp;margin=%5Bobject%20Object%5D&amp;originHeight=490&amp;originWidth=728&amp;size=0&amp;status=done&amp;style=none&amp;width=546" /><br />并且呢，这些损失函数的负梯度其实也可以看作是残差的一个近似，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917260219-846072de-d101-46ff-9674-9907085d1ff0.png#align=left&amp;display=inline&amp;height=286" alt="image.png" /><br />而且，由于决策树容易过拟合的特点，我们通常也会采用一些正则化的手段控制模型的复杂度来改进损失函数，如控制树的深度等<br /><span class="math display">\[obj=L(y,f(x))+\Omega(\Theta)\\\]</span><br />这里的<span class="math inline">\(\Omega(\Theta)\)</span>是正则项，用来控制模型的复杂度，使得模型在保持良好的性能的同时尽可能简单，以防止过拟合，这也是xgboost相对于GBDT的优化手段之一。<br />综上，由于负梯度可以应付各种各样千奇百怪的损失函数，所以GBDT用负梯度来拟合明显相比于残差是有着巨大的优势的。</p><h1 id="总结">总结</h1><p>本文主要介绍了GBDT的核心思想，先引入基于加法模型和前向分布算法的提升树模型，提升树模型每次迭代的拟合的是残差，然后介绍了GBDT，GBDT每次迭代拟合的是负梯度，通过拟合负梯度使得每次迭代后的损失函数值都能下降最快，最后说明了为什么负梯度与残差的关联以及为什么拟合负梯度更好。</p><h1 id="参考">参考</h1><ol type="1"><li>《The Elements of Statistical Learning Data Mining,Inference,and Prediction》</li><li><a href="https://mp.weixin.qq.com/s/9SrciKW-nJJMA2BbTs5CLg">GBDT算法原理以及代码实现</a></li><li><a href="https://blog.csdn.net/shine19930820/article/details/65633436">『机器学习笔记 』GBDT原理-Gradient Boosting Decision Tree</a></li><li><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting</a></li><li><a href="https://www.zhihu.com/question/63560633">gbdt的残差为什么用负梯度代替？</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="GBDT" scheme="http://example.com/tags/GBDT/"/>
    
    <category term="boosting" scheme="http://example.com/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>Ridge和Lasso回归与代码实践</title>
    <link href="http://example.com/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/"/>
    <id>http://example.com/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-05-22T16:00:00.000Z</published>
    <updated>2021-06-30T07:06:23.968Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。</p><h1 id="ridge-lasso回归">Ridge &amp; Lasso回归</h1><p>在传统的线性回归中，是使用最小二乘法来估计参数的，通过最小化残差平方和来估计参数的，这个在机器学习领域也被称为损失函数：<br /><span class="math display">\[LOSS_{ols}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2\\\]</span><br />这种最小二乘估计的方法被证明了具有最佳线性无偏估计（Best Linear Unbias Estimator, BLUE）的性质，所谓的最佳，就是方差最小，但这是在线性无偏估计的前提下，在有偏的情况下方差就不一定是最小了，设想一下，如果牺牲这个有偏的性质来使得方差变小呢，根据bias-variance trade-off，会不会有可能使得整体的预测误差进一步降低呢？<br />于是Ridge和Lasso的形式就被提出来了，通过牺牲传统ols回归中无偏的性质来使得方差降低，以寻求更低的预测误差，这两者的损失函数分别如下：<br /><span class="math display">\[LOSS_{Ridge}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}\beta_j^2\\\]</span><br /><span class="math display">\[LOSS_{Lasso}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|\\\]</span><br />可以发现，这两个损失函数呢，就是在原来的ols的损失函数上加了一个系数惩罚项，因为我们求解时是让损失函数最小，加了后面这个惩罚项呢，会使得系数变小，这个<span class="math inline">\(\lambda\)</span>就用来控制惩罚的力度，如果为0的话就和传统的线性回归没有差异了，如果是无穷大的话，那么所有的回归系数都会被弄到0，最后的所有的预测结果就是样本的均值了，但在实践中，我们可以通过交叉验证的方式调节<span class="math inline">\(\lambda\)</span>的大小，选取最优的惩罚力度，就可以使得最终的预测误差达到最小。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621696524735-840f5da7-5fe4-4149-80a7-13ef9c3cbec6.png#align=left&amp;display=inline&amp;height=249&amp;id=jCBob" alt="image.png" /><br />Ridge和Lasso这种加惩罚项的方式叫做正则化（Regularization），在机器学习的应用很广，比如神经网络中就有应用。因此，Ridge也被称为<span class="math inline">\(L_2\)</span>正则化，后者被称为<span class="math inline">\(L_1\)</span>正则化。<br />虽然两者的加的惩罚项看起来差不多，其实是有着非常大的区别的，具体表现为Lasso可以使得系数压缩到0，而Ridge则不会有这种效果，把系数压缩到0的话就可以起到降维和变量选择的作用，因此Lasso在高维的数据中表现更好。<br />那么为啥会有这样的差别呢，首先我们来看他们的惩罚项的形式，一个用的是平方的形式，另一个用的是绝对值的形式，我们把之前的那个损失函数转化成一个优化问题：<br /><span class="math display">\[Ridge: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}\beta_j^2 \le s\\Lasso: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}|\beta_j| \le s\\\]</span><br />假设只有两个系数，我们用几何的方式来表达这个优化问题，Ridge的约束条件是一个平方的形式，可行域就是一个圆，而Lasso的约束条件是绝对值的形式，可行域则是一个菱形，而目标函数在求解时，肯定是跟这个约束条件的可行域相切的，而Lasso由于他是一个菱形，那么他就更容易切到菱形的顶点，因此也会使得系数为0，而Ridge是一个圆，就不容易切到系数为0的地方，因此这就使得Lasso在压缩系数时会更倾向于压缩为0。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621697160270-8a07af21-ed57-4ad0-b2ca-de6eed23bcf8.png#align=left&amp;display=inline&amp;height=240&amp;id=hG528" alt="image.png" /></p><h1 id="代码实践">代码实践</h1><p>使用sklearn自带的波斯顿房价数据集做个试验，分别跑一遍Ridge和Lasso回归，并且通过交叉验证来选取<span class="math inline">\(\lambda\)</span>，将之与线性回归进行对比，结果如下：</p><table><thead><tr class="header"><th></th><th>MSE</th></tr></thead><tbody><tr class="odd"><td>线性回归</td><td>21.8977</td></tr><tr class="even"><td>Ridge回归</td><td>21.7536</td></tr><tr class="odd"><td>Lasso回归</td><td>21.8752</td></tr></tbody></table><p>可以发现，两者的预测效果较线性回归都有一定提升，其中Lasso回归提升较小，这是因为数据集的原因，只有13个变量，并且每个变量都make sense，因此效果就一般了，在高维的数据集中Lasso从理论上 讲应该就会有较好的表现了。<br />具体代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV,LassoCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">y_pred_lr = lr.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;线性回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lr, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ridge=RidgeCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>) <span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line">y_pred_ridge = ridge.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Ridge回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_ridge, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">lasso=LassoCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>)<span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">lasso.fit(X_train, y_train)</span><br><span class="line">y_pred_lasso = lasso.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Lasso回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lasso, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h1 id="总结">总结</h1><p>本文对Ridge和Lasso回归做了一个总结，并通过一个简单数据集做了实践。在写的同时发现需要再去看和学习的东西很多，一个流程下来对于算法原理的理解更加透彻了，这对于搭建自己的知识体系是很有帮助的，希望以后能够坚持学完一个新的东西就写篇总结。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="回归算法" scheme="http://example.com/tags/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>前向分步回归Forward Stagewise Regression原理及Python实现</title>
    <link href="http://example.com/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-05-03T16:00:00.000Z</published>
    <updated>2021-06-30T07:06:32.950Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。</p><h1 id="算法原理">算法原理</h1><p>这个算法的思想与boosting类似，每次迭代时都挑选出一个最优的变量来拟合残差，具体步骤如下：</p><ol type="1"><li>首先将截距项<span class="math inline">\(\beta _0\)</span>设置为<span class="math inline">\(\bar{y}\)</span>，所有的自变量系数<span class="math inline">\(\beta\)</span>都设为0，残差项设置为<span class="math inline">\(r=y-\bar y\)</span></li><li>挑选出与残差项最相关的自变量<span class="math inline">\(x_j\)</span></li><li>更新<span class="math inline">\(\beta _j\)</span>的值：，其中<span class="math inline">\(\delta_j=\epsilon \times \text{sign}[\langle x_j,r \rangle]\)</span>，这个<span class="math inline">\(\text{sign}[\langle x_j,r \rangle]\)</span>代表相关性的正负，<span class="math inline">\(\epsilon\)</span>代表步长。再更新下残差项的值：<span class="math inline">\(r=r-\delta_j x_j\)</span></li><li>重复步骤2，3，直到达到最大迭代次数或者所有的变量都与残差项无关。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620114719031-b1f1c1e8-155e-4258-a114-0d63a13b6a42.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=259&amp;id=uaa9a13b9" alt="image.png" /> 这个算法的优点在于与Lasso回归有着异曲同工之妙，通过选择合适的迭代次数和步长，可以使得部分变量的系数压缩为0，就可以起到变量选择和降低方差的作用，因此在高维数据的场景下会有较好的表现，再偷一张《The Elements of Statistical Learning》的变量系数路径图来说明这一点，左图的横轴为Lasso的L1范式，右图的横轴为前向分布回归的迭代次数，可以看到，变量系数的压缩路径大体上是一致的。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620118711730-c7178912-4b0e-447d-8355-2bdae92fcc77.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=330&amp;id=uff065e10" alt="image.png" /></li></ol><h1 id="python实现">Python实现</h1><p>用波斯顿房价的数据集来做个测试，将迭代次数设为2000的时候，mse要略小于线性回归： <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620121931201-e7594c64-9878-47d3-a851-0285bf12f751.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=44&amp;id=j2U1Z" alt="image.png" /> 因为这个数据集只有13个变量，而且每个变量都很重要，所以前向分布回归的优势并没有很明显，不过通过调参效果还是可以比普通的线性回归好那么一点，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ForwardStagewise</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">0.01</span>, max_iter=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="comment"># 初始化两个参数，eps步长和max_iter迭代次数</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 将X，y转化为数组形式</span></span><br><span class="line">        y = np.asarray(y)</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X = (X - X_mean) / X_std</span><br><span class="line">        self.y_mean = np.mean(y) <span class="comment"># 截距项，也就是y的平均</span></span><br><span class="line">        residual = y - self.y_mean <span class="comment"># 初始化残差项</span></span><br><span class="line">        x_num = np.shape(X)[<span class="number">1</span>] <span class="comment"># 变量数</span></span><br><span class="line">        self.beta = np.zeros((x_num)) <span class="comment"># 用来存储每一次系数更新的数组</span></span><br><span class="line">        self.betas = np.zeros((self.max_iter, x_num))  <span class="comment"># 用来存储每一迭代的系数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            c_hat = <span class="number">0</span></span><br><span class="line">            sign = <span class="number">0</span></span><br><span class="line">            best_feat = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x_num):</span><br><span class="line">                c_temp = X[:, j].T.dot(residual) <span class="comment"># 用来表示x与残差项的相关性</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(c_temp) &gt; c_hat:</span><br><span class="line">                    c_hat = <span class="built_in">abs</span>(c_temp)</span><br><span class="line">                    sign = np.sign(c_temp)</span><br><span class="line">                    best_feat = j</span><br><span class="line">            self.beta[best_feat] += sign * self.eps <span class="comment"># 更新系数</span></span><br><span class="line">            residual -= (self.eps * sign) * X[:, best_feat] <span class="comment"># 更新残差项</span></span><br><span class="line">            self.betas[i] = self.beta</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 预测</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 先标准化</span></span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_test = (X - X_mean) / X_std</span><br><span class="line">        y_pred = X_test.dot(self.beta) + self.y_mean</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">    boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">    X = boston.data</span><br><span class="line">    y = boston.target</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1</span>)</span><br><span class="line">    fs = ForwardStagewise(eps=<span class="number">0.01</span>, max_iter=<span class="number">2000</span>)</span><br><span class="line">    fs.fit(X_train, y_train)</span><br><span class="line">    y_pred = fs.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;前向逐步回归MSE：<span class="subst">&#123;mean_squared_error(y_pred, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line">    lg = LinearRegression()</span><br><span class="line">    lg.fit(X_train, y_train)</span><br><span class="line">    y_pred_lg = lg.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;线性回归回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lg, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="总结">总结</h1><p>前向分布回归和Lasso回归本质上其实差不多，而且两者好像都是最小角回归（Least angle regression）的一个变种，具体可以参见ESL这本书（太难了我看不懂），这两张回归算法都能起到压缩系数和变量选择的作用，但是前向分布回归的计算效率比较差，所以Lasso似乎更为人熟知，不过前者为我们学习boosting相关算法提供了一个不错的切入点。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="boosting" scheme="http://example.com/tags/boosting/"/>
    
    <category term="回归算法" scheme="http://example.com/tags/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（二）</title>
    <link href="http://example.com/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2020-12-10T16:00:00.000Z</published>
    <updated>2021-06-30T07:26:02.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一前言">一、前言</h1><p>书接上文：<a href="https://zhuanlan.zhihu.com/p/329185040">爬取厦门58同城二手房数据进行数据分析（一）</a> 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。</p><h1 id="二数据分析及可视化">二、数据分析及可视化</h1><h2 id="数据预处理">2.1 数据预处理</h2><p>首先导包，由于<code>seaborn</code>画图不支持中文显示，因此还需要加几行代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 中文字体设置-黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line">sns.<span class="built_in">set</span>(font=<span class="string">&#x27;SimHei&#x27;</span>)  <span class="comment"># 解决Seaborn中文显示问题</span></span><br></pre></td></tr></table></figure> 读入数据，删除不需要分析的字段，以及删除存在缺失值的数据： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">data = data.drop(columns=[<span class="string">&#x27;Unnamed: 0&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;产权年限&#x27;</span>, <span class="string">&#x27;location2&#x27;</span>])</span><br><span class="line">data = data[data[<span class="string">&#x27;location1&#x27;</span>] != <span class="string">&#x27;厦门周边&#x27;</span>] <span class="comment"># 删除厦门周边的数据</span></span><br><span class="line">data = data.dropna()</span><br><span class="line">data</span><br></pre></td></tr></table></figure> 最终得到的数据像这样子，去除缺失值后一共749行*16列： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620860545-327f70ad-5f36-4bb7-bb43-d88b1d34f9b1.png" /></p><p>为了方便后续的工作，我们在将数据做一些简单的处理： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;室&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;厅&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;卫&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>]))</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>] = data[<span class="string">&#x27;均价&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;房本面积&#x27;</span>] = data[<span class="string">&#x27;房本面积&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;建筑年代&#x27;</span>] = data[<span class="string">&#x27;建筑年代&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;总楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>:-<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;所在楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">&#x27;小区均价&#x27;</span>] = data[<span class="string">&#x27;小区均价&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;物业费&#x27;</span>] = data[<span class="string">&#x27;物业费&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;绿化率&#x27;</span>] = data[<span class="string">&#x27;绿化率&#x27;</span>].apply(<span class="built_in">float</span>)</span><br><span class="line">data[<span class="string">&#x27;车位信息&#x27;</span>] = data[<span class="string">&#x27;车位信息&#x27;</span>].apply(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure></p><h2 id="单变量可视化">2.1 单变量可视化</h2><p><strong>价格分布</strong></p><p>厦门市的房价总体来说还是非常贵的，一平方米平均要四万多，一套下来得四百多万，买不起买不起 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">&#x27;均价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>].mean()</span><br><span class="line">sns.distplot(data[<span class="string">&#x27;总价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;总价&#x27;</span>].mean()</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607617912033-306d3e0a-325a-4ae7-bbff-1d91d715aa58.png" /></p><p><strong>房屋区域分布</strong></p><p>有将近一半的二手房都在岛内（思明和湖里)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;位置1&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607619988435-7f18f6b6-fa96-4fa9-b889-e04cedf4e344.png" /> <strong>房屋朝向分布</strong></p><p>选取前五种最受欢迎的房屋朝向，可以看出，有2/3的房子都是南北朝向：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋朝向&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620480819-0821a116-4e07-45f8-90cb-dcbe6a31ce44.png" /></p><p><strong>房屋户型分布</strong></p><p>同样选取前五种最受欢迎的房屋朝向，可以发现3室2厅2卫的户型最受欢迎：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋户型&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620697989-2b958314-3805-4be7-8094-2e0237bc6f5d.png" /></p><p><strong>装修情况分布</strong></p><p>二手房基本上都是装修好了的，只有不到10%的是毛坯（为啥二手房还有毛坯的？）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;装修情况&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621101526-c6d1f02d-3bc1-4008-a9f0-7dbc1dbba179.png" /></p><h2 id="多变量间关系及可视化">2.2 多变量间关系及可视化</h2><p><strong>地域与房价</strong></p><p>画出各个区域的每平方米价格的箱型图，果然，岛内的房价更可怕了，思明区接近6万/平米，更有12万/平米的天价房，湖里区也接近5万/平米，就算在同安和翔安这两个鸟不拉屎的地方一平米也要两万多了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(data=data, x=<span class="string">&#x27;位置1&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621274809-4304e87f-f8bf-4720-b874-7cf6192373e5.png" /> 地域与其他变量 将数据做一个聚合，取平均，可以发现，岛内的房子都比较老，大概都在2000年上下（因为没地方可建了吧)，而岛外基本上都在2010年左右，而且岛内的房子就只有十三四层，而岛外的房子有二十层左右，面积也相对来说比岛内的小一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=[<span class="string">&#x27;位置1&#x27;</span>])[<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;房本面积&#x27;</span>,<span class="string">&#x27;建筑年代&#x27;</span>,<span class="string">&#x27;总楼层&#x27;</span>].mean()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621696738-bc35f9ce-7418-460b-a873-a5bf5001c76c.png" /></p><p><strong>建筑年代与房价</strong></p><p>看上去好像越老的房子越贵，上世纪末建的房子最值钱，而最近几年建的房子都不怎么值钱，当然这也跟我们之前分析的区域有关，因为最近建的房子基本都在岛外，所以当然不怎么值钱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;建筑年代&#x27;</span>)[<span class="string">&#x27;均价&#x27;</span>].mean().plot()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607623479793-fe610cab-0954-4669-afff-3824eabb0d12.png" /></p><p><strong>所在楼层与房价</strong></p><p>一般来说，大家都不太喜欢低楼层的房子，因为太吵了，当然太高也不行，这种关系，也反映在房价中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;所在楼层&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607622112702-1862764d-a020-4303-8938-244724f22518.png" /></p><p>再来看看厦门哪个小区的房子最贵吧，这里选取小区均价最高的15个小区：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;小区名&#x27;</span>)[<span class="string">&#x27;小区均价&#x27;</span>].mean().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">15</span>).plot(kind=<span class="string">&#x27;barh&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607651743576-9563d729-5c92-4f5c-a10d-1d85574a67fe.png" /></p><h2 id="地理可视化">2.3 地理可视化</h2><p>前阵子刚好接触到百度地图的API，非常强大，就顺手做个地图可视化吧！ 首先需要去百度地图开发者官网（ <a href="https://link.jianshu.com/?t=http://lbsyun.baidu.com/">http://lbsyun.baidu.com/</a>）注册一个密钥，然后创建两个应用，一个是服务端的，用来使用Python获取小区坐标，一个是浏览器端的，用来通过修改html源代码创建热力图，具体实现可以参考这篇文章：<a href="https://blog.csdn.net/ebzxw/article/details/80265796">Python使用百度地图API实现地点信息转换及房价指数热力地图</a> 最后生成的效果如下图所示，可以看出，厦门市最贵的地段基本上就在火车站周围那一块： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607655308085-a39295fa-8bad-495b-80b8-8fedada0be79.png" /> <em>ps: 这里可视化原本想使用 folium，但是存在 folium包存在两个问题，一个是热力图存在 bug，没有渐变效果，另外一个是因为我坐标采用的是百度的坐标，百度的坐标是经过加密的，用在 folium上会存在坐标偏移的情况，故弃用</em></p><h1 id="三预测模型">三、预测模型</h1><p>以每平方米价格为因变量，其余变量为自变量，并将分类变量使用 LabelEncoder 编码，将测试集与训练集以2：8的比例分割： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=data.drop(columns=[<span class="string">&#x27;小区均价&#x27;</span>,<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;均价&#x27;</span>,<span class="string">&#x27;房屋户型&#x27;</span>,<span class="string">&#x27;小区名&#x27;</span>])</span><br><span class="line">y=data[<span class="string">&#x27;均价&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">&#x27;位置1&#x27;</span>,<span class="string">&#x27;房屋朝向&#x27;</span>,<span class="string">&#x27;一手房源&#x27;</span>,<span class="string">&#x27;所在楼层&#x27;</span>,<span class="string">&#x27;装修情况&#x27;</span>]:</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    x[col]=le.fit_transform(x[col])</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure> 由于自变量中存在很多分类变量，因此考虑使用树模型进行预测，由于树模型本身就有着特征选择的功能，因此，不做特征选择，直接跑模型</p><p><strong>决策树</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeRegressor()</span><br><span class="line">dt.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;决策树绝对值误差：<span class="subst">&#123;mean_absolute_error(dt.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>随机森林</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">2000</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">rf.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;随机森林绝对值误差：<span class="subst">&#123;mean_absolute_error(rf.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>Catboost</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cb=CatBoostRegressor()</span><br><span class="line">cb.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Catboost绝对值误差：<span class="subst">&#123;mean_absolute_error(cb.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>结果对比</strong></p><table><thead><tr class="header"><th></th><th>决策树</th><th>随机森林</th><th>catboost</th></tr></thead><tbody><tr class="odd"><td>绝对值误差</td><td>2885.81</td><td>2286.76</td><td>2347.04</td></tr></tbody></table><p><strong>特征重要性</strong></p><p>用随机森林输出特征重要性看看： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fi = pd.DataFrame(</span><br><span class="line">    &#123;<span class="string">&#x27;x&#x27;</span>: x.columns, <span class="string">&#x27;feature_importance&#x27;</span>: rf.feature_importances_&#125;)</span><br><span class="line">fi = fi.sort_values(by=<span class="string">&#x27;feature_importance&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;feature_importance&#x27;</span>, y=<span class="string">&#x27;x&#x27;</span>, data=fi)</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607667452772-a2ae6636-935b-411c-b9fa-864472bf30a5.png" /> 啊这，小区均价一枝独秀，解释力度太大了，把其他特征的信息都全部吃下去了，为了更好的解释其他特征与每平方米价格的关系，我们考虑把它排除在外，再输出一次特征重要性： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607668343722-a06abc5d-4305-4e67-a2ea-ee97a3a68a0b.png" /> 这次就好点了，预测的绝对值误差虽然变成了四千，预测效果变差了，但是解释力度提高了，对房价影响最大的前五个特征为：位置1（区域）、物业费（反映小区的质量）、容积率（反映小区的居住的舒适度)、总楼层、建筑年代，而房屋朝向、所在楼层和装修情况这些特征居然没有想象中的那么重要，看来在厦门，<strong>决定一套房子价格的是房子所在小区的属性，而不是你这套房子本身的属性</strong>。</p><h1 id="四小结">四、小结</h1><p>好了，又一篇文章水完了，这篇文章还是花了我不少时间的，尤其是在研究怎么画图上，看来可视化这方面还是得继续学习一下啊！这个月总体来说还是比较忙的，希望能够坚持每周写一篇吧，下周可能会开始写一些算法的学习笔记。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;一前言&quot;&gt;一、前言&lt;/h1&gt;
&lt;p&gt;书接上文：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/329185040&quot;&gt;爬取厦门58同城二手房数据进行数据分析（一）&lt;/a&gt; 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="数据分析" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（一）</title>
    <link href="http://example.com/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2020-12-03T16:00:00.000Z</published>
    <updated>2021-06-30T07:21:13.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一前言">一、前言</h1><p>最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。</p><h1 id="二网页分析">二、网页分析</h1><p>首先，点进去首页，是一行行的信息，一页有120条： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010708512-8e3cc62c-fd97-4df6-a329-014b77fc3cf8.png#align=left&amp;display=inline&amp;height=419" alt="image.png" /> 点进去链接后是详情页的信息，主要分为两部分信息：房子属性和小区信息 <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011106024-d85dbb2a-08eb-47dd-b954-c89b1c253faa.png#align=left&amp;display=inline&amp;height=388" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010951590-61f97fe9-5be9-40f0-9eb2-47ef2046d244.png#align=left&amp;display=inline&amp;height=251" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010981428-3ead9cef-a4d1-4950-a749-ea116896ad27.png#align=left&amp;display=inline&amp;height=310" alt="image.png" /> 这里需要注意的是，虽然这些信息都可以直接用xpath获取，但是详情页里面的价格居然用了字体加密！！！ <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011288804-77aaeae6-90d6-4295-8486-30459b7c8642.png" alt="image.png" /> 虽然网上有很多这种破解办法，但其实根本不需要那么复杂，回到我们的首页，那边不也有价格嘛，而且，这个价格是没有加密的哦！（58同城程序员故意留的后门？？？)，所以我们爬虫思路大概就分三步走：</p><ol type="1"><li>先爬取首页一行行的信息，用xpath获取标题，价格，链接，一条信息用一个字典存储，然后将这条字典并存储在一个列表里面</li><li>第一步爬完后，然后开始遍历我们的列表，进入详情页的链接，把详情页的信息也用xpath一个个扒到我们的字典里，最终返回一个具有完整信息的列表</li><li>使用pandas的Dataframe函数就可以将第二部返回的列表直接转化为一个Dataframe，就能直接导出csv了。 原以为这样就能直接收工了，没想到小看58同城了，还没爬几条就给你来了个人机验证，访问再频繁点就需要登录了，然后就啥信息都爬不到了，因为自己之前爬的都是比较简单的网站，从来就没出现过这个问题，看来还是自己naive了啊，所以前几天也花了一整天的时间来解决这个问题，接下来就看看如何解决这个问题吧！</li></ol><h1 id="三ip访问频繁问题">三、ip访问频繁问题</h1><p>一般这种问题的解决方案有两种：</p><ol type="1"><li>使用代理ip访问。</li><li>设置程序休眠时间和断点续传机制。</li></ol><p>第一种解决方案的话，一般来说是需要自己花钱去买付费ip的，因为大多数免费ip都被人用烂了，考虑到穷和需要花时间去学习怎么构建代理池的问题，于是第一种方案就被我放弃了。 第二种方案算是一种土办法吧，只要我访问足够慢，跟正常用户一样，那么它就不会封我ip（所以这次我爬500条数据都用了两个小时），但是，为了避免还是弹出人机验证的问题，我们需要一个断点续传机制，即当我知道自己的ip被封了的时候，就马上停止访问，并且把已经爬取的数据全部保存下来，然后手动用浏览器去网站上点一下验证码，然后再从之前停止的地方继续开始，这就需要我们给每条信息编个号，当检测到被封ip的时候输出这个编号，手动验证之后继续从这个编号处开始。 说了这么多都是废话，接下来直接上代码吧！ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="comment"># 输入链接，返回解析后的html</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36 Edg/86.0.622.63&quot;</span>&#125;</span><br><span class="line">    response = requests.get(url=url, headers=headers)</span><br><span class="line">    content = response.content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    html = etree.HTML(content)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_base_info</span>(<span class="params">page_url</span>):</span></span><br><span class="line">    <span class="comment"># 获取基础信息</span></span><br><span class="line">    html = parse_url(page_url)</span><br><span class="line">    titles = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/text()&#x27;</span>)  <span class="comment"># 标题</span></span><br><span class="line">    urls = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/@href&#x27;</span>)  <span class="comment"># 链接</span></span><br><span class="line">    total_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;sum&quot;]/b/text()&#x27;</span>)  <span class="comment"># 总价</span></span><br><span class="line">    unit_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;unit&quot;]/text()&#x27;</span>)  <span class="comment"># 均价</span></span><br><span class="line">    base_infos = []  <span class="comment"># 使用一个列表存储所有信息</span></span><br><span class="line">    <span class="keyword">for</span> title, url, total_price, unit_price <span class="keyword">in</span> <span class="built_in">zip</span>(titles, urls, total_prices, unit_prices):</span><br><span class="line">        <span class="comment"># 将信息写入一个字典中</span></span><br><span class="line">        info = &#123;&#125;</span><br><span class="line">        info[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">        <span class="keyword">if</span> url[<span class="number">0</span>:<span class="number">5</span>] != <span class="string">&#x27;https&#x27;</span>:  <span class="comment"># 有的链接不是https开头的，手动加上</span></span><br><span class="line">            url = <span class="string">&#x27;https:&#x27;</span>+url</span><br><span class="line">        info[<span class="string">&#x27;url&#x27;</span>] = url.split(<span class="string">&#x27;?&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># 删掉链接后面跟的cookie参数什么乱七八糟的东西</span></span><br><span class="line">        info[<span class="string">&#x27;total_price&#x27;</span>] = total_price</span><br><span class="line">        info[<span class="string">&#x27;unit_price&#x27;</span>] = unit_price</span><br><span class="line">        base_infos.append(info)</span><br><span class="line">    <span class="keyword">return</span> base_infos</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_extra_info</span>(<span class="params">info</span>):</span></span><br><span class="line">    <span class="comment"># 进入详情页获取更多信息</span></span><br><span class="line">    info_url = info[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">    html = parse_url(info_url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[1]/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[2]/text()&#x27;</span>)[<span class="number">0</span>].replace(<span class="string">&#x27;－&#x27;</span>, <span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 获取详情页表格中的信息</span></span><br><span class="line">    info_keys = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;mr_25 c_999&quot;]/text()&#x27;</span>)[<span class="number">1</span>:]</span><br><span class="line">    info_values = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;c_000&quot;]&#x27;</span>)</span><br><span class="line">    info_values = [v.text <span class="keyword">for</span> v <span class="keyword">in</span> info_values]</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> <span class="built_in">zip</span>(info_keys, info_values):</span><br><span class="line">        info[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取小区及周边信息</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/h3/a/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[1]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[3]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[4]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[5]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[6]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://xm.58.com/ershoufang/pn&#x27;</span></span><br><span class="line">infos = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))  <span class="comment"># 设置休息时间应对反爬</span></span><br><span class="line">    page_url = base_url+<span class="built_in">str</span>(i)</span><br><span class="line">    results = get_base_info(page_url)</span><br><span class="line">    infos.extend(results)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;爬取页面<span class="subst">&#123;i&#125;</span>的基础信息成功！&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(infos)):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))</span><br><span class="line">    infos[i] = get_extra_info(infos[i])</span><br><span class="line">    <span class="keyword">if</span> infos[i][<span class="string">&#x27;location1&#x27;</span>] == <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> infos[i][<span class="string">&#x27;xiaoqu_name&#x27;</span>] == <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 如果这两个值都为空值，说明开始人机验证了</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;爬取第<span class="subst">&#123;i&#125;</span>条信息失败,请进行人机验证! &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(infos[i][<span class="string">&#x27;url&#x27;</span>])</span><br><span class="line">        <span class="comment"># 及时保存数据</span></span><br><span class="line">        data = pd.DataFrame(infos)</span><br><span class="line">        data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取第&#123;&#125;条信息成功：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, infos[i][<span class="string">&#x27;title&#x27;</span>]))</span><br><span class="line">        </span><br><span class="line">data = pd.DataFrame(infos)</span><br><span class="line">data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)  <span class="comment"># 导出到csv文件</span></span><br></pre></td></tr></table></figure></p><h1 id="三总结">三、总结</h1><p>这次爬虫主要学了三个东西吧：</p><ol type="1"><li>字典是个很有用的数据类型，不仅存储效率高，而且将多个字典放在列表里可以直接转化为pandas的Dataframe，还就能直接导出，不需要读写文件那么麻烦。</li><li>xpath的异常处理机制很重要。有些信息可能会在某个页面上神奇的消失了，所以最好事先做好异常处理机制，爬不到就置为空值，不然程序一报错之前爬的数据全没掉了。</li><li>断点续传机制也很重要。天知道网站会在什么时候给你跳出验证码，所以最好有断点续传机制，防止你要从头来过。</li></ol><p>好了，第一篇技术性的文章就这样水成了，第二篇过几天有空写。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;一前言&quot;&gt;一、前言&lt;/h1&gt;
&lt;p&gt;最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>2020美赛参赛经验</title>
    <link href="http://example.com/2020/05/03/2020%E7%BE%8E%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    <id>http://example.com/2020/05/03/2020%E7%BE%8E%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/</id>
    <published>2020-05-02T16:00:00.000Z</published>
    <updated>2021-06-30T07:29:29.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>前几天美赛出成绩了，有幸能够获得M奖，虽然说在知乎这种人均F奖、M奖的环境下看似乎算不了什么，但是对于我个人而言，和两位队友四天四夜投入所有时间精力来解决一个陌生的问题在我的大学生活中也算是比较珍贵的一段经历吧！ 鉴于参赛前参考了很多前辈们的参赛经验帖，受益颇深，因此，我也在此将自己本次参赛的一些心得分享给之后参赛的同学们，但凡能够从某个角度上帮助到某些人，那么我写的这些东西也都是值得的了。 文章提到的资料和我本次参赛的作品均已上传到GitHub，好的正文开始：</p><h1 id="参赛经验">参赛经验</h1><h2 id="关于组队">关于组队</h2><p>因为一开始我的想法就是奔着C题大数据去的，因此就直接找了两个经济统计专业的好朋友组队，我们三人都有参加过之前国赛的经历，而且我知道他们两个都很靠谱，所以在组队上我并没有遇到什么太大的问题，我想这也是我们能够拿奖很重要的原因之一吧，但是我知道这次有很多队伍因为组队不慎的原因造成了两人建模甚至一人建模的尴尬处境，所以组队还是得慎重，我的建议就是：优先找自己熟悉的、靠谱的人，而不是一味追求队友要跨专业背景（比如计算机+数学+文科），像我们之前的国赛，我这次的两个队友当时就是为了追求专业背景多样性都遇到了坑队友，队友A找了个数学妹子，结果数学妹子对于建模并没有什么优势，而且当时中秋节还跑出去和她的男票过节了...队友B找了个计算机同学，结果发现好像他编程能力也不怎么样，而且比赛的时候一直在看NBA...所以我觉得组队的一个原则就是首先找的队友要靠谱，就算能力目前不是很强，但是起码会认真对待而且不会半途甩锅走人的那种，在此基础上再去追求队友优势的互补。</p><h2 id="赛前准备">赛前准备</h2><p>他山之石可攻玉，所以我建议在赛前尽可能多的看别人分享的参赛经验贴，然后从中总结出适用于自己的方法和套路，阅读往年的O奖论文也挺重要的（虽然我一篇也没完整的看下去，不过也还是从中学到了一点套路），可以学习别人的行文逻辑等。 队友的分工最好赛前就确定好，可以针对性的去做准备，而且最好是一人负责学习两块这样子，个人觉得建模编程和写作不应该完全割裂开来，比如在这次比赛中，我就是负责编程和建模，然后其他两个队友负责建模和写作，以建模为核心，一人负责两块，这样会减轻沟通成本，就不至于模型都建好了，但是写作的人不理解无法将模型的思想表达到文章中，或者编程的人根本不理解这个模型导致写不出来程序。接下来我会根据这三个模块分享下自己赛前的准备经验。</p><p><strong>关于编程</strong></p><p>有的人可能会纠结用哪种编程语言，比如用MATLAB还是Python，如果你还没学的话，我推荐学Python吧，由于有着各种第三方库的支持，Python在处理尤其是像C题这种大数据类型的题目简直有着得天独厚的优势，很多模型或者算法都能直接调包解决，比如这次C题的情感分析，不用Python我还真不知道要怎么做。 这里推荐两本书籍，如果你想选C题的话或者以后想用Python做数据分析的话，可以考虑好好阅读一下，寒假时我就粗略看了下，比赛时的我的代码编写效率提高了很多很多：</p><ol type="1"><li><a href="https://seancheney.gitbook.io/python-for-data-analysis-2nd/">《利用Python进行数据分析第二版》</a>：主要讲的是数据处理，pandas，numpy，matplotlib的使用方法，较为详细</li><li>《Python数据科学手册》：一部分和上面那本内容重叠，后面也有讲一些机器学习模型的介绍和使用</li></ol><p>由于好像目前建模主流的编程语言都是MATLAB，所以网上能找到很多大佬或者培训机构整理的MATLAB实现算法代码，但是Python好像还很少有人做这样的工作，这里推荐一篇自己赛前收藏的文章，作者整理的还是比较详细的，覆盖了大部分的机器学习模型和启发式算法：<a href="https://zhuanlan.zhihu.com/p/105605953">数学建模部分算法整理</a></p><p><strong>关于建模</strong></p><p>因为在去年暑假的时候参加过学校组织的国赛培训，当初就了解了不少模型，因此在这美赛中我并没有投入太多时间来学习模型。在模型这一块，如果时间足够的话可以去系统的学习下以下的经典的模型，基本上是能够覆盖大部分题型的要求，如果没时间的话，其实大概看一下就行了，知道这个模型的主要思想是什么，能用来解决什么问题，到时候比赛的时候就只需要现学现卖就行了：  <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1580662526815-7abbd040-dda3-4a9a-928f-197227a4c8c0.png" /> 这里也推荐两本书籍（虽然我都没怎么看，但看了还是很有好处的）：</p><ol type="1"><li>《数学建模算法与应用》--司守奎</li><li>《数学模型》--姜启源</li></ol><p><strong>关于写作</strong></p><p>写作工具推荐使用latex，当然用word也不是不可以，我看往年的很多O奖文章也都是用word写出来的，使用latex的好处在于能够免去排版的烦恼，如果word的话还得去慢慢调一些字体字号行距页眉页脚等。而且其实latex上手也不是很难，但是需要找到系统的入门教程，比如《Ishort》，该教程可以在<a href="https://www.latexstudio.net/">LaTeX工作室</a>找到，他家也有提供各种latex模板，我的美赛和国赛的模板都使用他们的，相对于很多网上随便搜索找到的模板制作要好很多，这里也要说一下，如果使用latex 的话一定要用专门的模板，不然的话还不如用word。找到了模板和入门教程，差不多折腾一个晚上也就能上手latex写作了。这里推荐几个latex在线的网站，一定程度上能够提高写作的效率：</p><ol type="1"><li><a href="https://cn.overleaf.com/">Overleaf, 在线LaTeX编辑器</a>：可以在线协同写作，多人编辑需要花钱，不想花钱可以考虑一周试用，原本想用这个的，但是队友没有科学上网条件访问会很慢，遂放弃。</li><li><a href="http://www.tablesgenerator.com/#">http://www.tablesgenerator.com/#</a>：在线生成表格代码，因为latex弄表格实在太麻烦了，所以我都是现在Excel里面打好表格，然后复制到这个网站里面生成代码，再粘贴到latex编辑器里面。</li><li><a href="http://latex.codecogs.com/eqneditor/editor.php">http://latex.codecogs.com/eqneditor/editor.php</a>：在线生成公式代码，没怎么用过，打公式不熟的话用这个应该可以提高效率</li></ol><p>美赛需要英文写作的，专门负责写作的同学不妨看下这本美赛官方出品的《正确写作美国大学生数学建模竞赛论文》，当然我的队友写作都是直接用机翻，写一句中文然后翻一句英文，写出来非常的Chinglish哈哈哈。</p><h2 id="比赛时">比赛时</h2><p>有效的查找资料，无疑是打开思路最重要的一环，像上次的国赛的出租车问题，我就在IEEE上找到一篇文献，为我们那时候提供了很重要的思路，这次的美赛也是，虽然并没有找到什么有价值的文献，于是我就往kaggle上面找，结果还真发现了很多做电商评论挖掘的，于是带着惊喜认真的看了几个大佬开源的notebook，从中也收获了不少。这里推荐自己查找资料的一些方法：</p><ol type="1"><li>谷歌学术+sci-hub：科研狗必备，选择合适的关键词检索往往能够找到不错的文章，就是英文文档读起来比较费劲，可以用谷歌的文档翻译或者更为专业的翻译狗先翻译成中文粗略看一遍如果觉得可以再看原文</li><li>知网上名牌大学发表的期刊论文或者硕博士论文：虽然知网上很多期刊论文都很水，但是这些名牌大学发表的论文硕博士论文质量还是挺有保障的</li><li>kaggle或者天池等数据竞赛平台：针对大数据题目有奇效，我去年国赛的数据和这次美赛的思路就是在这里找到的</li></ol><p>这次比赛整体来说我和队友还是相处的很愉快的，没有太多冲突，时不时还能互相吹下牛逼，但是在要提交论文的那天晚上，我却因为一个标点符号加不加空格的问题和一个队友吵起来了。。。我们都知道英文每写完一句标点符号后面是要加一个空格的，但是我写作的队友很多地方都没加，不过用的是机翻，从翻译复制过来的那部分就会自己加，于是我就提出了要把所有标点都加上空格，但那个队友嫌麻烦，觉得这种细节的问题并不重要，而且也比较晚了，我们当时就想收工的了，我对于细节要求比较高，所以当时就和那个队友吵起来了，当时情绪都比较激动，所以我就做了让步干脆不加了直接交上去，后面发现其实问题也不是很大嘛。所以我建议比赛时要是和队友发生冲突，先思考下冲突的点是不是问题的本质，如果只是细节问题的话，我就觉得没必要吵了，因为对整体影响不大，没必要因此进行不必要的争吵浪费时间并且破坏大家的心情。</p><h1 id="c题思路">C题思路</h1><p>比赛已经过去快两个月了，我现在对当时的思路印象也不是很深了，而且这个对于后面的人参考意义也不是很大，但还是简单写一下吧！</p><p><strong>数据的预处理</strong></p><p>仔细看下这些数据，其实是存在很多垃圾数据的，比如奶嘴里面就有很多行商品其实根本不是奶嘴，可能是因为爬虫的原因吧，所以第一步是把那些垃圾数据剔除掉，这里我采取的方法比较简单粗暴，就只是对商品名做了一个判断，比如对于microwave，我会判断他的商品标题里面是不是含有microwave，如果没有microwave这个词，那就给这行数据删掉，这样至少能剔除大部分的垃圾数据。 然后就是评论数据的处理，很关键的一点就是怎么将这些评论数据的情感值量化，一开始我觉得这个很难，因为没做过这种，甚至当时都有点想放弃C题了，不过后面发现其实也很简单，用Python的NLTK包里面就有训练好的情感量化模型，剔除禁用词后直接调用即可，然后就可以将每一行的评论数据都转化成一个情感值，还蛮准的其实，但是在上手NLTK时踩了几个坑（一个是语料库的下载，不知道为什么有的调用代码会下载失败，所以最后我索性直接把的6G多的全部语料库手动下载下来在导进去），所以这里花了挺多时间的。</p><p><strong>模型构建</strong></p><p>这次我们用到的模型其实都比较简单，题目有一个是需要从评论数据中挖掘出有价值的信息，一开始我们是想用LDA主题建模的，但是发现效果很差，分出来的那几个主题很难找到一个逻辑，遂放弃。后来就直接用了一个简单的逻辑回归模型，将评论分为两类，好的和差的，然后将那列评论数据用Tf-Idf算法向量化，跑个回归，根据逻辑回归的得出的系数大小和显著性就可以筛选出每一个商品最正面的词和最负面的词，这些词就可以反映顾客最关注的产品的特性，然后一通bb即可。 再后面题目有要求构建一个根据时间的评价模型，这里队友想出了一个比较“随意”的模型，把那几列变量用某种方式相加相乘组合得出了一个评分模型，然后按照月份画个图，看看这些商品的评分有没有存在季节性，记得还有一个要求是看这个商品先前的评价是否会影响之后的评价，这个我们就简单测试了一下这个评分是否存在自相关，结论是没有，即不会影响。然后最后就是一通bb。</p><h1 id="总结">总结</h1><p>这次的美赛对于我来说还是比较爽的一段经历，和两位很靠谱的队友在四天之内投入所有时间精力解决了一个自己陌生的问题，说实话也挺累的，不过还是从中收获了很多东西，我之后应该不会再参加什么建模比赛了，所以这篇文章就献给那些之后参赛的朋友，希望能够对你们有所帮助，同时，也用来记录下自己这段比较珍贵的经历。 第一次写这种经验贴，如有不足，还请指正，也欢迎私信和我交流！</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;前几天美赛出成绩了，有幸能够获得M奖，虽然说在知乎这种人均F奖、M奖的环境下看似乎算不了什么，但是对于我个人而言，和两位队友四天四夜投入所有时间精力来解决一个陌生的问题在我的大学生活中也算是比较珍贵的一段经历吧！ 鉴于参赛前参考了很多前辈们的参赛经验帖，受益颇深，因此，我也在此将自己本次参赛的一些心得分享给之后参赛的同学们，但凡能够从某个角度上帮助到某些人，那么我写的这些东西也都是值得的了。 文章提到的资料和我本次参赛的作品均已上传到GitHub，好的正文开始：&lt;/p&gt;</summary>
    
    
    
    <category term="经验" scheme="http://example.com/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
    <category term="数学建模" scheme="http://example.com/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="美赛" scheme="http://example.com/tags/%E7%BE%8E%E8%B5%9B/"/>
    
  </entry>
  
</feed>
