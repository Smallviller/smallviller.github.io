<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lukan&#39;s Blog</title>
  
  <subtitle>记录点滴成长</subtitle>
  <link href="https://lukan217.github.io/atom.xml" rel="self"/>
  
  <link href="https://lukan217.github.io/"/>
  <updated>2022-08-16T18:05:47.287Z</updated>
  <id>https://lukan217.github.io/</id>
  
  <author>
    <name>Lukan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>时间序列的区间预测/概率预测</title>
    <link href="https://lukan217.github.io/2022/08/17/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%8C%BA%E9%97%B4%E9%A2%84%E6%B5%8B_%E6%A6%82%E7%8E%87%E9%A2%84%E6%B5%8B/"/>
    <id>https://lukan217.github.io/2022/08/17/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%8C%BA%E9%97%B4%E9%A2%84%E6%B5%8B_%E6%A6%82%E7%8E%87%E9%A2%84%E6%B5%8B/</id>
    <published>2022-08-16T17:57:01.824Z</published>
    <updated>2022-08-16T18:05:47.287Z</updated>
    
    <content type="html"><![CDATA[<p>一般我们做时间序列预测都是做点预测（point forecasting），很少会去考虑区间预测（interval forecasting），或者概率预测（probabilistic forecasting），但实际上区间预测也是很重要的，具体来说有这三方面的作用：</p><ol type="1"><li>刻画不确定性以应对风险，预测都是服务于决策，那么在决策时就必然要考虑到可能的风险，也就是最好的情况和最坏的情况，因此就需要使用区间预测来描述预测值可能的上下限</li><li>特定场景下的用处，比如供应链中的库存管理模型，使用区间预测/概率预测可以用来最优化补货量，如果需要满足95%的服务水平，那么就可以输出95%分位数下对应的销量预测值来作为补货量</li><li>好看，业务方和老板在看你预测结果时，如果只有一条干巴巴的曲线是不好说服他们的，毕竟预测就是个玄学，那就整一些花里胡哨的的，比如加个区间，这样他们看着就很开心了（）</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1660099121338-60770410-fc82-4fa4-806a-5131f66769b8.png#clientId=u6d0bf445-93be-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u3b9790df&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=433&amp;originWidth=603&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=93491&amp;status=done&amp;style=none&amp;taskId=u69c564d1-67c8-4b3c-b8f4-17b9e5e996e&amp;title=" alt="image.png" /><br />目前关于时间序列区间预测的方法似乎还没有一个系统性的总结，这里我就把自己调研看到的几种方法整理出来，根据我的分类，区间预测具体可以分为两类方法：</p><ol type="1"><li><p>统计学方法，使用统计学上的一些方法来估计区间</p></li><li><p>损失函数法，通过定义特定的损失函数来输出区间</p></li></ol><h1 id="统计学方法">统计学方法</h1><h2 id="区间估计法">区间估计法</h2><p>学过统计学的都会知道区间估计，要估计一个值的区间，首先会假设它服从正态分布，进一步再计算出这个值的估计标准差，然后给定某个置信度，比如95%，查表就可以得到Z值为1.96，那么就可以把Z值乘上标准差，再用均值加减一下，就可以得到区间的左端和右端。<br />区间预测也用到的同样的思路，但是在区间预测里，我们并不会假设预测值服从正态分布，而是假设误差服从正态分布，然后估计出误差的上下限，再把他加到预测值里面，就可以得到预测值的上下限了，具体来说，步骤如下：</p><ol type="1"><li><p>假设预测误差服从均值为0的正态分布，估计预测误差的标准差</p></li><li><p>给定置信度，查表得到Z值</p></li><li><p>计算预测误差的上限和下限</p></li><li><p>将上下限加到预测值里面，得到一个预测区间</p></li></ol><p><span class="math inline">\(\hat{y}_{T+h \mid T} \pm k \hat{\sigma}_{h}\)</span><br />但是，怎么得到预测误差的标准差呢？很多传统统计学的预测方法都是直接用训练误差的标准差，也有了很多成熟的估计方法，但是由于训练数据上是过拟合的，会导致这个标准差比较小，就导致最后出来的预测区间也比较小，所以，比较合理的做法是在训练数据集上先划分出一个验证集，然后使用验证集上的预测误差来估计标准差。 <a name="lPNfh"></a></p><h2 id="bootstrap">Bootstrap</h2><p>bootstrap，也就是自助采样法，这个方法的思路也是先估计出一个误差的上下限，然后把这个上下限加到原来的预测值中，进而得到预测区间，不过，bootstrap不需要假定误差服从正态分布，而是通过采用N次的预测误差，然后取这N次的误差的分位数作为上下限，比如抽样了三次误差，分别为[-50,0,50], 则5%分位数为-45，95%分位数为45，把这个分位数误差加到预测值上就得到了预测区间。<br />参考链接：<a href="https://otexts.com/fppcn/prediction-intervals.html">https://otexts.com/fppcn/prediction-intervals.html</a> <a name="fO0kk"></a></p><h1 id="损失函数">损失函数</h1><p>最近几年，基于深度的时序预测方法也很多，所以也衍生出了一些区间预测的方法，但具体来说都是从损失函数层面来实现的。 <a name="jQRFV"></a></p><h2 id="分位数损失">分位数损失</h2><p>分位数的损失函数形式如下所示：<br /><span class="math inline">\(L_{q}(y, \hat{y})=q(y-\hat{y})_{+}+(1-q)(\hat{y}-y)_{+}\)</span><br />其中，<span class="math inline">\((\cdot)_{+}=\max (0, \cdot)\)</span>.，加号左边那项代表的就是预测值小于真实值的loss，右边那项代表队是预测值大于真实值时的loss，我们通过取不同的q来理解下这个函数：</p><ul><li>当 <span class="math inline">\(q=0.5\)</span>时，两边的权重相等，这个损失函数就和MAE一样</li><li>当<span class="math inline">\(q=0.95\)</span>时，左边那项的loss权重比较大，因此，模型就会尽可能的使得预测值大于真实值，这样才能使得整体的loss小，这就起到了一个拉高预测值的作用，也可以理解为预测区间的上限</li><li>当<span class="math inline">\(q=0.05\)</span>时，这时候就是右边的那项loss权重比较大，因此，模型就会尽可能使得预测值小于真实值，才能保证整体的loss小，这就起到了一个拉低预测值的作用，也可以理解为预测区间的下限</li></ul><p>在实操时，我们一般会指定三个分位数，如（0.1, 0.5, 0.9），把这三个分位数损失加起来作为最终的损失函数，在预测时就可以输出三个值，分别对应：10%的区间预测，点预测以及90%的区间预测，目前很多基于深度学习的时序预测算法都用到了这个损失函数，比如MQRNN/CNN, TFT等，GBDT也可以使用这个损失，像lightgbm和xgboost的objective里面也都有quantile这个选项，也都可以输出区间预测。 <a name="vQtET"></a></p><h2 id="负对数似然损失">负对数似然损失</h2><p>这个思路我最早是在DeepAR那看到的，大概思路是首先指定一个预测值服从的概率分布，如正态分布，然后，使用神经网络模型分别预测这个概率分布的参数，比如正态分布就是预测他的均值和方差，接着构造负对数似然函数作为损失函数，优化这个损失函数就可以到得到概率分布的参数，最后就可以得到预测时每一步的概率分布，知道了概率分布，那么就可以通过蒙特卡洛采样的方式来生成预测值和区间预测了，比如对这个概率分布采样100次，那这100次的均值就是点预测的结果，95%分位数和5%分位数就可以对应区间预测的结果。 # 开源工具包的实现</p><h2 id="gluonts">GluonTS</h2><ol type="1"><li><p>对于自回归模型，通过预测概率分布来实现概率预测</p></li><li><p>对于其他模型，使用分位数回归</p></li></ol><h2 id="darts">Darts</h2><ol start="3" type="1"><li><p>对于传统统计学模型，使用区间估计法进行概率预测</p></li><li><p>对于部分深度学习模型，使用负对数似然损失</p></li></ol><h2 id="mapie">MAPIE</h2><p>这是一个专门用来做区间预测的包，基于sklearn接口进行开发的，支持回归、分类、时序回归的区间预测，其中，时序部分的区间预测用了一篇论文（<a href="https://arxiv.org/abs/2010.09107">https://arxiv.org/abs/2010.09107</a>）的算法，叫做EnbPI，号称是一个通用的distribution-free的时序区间预测框架，不需要划分验证集重新训练，原理太复杂没去看，试着跑了下demo有点慢，并且目前这个包只支持sklearn那边的模型。</p><h1 id="总结">总结</h1><p>时间序列的区间预测方法按照我的分类方式大致可以分为两大类，其中，统计学方法通过估计误差的上下限再加到原来的预测值上面进行区间预测，一般在传统统计学模型（ARIMA、指数平滑法等）上应用很多，因为估计误差的方法已经有了非常成熟的公式，很多包也集成了这些区间预测，但应用在其他模型上面可能先需要划分训练集和验证集，估计出误差后再对测试集进行区间预测。</p><p>而损失函数的方法只能用在深度学习和GBDT这些靠优化损失函数来预测的模型，具体可以分为分位数损失和负对数似然损失，分位数损失通过损失函数拉高/拉低预测值来实现区间预测的效果，GBDT和深度学习都能用，而负对数似然损失通过直接预测概率分布，然后采样的方式来实现预测，只能用在深度学习模型上</p><h1 id="参考">参考</h1><ol type="1"><li><p><a href="https://unit8.com/resources/probabilistic-forecasting-in-darts/">Probabilistic Forecasting in Darts - Unit8</a></p></li><li><p><a href="https://towardsdatascience.com/time-series-forecasting-prediction-intervals-360b1bf4b085">Time Series Forecasting: Prediction Intervals | by Brendan Artley | Towards Data Science</a></p></li><li><p><a href="https://otexts.com/fppcn/prediction-intervals.html">3.5 预测区间 | 预测： 方法与实践</a></p></li><li><p><a href="https://arxiv.org/abs/1906.05264">[1906.05264] GluonTS: Probabilistic Time Series Models in Python</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;一般我们做时间序列预测都是做点预测（point forecasting），很少会去考虑区间预测（interval forecasting），或者概率预测（probabilistic forecasting），但实际上区间预测也是很重要的，具体来说有这三方面的作用：&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="时间序列" scheme="https://lukan217.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>code-server搭建指南</title>
    <link href="https://lukan217.github.io/2022/07/10/code-server%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/"/>
    <id>https://lukan217.github.io/2022/07/10/code-server%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/</id>
    <published>2022-07-10T10:17:42.730Z</published>
    <updated>2022-07-10T10:24:59.765Z</updated>
    
    <content type="html"><![CDATA[<p>虽然自己之前搞了一台服务器，也在服务器上<a href="https://zhuanlan.zhihu.com/p/384888122">部署了jupyter notebook</a>，但是仍有两个痛点没有解决：</p><ol type="1"><li>服务器部署了一些代码，有时候需要修改，通过vim直接修改是不现实的，因为没有补全高亮，改起来很麻烦，只能本地改好再上传上去</li><li>虽然部署了jupyter，能够实现一些简单的代码编辑需求，但是仅限于ipynb，其他文件无法编辑查看，并且补全功能十分鸡肋</li></ol><p>因此，为了能够在浏览器里面得到和本地编程一样丝滑的体验，最终决定部署一个code-server，也就是web版的vscode，实测体验和本地的vscode没有任何区别，效果如下：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1657447452075-701b98ea-8435-4da3-b86a-5181aac0797d.png#clientId=u7f73bf32-1eb1-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=841&amp;id=u78da3e34&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=1262&amp;originWidth=2560&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=83496&amp;status=done&amp;style=none&amp;taskId=ucf3b3635-f4a1-4f4e-ab2c-a5e8c1e0bd3&amp;title=&amp;width=1706.6666666666667" alt="image.png" /></p><p><a name="FavUM"></a></p><h1 id="搭建过程">搭建过程</h1><p><a name="wJ0di"></a></p><h2 id="准备">准备</h2><ol type="1"><li>一台云服务器</li><li>一个经过公安部备案的域名</li></ol><p>为什么要域名呢？因为我经常需要用jupyter notebook，但是这玩意在code-server里面由于安全性的原因需要通过https才能打开，但是通过ip地址是没办法走https的，因此就需要一个域名，而且是要经过备案的，不然没法访问。当然，如果你不需要用到jupyter notebook可以直接跳过这个步骤。<br />具体申请流程如下，以腾讯云为例：</p><ol type="1"><li>购买一个域名：<a href="https://console.cloud.tencent.com/domain">https://console.cloud.tencent.com/domain</a></li><li>为域名备案，走完整套流程大概要2周：<a href="https://console.cloud.tencent.com/beian">https://console.cloud.tencent.com/beian</a></li><li>最后一步，添加DNS解析：<a href="https://console.dnspod.cn/">https://console.dnspod.cn/</a></li></ol><p>主机记录可以填一个前缀，比如code，最后就是通过code.xxx.com来访问，记录值填写服务器公网ip，这样就在浏览器里面输入域名就会自动解析到服务器的地址了<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1657434255367-9c6b978a-c2c9-4976-86d6-8588c85b01d1.png#clientId=u7f73bf32-1eb1-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=355&amp;id=u5a4e0ffc&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=533&amp;originWidth=2034&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=93394&amp;status=done&amp;style=none&amp;taskId=uc9c8c788-7296-44dd-bfb1-93ba8620160&amp;title=&amp;width=1356" alt="image.png" /> <a name="BqcT9"></a></p><h2 id="code-server配置">code-server配置</h2><p>完成准备步骤后，就可以配置code-server了，安装步骤也很简单，依次输入以下命令就行了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://code-server.dev/install.sh | sh </span><br></pre></td></tr></table></figure><p>如果上面的命令因为墙的原因下载不了，就只能通过本地下载安装包，传到服务器，再手动安装，这里以ubuntu为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i code-server_4.5.0_amd64.deb</span><br></pre></td></tr></table></figure><p>然后输入命令行输入<code>code-server</code>, 会生成一个本地配置文件，ctrl+C关闭，再去改配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.config/code-server/config.yaml</span><br><span class="line">===============</span><br><span class="line">bind-addr: 0.0.0.0:8080 # 如果没域名需要改成这个</span><br><span class="line">auth: password</span><br><span class="line">password: 123456</span><br><span class="line">cert: false</span><br><span class="line">===============</span><br><span class="line">code-server</span><br></pre></td></tr></table></figure><p>这时候浏览器输入：公网ip:8080应该就能访问了 <a name="HBU8I"></a></p><h2 id="配置https访问">配置https访问</h2><p>完成以上的操作，code-server的基本配置就完成了，但是之前说过，这样是不完整的，因为没有域名，并且没有https，很多操作进行不了，所以建立弄一个备案好的域名，然后根据官网给的操作说明，配置nginx和用Let's Encrypt生成证书，依次进行以下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装nginx并配置</span></span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install -y nginx certbot python3-certbot-nginx</span><br><span class="line">vim /etc/nginx/sites-available/code-server</span><br><span class="line"><span class="meta">#</span><span class="bash"> 填入以下内容，域名记得改一下</span></span><br><span class="line">===========================================</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen [::]:80;</span><br><span class="line">    server_name mydomain.com;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">      proxy_pass http://localhost:8080/;</span><br><span class="line">      proxy_set_header Host $host;</span><br><span class="line">      proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">      proxy_set_header Connection upgrade;</span><br><span class="line">      proxy_set_header Accept-Encoding gzip;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">============================================</span><br><span class="line">sudo ln -s ../sites-available/code-server /etc/nginx/sites-enabled/code-server</span><br><span class="line"><span class="meta">#</span><span class="bash"> 为域名生成证书，最后那个是你的邮箱</span></span><br><span class="line">sudo certbot --non-interactive --redirect --agree-tos --nginx -d mydomain.com -m me@example.com</span><br></pre></td></tr></table></figure><p><a name="eaemE"></a></p><h2 id="配置守护进程">配置守护进程</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/code-server.service</span><br><span class="line"><span class="meta">#</span><span class="bash"> 输入以下配置</span></span><br><span class="line">=========================</span><br><span class="line">[Unit]</span><br><span class="line">Description=code-server</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=exec</span><br><span class="line">Environment=HOME=/root</span><br><span class="line">ExecStart=/usr/bin/code-server</span><br><span class="line">Restart=always</span><br><span class="line">=========================</span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后就可以通过以下命令来启动和关闭code-server服务了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> start code-server</span></span><br><span class="line">systemctl start code-server</span><br><span class="line"><span class="meta">#</span><span class="bash"> stop code-server</span></span><br><span class="line">systemctl stop code-server</span><br><span class="line"><span class="meta">#</span><span class="bash"> code-server status</span></span><br><span class="line">systemctl status code-server</span><br></pre></td></tr></table></figure><p>这样就基本配置成功了，之后再根据自己的需要装上插件，换下主题，就完全和本地的vscode没啥区别，可以随时随地在浏览器连接服务器进行编程了！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;虽然自己之前搞了一台服务器，也在服务器上&lt;a href=&quot;https://zhuanlan.zhihu.com/p/384888122&quot;&gt;部署了jupyter notebook&lt;/a&gt;，但是仍有两个痛点没有解决：&lt;/p&gt;</summary>
    
    
    
    <category term="计算机" scheme="https://lukan217.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="服务器" scheme="https://lukan217.github.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle HM推荐赛获奖方案总结</title>
    <link href="https://lukan217.github.io/2022/05/14/kaggle%20HM%E6%8E%A8%E8%8D%90%E8%B5%9B%E8%8E%B7%E5%A5%96%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2022/05/14/kaggle%20HM%E6%8E%A8%E8%8D%90%E8%B5%9B%E8%8E%B7%E5%A5%96%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/</id>
    <published>2022-05-14T09:39:02.476Z</published>
    <updated>2022-05-14T16:22:13.698Z</updated>
    
    <content type="html"><![CDATA[<p><a name="uOrxS"></a></p><h1 id="st-place-solution">1st place solution</h1><p>文档链接：<a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324070">1st place solution</a><br />整体框架：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652519977145-990e767f-fd60-4ced-bd79-d24e225fd1a3.png" alt="overview (1).png" /> <a name="agjGv"></a></p><h2 id="召回策略">召回策略</h2><ol type="1"><li>用户上次购买的商品</li><li>Item CF</li><li>属于同个product code的商品</li><li>热门商品</li><li>graph embedding：ProNE</li><li>逻辑回归：训练Logistic回归模型，从前1000个热门商品中检索到50~200个商品</li></ol><p>召回策略一共为每个用户召回了100-500个商品，使用HitNum@100来评估召回策略的质量，尽可能覆盖更多的正样本。 <a name="jbxxm"></a></p><h2 id="特征工程">特征工程</h2><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>类型</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>Count</td><td>user-item, user-category of last week/month/season/same week of last year/all, time weighted count…</td></tr><tr class="even"><td>Time</td><td>first,last days of tranactions…</td></tr><tr class="odd"><td>Mean/Max/Min</td><td>aggregation of age,price,sales_channel_id…</td></tr><tr class="even"><td>Difference/Ratio</td><td>difference between age and mean age of who purchased item, ratio of one user's purchased item count and the item's count</td></tr><tr class="odd"><td>Similarity</td><td>item2item的协同过滤分数, item2item(word2vec)的余弦相似度, user2item(ProNE)的余弦相似度</td></tr></tbody></table><p><a name="WKsy0"></a></p><h2 id="排序模型">排序模型</h2><p>5个lightgbm classifier + 7个catboost classifier<br />不同的分类器分别用不同的时间跨度以及召回数量的数据进行训练，如下图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652512638734-0abf8e61-2a80-4042-9547-38db5e32de91.png" alt="cv.png" /> <a name="rrbKI"></a></p><h2 id="cv策略">CV策略</h2><p>使用最后一周作为验证集 <a name="rZbAm"></a></p><h2 id="优化技巧">优化技巧</h2><ol type="1"><li>模型推理优化：使用TreeLite 来优化lightgbm推理的速度（快了2倍），caboost-gpu版本比lightgbm-cpu版本快了30倍</li><li>内存优化：类别特征用了labelencoder，并且使用了reduce_mem_usage函数</li><li>特征存储：将创建的特征保存为feather格式，方便使用</li><li>并行：将用户分为28组，在多个服务器上同时进行推理</li><li>机器：128g内存，64核CPU, TITAN RTX GPU（真特么有钱啊！） <a name="WQeOZ"></a></li></ol><h2 id="亮点">亮点</h2><ol type="1"><li><p>使用逻辑回归作为pre-ranker进行召回</p></li><li><p>模型融合，用了12个模型进行融合</p></li><li><p>各种优化技巧 <a name="Q561R"></a></p></li></ol><h1 id="nd-place-solution">2nd place solution</h1><p>文档链接：<a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324197">2nd place solution</a> <a name="tfTnq"></a></p><h2 id="召回策略-1">召回策略</h2><ol type="1"><li><p>热门商品，基于不同的维度进行召回：</p><ol type="1"><li>用户属性：如不同年龄、地域购买的热门商品（其中，基于地域postal_code的召回最为显著的提升了分数）</li><li>商品属性：如果用户购买了具有特定属性的商品，会寻找具有相同属性的热门商品</li><li>使用不同的时间窗口：1、3、7、30、90天。</li></ol></li><li><p>用户历史购买过的全部商品</p></li><li><p>使用MobileNet embedding计算的图像相似度及进行召回</p></li><li><p>graph embedding：random walk <a name="xCAkG"></a></p></li></ol><h2 id="特征工程-1">特征工程</h2><ol type="1"><li><p>用户基本特征：包括购买数量、价格、sales_channel_id</p></li><li><p>商品基本特征：根据商品的每个属性进行统计，包括：times, price, age, sales_channel_id, FN, Active, club_member_status, fashion_news_frequency, last purchase time, average purchase interval</p></li><li><p>用户商品组合特征：基于商品每个属性的统计信息，包括：num, time, sales_channel_id, last purchase time, and average purchase interval</p></li><li><p>年龄商品组合特征：每个年龄组的商品受欢迎程度。</p></li><li><p>用户商品回购特征：用户是否会回购商品以及商品是否会被回购</p></li><li><p>高阶组合特征：例如，预测用户下次购买商品的时间</p></li><li><p>相似度特征：通过各种手段计算商品与客户购买的商品的平均相似度 <a name="M5TgH"></a></p></li></ol><h2 id="排序模型-1">排序模型</h2><p>魔改的lightgbm ranker，使用lambdarankmap作为目标函数（从xgboost里面copy的代码），比lightgbm原生的lambdarank目标要好。 <a name="J9f3W"></a></p><h2 id="cv策略-1">cv策略</h2><p>2-3个月作为训练集，最后一周作为测试集。 <a name="GX3mX"></a></p><h2 id="亮点-1">亮点</h2><ol type="1"><li><p>使用MobileNet生成了图像的embedding特征</p></li><li><p>各种高阶特征：用户是否会回购商品以及商品是否会被回购、用户下次购买商品的时间等</p></li><li><p>使用lambdarankmap作为目标函数的lgb ranker <a name="G7QZT"></a></p></li></ol><h1 id="rd-place-solution">3rd place solution</h1><p>文档链接：<a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324129">3rd place solution</a><br />作者说他整体建模的套路都和其他人差不多，因此分享了两点他觉得跟别人不同但是有用的东西：</p><ol type="1"><li><p>召回策略特征：这个商品被哪种策略召回，以及被召回时的排名，加上这两个特征，以及将召回的数量从几十增加到上百，将LB分数从0.02855提高到0.03262，直接从银牌区进去了金牌区，此外，如果只增加召回的数量，而不添加增加召回的特征的话，CV分数非常低。</p></li><li><p>BPR 矩阵分解得到的用户-商品相似度特征：用户和商品的相似性特征对排序模型很重要。关于item2item相似性的特征，例如Buyd together计数和word2vec，在大多数竞争对手的模型中都很常用，这些特征也大大提高了我的分数，但最能改善我的模型的是通过BPR矩阵分解获得的user2item相似性。这个BPR模型是在目标周之前（每周训练一个BPR）使用<a href="https://implicit.readthedocs.io/en/latest/bpr.html">implicit</a>训练所有交易数据的。BPR相似性的auc约为0.720，而整个排序模型的auc约为0.806，其他单一特征的最佳auc约为0.680。最后，这个相似性特征将我的LB分数从0.03363提高到了0.03510，这将我从金牌区带到了奖品区。 <a name="fU9mA"></a></p></li></ol><h1 id="th-place-solution">4th place solution</h1><p>文档链接：<a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324094">4th place solution</a><br />整体思路：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652517925129-614ba42d-e10b-44aa-87ff-980f75d216a0.png" alt="arch.png" /> <a name="qL1xq"></a></p><h2 id="召回策略-2">召回策略</h2><ol type="1"><li>item CF</li><li>最近购买：用户最近购买的12个商品</li><li>热门商品：上周的热门商品</li><li>Two Tower MMoE：作者主要关注这个模型的训练，因为它不仅能够为用户生成任意多的候选商品，还能够创建用户-商品相似度特征，这是模型的架构：</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652518291522-78e85301-578d-4bbf-9d28-f7b84a34311a.png" alt="loss.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652518252440-b3e414e8-19fc-4429-ad96-38c68c168fb7.png" alt="item-tower.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1652518271421-f86f7bb6-b8d5-409a-9a52-ebd109add659.png" alt="user-tower.png" /><br />对于用户侧的tower，作者还用了一个门控网络，以确保用户塔可以通过使用不同的expert为最近的活跃客户和非活跃客户进行学习。 <a name="ZYVEp"></a></p><h2 id="冷启动">冷启动</h2><ul><li><p>用户冷启动：双塔MMoE可以使用用户基础特征为没有购买日志的用户生成候选商品。</p></li><li><p>商品冷启动：除了商品的基本特征，作者还用了图像和文本的特征</p><ul><li><p>文本：从商品描述中提取TF-IDF特征，使用SVD+K-Means对商品进行聚类。然后使用聚类的label作为特征。</p></li><li><p>图像：使用预训练的tf_efficientnet_b3_ns提取图像向量，使用PCA+K-均值聚类。然后使用聚类的label作为特征。 <a name="eXWZi"></a></p></li></ul></li></ul><h2 id="亮点-2">亮点</h2><ol type="1"><li><p>召回策略用了Two Tower MMoE</p></li><li><p>文本和图像特征用的是embedding过后再进行一次聚类的标签 <a name="BctOx"></a></p></li></ol><h1 id="th-place-solution-1">5th place solution</h1><p>文档链接：<a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324098">5th place solution</a> <a name="yjDMN"></a></p><h2 id="召回策略-3">召回策略</h2><ol type="1"><li>用户上一篮子的购买（以及和用户上次购买商品相同product code的商品），最近购买的商品</li><li>User CF</li><li>Item CF</li><li>word2vec</li><li>经常会被一起购买的商品</li><li>根据用户的属性（年龄、性别等）召回的热门商品</li></ol><p>关于召回的数量，作者使用一个正样本的比例作为阈值来控制的，比如把阈值设为0.05的话，然后某个策略召回100个商品，正样本比例为0.01，那么可以召回少一点的商品，比如50个，这样刚好可以使得正样本比例刚好卡在0.05. <a name="EANNj"></a></p><h2 id="embedding方法">Embedding方法</h2><ol type="1"><li><p>商品的图像信息：使用swin transformer来提取embedding</p></li><li><p>商品的文本信息：使用SentenceTransformer提取</p></li><li><p>tf-idf获取商品的embedding</p></li><li><p>word2vec获取商品的embedding <a name="V67L5"></a></p></li></ol><h2 id="特征工程-2">特征工程</h2><ol type="1"><li>用户特征</li><li>商品特征</li><li>用户-商品特征：如相似度特征，聚合的统计特征等</li></ol><p>对于第1部分和第2部分，可以进行计算并保存一次，然后与第3部分的特征合并。这种方法可以节省很多时间，尤其是在推理的时候。 <a name="iBcjc"></a></p><h2 id="模型">模型</h2><p>作者用了大量时间在召回策略的设计上，因此只用了lightgbm单模 <a name="pKWxi"></a></p><h2 id="亮点-3">亮点</h2><ol type="1"><li>召回策略非常丰富，一共用了21种</li><li>图像和文本的embedding，图像用了swin transformer，文本用了SentenceTransformer</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a name=&quot;uOrxS&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="比赛" scheme="https://lukan217.github.io/categories/%E6%AF%94%E8%B5%9B/"/>
    
    
    <category term="kaggle" scheme="https://lukan217.github.io/tags/kaggle/"/>
    
    <category term="推荐系统" scheme="https://lukan217.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>BERT原理总结</title>
    <link href="https://lukan217.github.io/2022/04/08/Bert%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2022/04/08/Bert%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/</id>
    <published>2022-04-08T08:10:03.489Z</published>
    <updated>2022-05-14T16:20:47.163Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做nlp相关的任务，发现无脑上bert就能达到很好的效果了，于是就去看了原论文，写篇文章好好总结一下吧！ <a name="YRyi1"></a></p><h1 id="背景">背景</h1><p>在计算机视觉领域，预训练已经被证明是行之有效的了，比如ImageNet，训练了一个很大的模型，用来分类1000种东西，然后底层的模型架构就能很好的捕捉到图像的信息了，就可以直接迁移到其他任务上，比如一个猫狗的二分类问题，就只需要把模型拿来微调，接一个softmax输出层，然后重新训练几个epoch就能达到很好的效果了。类似的预训练一个大模型然后拿来做迁移学习的思想也被用在了nlp上，语言模型的预训练用在下游任务的策略主要有两种：</p><ol type="1"><li>基于特征（feature-base）：也就是词向量，预训练模型训练好后输出的词向量直接应用在下游模型中。如ELMo，用了一个双向的LSTM，一个负责用前几个词预测下一个词，另一个相反，用后面几个词来预测前一个词，一个从左看到右，一个从右看到左，能够很好地捕捉到上下文的信息，不过只能输出一个词向量，需要针对不同的下游任务构建新的模型。</li><li>基于微调（fine-tuning）：先以自监督的形式预训练好一个很大的模型，然后根据下游任务的不同接一个输出层就行了，不需要再重新去设计模型架构，如OpenAI-GPT，但是GPT用的是一个单向的transformer，训练时用前面几个词来预测后面一个词，只能从左往右看，不能够很好的捕捉到上下文的信息。</li></ol><p>ELMo虽然用了两个单向的LSTM来构成一个双向的架构，能够捕捉到上下文信息，但是只能输出词向量，下游任务的模型还是要自己重新构建，而GPT虽然是基于微调，直接接个输出层就能用了，但是是单向的模型，只能基于上文预测下文，没有办法很好的捕捉到整个句子的信息。<br />因此，BERT（Bidirectional Encoder Representations from Transformers）就把这两个模型的思想融合了起来，首先，他用的是基于微调的策略，在下游有监督任务里面只需要换个输出层就行，其次，他在训练的时候用了一个transformer的encoder来基于双向的上下文来表示词元，下图展示了ELMo、GPT和BERT的区别：<br /><img src="https://cdn.nlark.com/yuque/0/2022/svg/764062/1649144748837-d0dd42fe-0a8b-4e29-820f-c9b923850db3.svg#clientId=u0872f9ad-41c6-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u00a83598&amp;margin=%5Bobject%20Object%5D&amp;originHeight=392&amp;originWidth=611&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=uf43e63ab-467c-46cb-ad61-51e39d67573&amp;title=" /><br />BERT很好的融合了ELMo和GPT的优点，论文中提到在11种自然语言处理任务中（文本分类、自然语言推断、问答、文本标记）都取得了SOTA的成绩。 <a name="HEjdw"></a></p><h1 id="核心思想">核心思想</h1><p>BERT的模型结构采用的是transformer的编码器，模型结构如下，其实就是输入一个<span class="math inline">\(n\times h\)</span>（n为最大句子长度，h为隐藏层的个数）的向量，经过内部的一些操作，也输出一个<span class="math inline">\(n\times h\)</span>的向量。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1649397385919-e4ce313b-f3d7-4dee-8c05-a1ee3e48c5f3.png" alt="image.png" /><br />根据模型的一些参数设置的不同，BERT又分为：</p><ul><li><span class="math inline">\(BERT_{BASE}\)</span>：transformer层12，隐藏层大小768，多头注意力个数12，总共1.1亿参数</li><li><span class="math inline">\(BERT_{LARGE}\)</span>：transformer层24，隐藏层大小1024，多头注意力个数16，总共3.4亿参数</li></ul><p>BERT主要的工作在于对于<strong>输入表示的改造</strong>以及<strong>训练目标的设计</strong>。 <a name="CaisL"></a></p><h2 id="输入表示">输入表示</h2><p>在自然语言处理中，有的任务的输入可能只需要一个句子，比如情感分析，但是有的任务的输入是需要一对句子的，比如自然语言推断，因此，为了使Bert能够用在更多的下游任务上，BERT的输入被设计为不仅可以输入一个句子，也可以输入一个句子对。<br />不管输入的是一个句子还是句子对，BERT的输入的第一个词元都是一个特殊的词元<CLS>，作为句子的开始，并且这个<CLS>在最后输出的表征中也有很重要的作用，对于两个句子，BERT用一个分隔符<SEP>，因此：</p><ul><li>对于一个句子，BERT的输入结构为：<CLS>句子<SEP></li><li>对于一个句子对，BERT的输入为：<CLS>句子1<SEQ>句子2<SEP></li></ul><p>由于注意力机制是无法捕捉到位置信息的，因此BERT还加了一个position embedding，这里的position embedding的参数是自己学出来的，用来加在每个词元上的token embedding。<br />并且，为了区分句子对，BERT又训练了一个两个Segment Embeddings，分别加在原来的两个句子对应的token embedding上。<br />因此，最后BERT的输入就是三个embedding相加的结果，如下图所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1649399414503-4c3e9e2a-5555-4d35-937b-d6f4eee065f2.png" alt="image.png" /> <a name="w8GyY"></a></p><h2 id="masked-language-model-mlm"><strong>Masked Language Model (MLM)</strong></h2><p>前面说到，之前的预训练模型都是单向的，也就是用前几个词来预测下一个词，这样有个缺陷就是无法捕捉整个句子的上下文信息。因此BERT采用了在输入时随机mask词元的方式，然后基于上下文，在输出层里面预测这些被mask的词元，其实这就是完型填空了，就像我们以前高中英语做的一样，要能够填空，那么就得对上下文的语义有一个比较深入的了解，因此bert最后训练出来的参数就能够很有效的表征整个句子的语义。<br />具体来说，输入的时候会会把一个句子中的词随机mask掉一部分，比如：“你笑起来真好看”变成“你<mask>起来真<mask>看”，然后还会记住这些被mask住的词的位置，然后再输出的地方找到这些词元的对应的表征，再接一个和词典大小一样的输出层，就可以预测这些位置上被<mask>掉的词是什么了，训练时使用的损失函数也使用交叉熵。<br />但是该遮掉多少词也是个问题，论文里给了一个15%的比例，在训练时将15%的词替换为用一个特殊的“<mask>”替换，不过在训练时可以这么做，在我们微调的时候可就没有<mask>词元了，因此BERT选择这样的设计：</p><ul><li>80%时间为特殊的“<mask>“词元（例如，“this movie is great”变为“this movie is<mask>”；</li><li>10%时间为随机词元（例如，“this movie is great”变为“this movie is drink”），这里的目的是为了引入一些噪声，有点像纠错了；</li><li>10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”） <a name="DFglo"></a></li></ul><h2 id="next-sentence-prediction-nsp"><strong>Next Sentence Prediction (NSP)</strong></h2><p>因为研究者想让bert还能够适应像自然语言推理这类的任务，因此还加入了另一个任务，也就是当输入的是一个句子对的时候，BERT会预测这两个句子在上下文中是否是相邻的，具体在训练时，就会有50%概率输入的句子对是相邻的，50概率输入的句子对是不相邻的，其实就是一个二分类任务，这里刚好用之前提到的句子开头那个<CLS>标记最终输出的隐藏层再接一个softmax二分类输出层就行了，然后用交叉熵来作为损失函数。<br />最终把MLM的损失函数和NSP的损失函数加起来就是BERT最终的损失了，可以用Adam来做优化。 <a name="y0agw"></a></p><h1 id="bert的使用">BERT的使用</h1><p>接下来主要讲讲BERT在各个任务上是怎么使用的，其实也就是接一个输出层啦。</p><ol type="1"><li>文本分类任务：和NSP类似，在<CLS>这个词元的输入顶部接一个softmax分类层</li><li>问答任务：输入一个文本序列，需要从这个序列中找到答案的位置，就是接两个输出层，一个用来代表答案开始的地方，一个用来代表答案结束的地方。</li><li>命名实体识别（NER）任务：输入一个文本，标记文本中每个词元属于什么类型，直接把每个词元的输出向量输入到一个分类层就行。</li></ol><p>具体在使用的时候，直接使用huggingface的<a href="https://huggingface.co/docs/transformers/index">🤗 Transformers</a>就行，里面内置了很多预训练模型，并且对于每个任务也都有很好的封装，使用成本很低。 <a name="EwtoE"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://arxiv.org/abs/1810.04805">[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained: State of the art language model for NLP | by Rani Horev | Towards Data Science</a></li><li><a href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html">14.8. 来自Transformers的双向编码器表示（BERT） — 动手学深度学习 2.0.0-beta0 documentation</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在做nlp相关的任务，发现无脑上bert就能达到很好的效果了，于是就去看了原论文，写篇文章好好总结一下吧！ &lt;a name=&quot;YRyi1&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://lukan217.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="自然语言处理" scheme="https://lukan217.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    <category term="深度学习" scheme="https://lukan217.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>树模型的特征选择-Boruta</title>
    <link href="https://lukan217.github.io/2022/03/12/%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Boruta/"/>
    <id>https://lukan217.github.io/2022/03/12/%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Boruta/</id>
    <published>2022-03-12T12:44:33.274Z</published>
    <updated>2022-05-14T16:24:34.122Z</updated>
    
    <content type="html"><![CDATA[<p>对于结构化数据建模，现在主流使用的模型是都是树模型，lightgbm、xgboost等，这些模型有一个很重要的特性就是可以输出特征重要性，可以用来指导特征工程，但是却不能直接用来做特征选择，这篇文章就先主要谈谈使用特征重要性来筛选特征的缺陷，然后介绍一种基于特征重要性改进的特征选择方法——Boruta。 <a name="oDFBy"></a></p><h1 id="使用特征重要性来筛选特征的缺陷">使用特征重要性来筛选特征的缺陷</h1><ol type="1"><li>特征重要性只能说明哪些特征在训练时起到作用了，并不能说明特征和目标变量之间一定存在依赖关系。举例来说，随机生成一大堆没用的特征，然后用这些特征来训练模型，一样可以得到特征重要性，但是这个特征重要性并不会全是0，这是完全没有意义的。</li><li>特征重要性容易高估数值特征和基数高的类别特征的重要性。这个道理很简单，特征重要度是根据决策树分裂前后节点的不纯度的减少量（基尼系数或者MSE）来算的，那么对于数值特征或者基础高的类别特征，不纯度较少相对来说会比较多。</li><li>特征重要度在选择特征时需要决定阈值，要保留多少特征、删去多少特征，这些需要人为决定，并且删掉这些特征后模型的效果也不一定会提升。</li></ol><p>正由于特征重要性存在的这些缺陷，所以一般来说，特征重要性只能用来指导特征工程，比如看排名前几的特征都有啥，之后可以怎么根据这几个特征进行交叉，但是是不能够用来作为特征选择的依据的。但是特征重要性也不是完全没有用，使用得当还是能够作为特征选择的手段的，比如Boruta和Null Importance的特征选择就是基于特征重要性来做的。 <a name="k1sKx"></a></p><h1 id="boruta">Boruta</h1><p>Boruta的名字来自斯拉夫神话中一个住在树上里的恶魔，专门吃贵族，大致含义就是，专门用来剔除树模型那些特征重要性看起来很大，但是实际上并没有用的特征。<br />Boruta的主要思想包含两个，阴影特征（shadow feature）和二项分布，下面一一阐述： <a name="Drf6l"></a></p><h2 id="阴影特征">阴影特征</h2><p>特征重要性的一个缺陷就是无论这些特征的效果如何，重要性都是在这些特征之间对比，就有可能出现矮个里面选高个的现象，那能不能让他们和随机生成的特征比呢，按理来说随机生成特征的重要性应该都很低，那么这样就有了一个基准，就可以识别出哪些特征是有用的了。<br />阴影特征的思想就是把原来所有特征的取值都打乱，打乱后的特征就叫做阴影特征（这里用打乱原来特征的取值而不是新生成特征一个好处就是就保留了原来特征的分布，而不用生成一个新的分布），然后把这些阴影特征加入到原来的数据集中进行训练，如果原始特征的特征重要性还不如阴影特征的话，那说明这个原始特征的效果还不如随机的，可以直接剔除，具体来说步骤如下：</p><ol type="1"><li>对于一个包含有m个特征的数据集，对于每个特征都会创建一份副本</li><li>将特征副本的取值打乱顺序，得到m个阴影特征</li><li>将m个阴影特征加入到原数据集中进行训练，输出特征重要性</li><li>观察m个阴影特征的特征重要性的最大值，将之与原始特征的重要性进行比较，如果原始特征的重要性还不如阴影特征的话，那么就说明这个原始特征是没有用的</li></ol><p>不过这样做还是有个问题，因为这样只做了一次实验，会不会有随机性在里面呢？碰巧某个阴影特征就是特别的强，因此需要做多次实验，才能保证结果更可靠，这就是Boruta的第二个思想，用迭代的方式来进行特征选择。 <a name="eXnle"></a></p><h2 id="二项分布">二项分布</h2><p>前面说到，需要做多次试验才能保证结果更可靠，那么做完多次试验后怎么判断某个特征的去留？假设做了20次实验，然后有三个变量，age、height和weight，在20次实验中，age都被保留了，height被保留了4次，而weight一次都没被保留，那么应该选择哪些变量保留？哪些变量剔除呢？<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1647071703892-4bc4ec90-ff61-44dd-9c8a-87aa81ddfbff.png" alt="image.png" /><br />这里就用到了二项分布，假设每个特征被保留和被剔除的概率都是0.5的话，就跟抛硬币一样，所以n次实验的概率遵从二项分布，就可以通过设置一个阈值（如<span class="math inline">\(p=0.01\)</span>)，把分布的两端截断，分为三个区域：</p><figure><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1647070157764-ea4e794d-f9f3-442c-92ef-61c84b48ef64.png" alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><ol type="1"><li>拒绝区域（红色）：落在这块区域的特征在大部分实验中都被剔除了，因此是无用特征，可以直接剔除</li><li>不确定区域（紫色）：落在这块区域的特征，有时候被剔除了，有时候又被保留，这时候就需要自行决定是否保留，算法默认保留</li><li>接受区域（绿色）：落在这块区域的特征，大部分实验中都被保留了，可以视为有用特征。</li></ol><p><a name="ghaYE"></a></p><h1 id="使用">使用</h1><p>Boruta原本是R的包，现在也有了Python实现，可以直接调包使用： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install boruta</span><br></pre></td></tr></table></figure> Bortuta使用了类sklearn的接口，用起来也很方便，理论上lightgbm、xgboost、catboost都可以放进Boruta里面，但是实操中有时候会报错，原因未明，但实际上用官方例子的随机森林就可以了： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> boruta <span class="keyword">import</span> BorutaPy</span><br><span class="line"></span><br><span class="line"><span class="comment"># load X and y</span></span><br><span class="line"><span class="comment"># NOTE BorutaPy accepts numpy arrays only, hence the .values attribute</span></span><br><span class="line">X = pd.read_csv(<span class="string">&#x27;examples/test_X.csv&#x27;</span>, index_col=<span class="number">0</span>).values</span><br><span class="line">y = pd.read_csv(<span class="string">&#x27;examples/test_y.csv&#x27;</span>, header=<span class="literal">None</span>, index_col=<span class="number">0</span>).values</span><br><span class="line">y = y.ravel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define random forest classifier, with utilising all cores and</span></span><br><span class="line"><span class="comment"># sampling in proportion to y labels</span></span><br><span class="line">rf = RandomForestClassifier(n_jobs=-<span class="number">1</span>, class_weight=<span class="string">&#x27;balanced&#x27;</span>, max_depth=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define Boruta feature selection method</span></span><br><span class="line">feat_selector = BorutaPy(rf, n_estimators=<span class="string">&#x27;auto&#x27;</span>, verbose=<span class="number">2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find all relevant features - 5 features should be selected</span></span><br><span class="line">feat_selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check selected features - first 5 features are selected</span></span><br><span class="line">feat_selector.support_</span><br><span class="line"></span><br><span class="line"><span class="comment"># check ranking of features</span></span><br><span class="line">feat_selector.ranking_</span><br><span class="line"></span><br><span class="line"><span class="comment"># call transform() on X to filter it down to selected features</span></span><br><span class="line">X_filtered = feat_selector.transform(X)</span><br></pre></td></tr></table></figure> <a name="GST3P"></a></p><h1 id="总结">总结</h1><p>总结来说，Boruta就是生成了随机的阴影特征加入到原数据中，并比较阴影特征和原始特征的重要性大小，然后多次迭代，最终根据二项分布来比较特征优于阴影特征的次数来决定是否保留或者剔除特征，这样筛选最后得到的特征都是对于模型的预测能够起到积极作用的特征，注意到这是能够起到积极作用，但是并不代表特征筛选后一定会使得预测的效果最好，不过根据自己的实验，使用Boruta之后的效果基本上都不亚于原来未筛选时的效果，并且训练速度也大大加快了。 <a name="KP7iR"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a">Boruta Explained Exactly How You Wished Someone Explained to You | by Samuele Mazzanti | Towards Data Science</a></li><li><a href="https://danielhomola.com/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method/">BorutaPy - Daniel Homola</a></li><li><a href="https://github.com/scikit-learn-contrib/boruta_py">scikit-learn-contrib/boruta_py: Python implementations of the Boruta all-relevant feature selection method.</a></li></ol><p><a href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a"></a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;对于结构化数据建模，现在主流使用的模型是都是树模型，lightgbm、xgboost等，这些模型有一个很重要的特性就是可以输出特征重要性，可以用来指导特征工程，但是却不能直接用来做特征选择，这篇文章就先主要谈谈使用特征重要性来筛选特征的缺陷，然后介绍一种基于特征重要性改进的特征选择方法——Boruta。 &lt;a name=&quot;oDFBy&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征选择" scheme="https://lukan217.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>特征选择的基本方法总结</title>
    <link href="https://lukan217.github.io/2022/03/12/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2022/03/12/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2022-03-12T12:44:33.272Z</published>
    <updated>2022-05-14T16:24:45.617Z</updated>
    
    <content type="html"><![CDATA[<p>做机器学习时往往会通过特征交叉来衍生出一系列的特征，那么如何来确保这些特征是有用的呢？太多的特征一方面会加重模型的负担，跑得很慢，另一方面无效的特征也会使得模型效果下降，因此就需要一些特征选择的方法来剔除无效的特征，这篇文章就主要总结下特征选择的几种基本思路。 <a name="LZb8F"></a></p><h1 id="img过滤法"><img src="https://pic4.zhimg.com/80/v2-05756baf02bd7a023f7b27842594bc2b_1440w.jpg" alt="img" />过滤法</h1><p>过滤法的思想就是不依赖模型，仅从特征的角度来做特征的筛选，具体又可以分为两种方法，一种是根据特征里面包含的信息量，如方差选择法，如果一列特征的方差很小，每个样本的取值都一样的话，说明这个特征的作用不大，可以直接剔除。另一种是对每一个特征，都计算关于目标特征的相关度，然后根据这个相关度来筛选特征，只保留高于某个阈值的特征，这里根据相关度的计算方式不同就可以衍生出一下很多种方法：</p><ol type="1"><li>Pearson相关系数</li><li>卡方验证</li><li>互信息和最大信息系数</li></ol><p>过滤法的优势在于效率非常快，不用跑模型就能很快筛选出一批的特征，不过缺陷是筛选出来的特征对于模型不一定是有用的，并且可能会有冗余特征，而且也不能考虑到特征之间的相互作用。 <a name="Fnzju"></a></p><h1 id="包装法">包装法</h1><p>与过滤法不同，包装法是根据训练模型的效果来筛选特征的，所以第一步就是先划分训练集和测试集，然后搜索一个最优的特征子集，使得模型在测试集上的指标表现最好。根据搜索方式的不同又可以分为以下三种类型：</p><ol type="1"><li>暴力搜索：穷举所有的特征子集，获取最好的一个，不过计算复杂度是指数级的，一般不会用</li><li>贪心搜索：分为前向搜索和后向搜索，就是每次增加一个或者减少一个特征，看模型的效果会不会变好，依此来选择特征是否保留，计量里面学的逐步回归就属于这种方法</li><li>启发式搜索：根据遗传算法、模拟退火、蚁群算法等启发式算法来搜索最优的特征子集</li></ol><p>包装法的优势在于选出的特征都是对于模型的效果有提升的，但是缺陷是需要训练很多次模型来评估特征的作用，效率低下，只能在特征和样本量相对较少的时候用一下。 <a name="hEZGe"></a></p><h1 id="嵌入法">嵌入法</h1><p>嵌入法的嵌入就是把特征的选择嵌入到模型的训练过程中，在模型训练完成之后便可得出哪些特征是有用的，哪些特征是没用的。这种方法就需要特定的模型才能做了，具体来说有两种：</p><ol type="1"><li>Lasso：通过控制惩罚项<span class="math inline">\(\lambda\)</span>的大小，可以将一些变量的系数压缩为0，就起到了特征选择的作用</li><li>树模型：随机森林和GBDT这些树模型，可以输出特征重要性，通过特征重要性可以大概知道哪些特征有用，哪些没用</li></ol><p>嵌入法的同时兼顾了包装法和嵌入法的优点，不过实际使用过程中，还是会有些问题，尤其是用特征重要性，之后更新下两篇文章，主要谈谈使用特征重要性作为特征选择的缺陷，以及两种基于特征重要性改进的特征选择方法：Boruta和Null Importance。 <a name="A8Flw"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature Selection)方法汇总 - 知乎</a></li><li>《美团机器学习实践》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;做机器学习时往往会通过特征交叉来衍生出一系列的特征，那么如何来确保这些特征是有用的呢？太多的特征一方面会加重模型的负担，跑得很慢，另一方面无效的特征也会使得模型效果下降，因此就需要一些特征选择的方法来剔除无效的特征，这篇文章就主要总结下特征选择的几种基本思路。 &lt;a name=&quot;LZb8F&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征选择" scheme="https://lukan217.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯优化基本原理总结</title>
    <link href="https://lukan217.github.io/2022/01/23/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2022/01/23/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/</id>
    <published>2022-01-22T17:50:03.926Z</published>
    <updated>2022-06-11T09:08:15.144Z</updated>
    
    <content type="html"><![CDATA[<p>贝叶斯优化（Bayesian Optimization），主要用来解决计算成本昂贵的黑盒优化问题，这种问题有着以下两个特点：</p><ol type="1"><li>目标函数<span class="math inline">\(f(x)\)</span>及其导数未知，否则就可以用梯度下降等方法求解</li><li>计算目标函数时间成本大，意味着像蚁群算法、遗传算法这种方法也失效了，因为计算一次要花费很多时间</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642518880682-ca49f801-c6dd-406d-965d-9ba3690c74a6.png#clientId=u3fe6f5be-444f-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=52&amp;id=SNVtL" alt="image.png" /><br />这种问题最典型的就是机器学习里面的超参数优化，使用的模型为 <span class="math inline">\(f\)</span>，超参数为输入的 <span class="math inline">\(x\)</span>，评估指标（MSE, AUC等）为输出的目标函数值，在这个场景下，很多机器学习的入门课程都会提到网格搜索和随机搜索，但是这两个其实本质上也是一种类似于穷举的方式，随便选取一组可能的<span class="math inline">\(x\)</span>，然后分别计算目标值，最后对比所有的结果得到最好的解，可以看出来这种求解是很低效的，因此，解决这种问题需要设计一种高效的算法，来在有限的时间里面找到一个相对不错的解，这就是贝叶斯优化。<br />贝叶斯优化，是一种使用贝叶斯定理来指导搜索以找到目标函数的最小值或最大值的方法，就是在每次迭代的时候，利用之前观测到的历史信息（先验知识)来进行下一次优化，通俗点讲，就是在进行一次迭代的时候，先回顾下之前的迭代结果，结果太差的<span class="math inline">\(x\)</span>附近就不去找了，尽量往结果好一点的<span class="math inline">\(x\)</span>附近去找最优解，这样一来搜索的效率就大大提高了，这其实和人的思维方式也有点像，每次在学习中试错，并且在下次的时候根据这些经验来找到最优的策略。 <a name="raVsx"></a></p><h1 id="贝叶斯优化过程">贝叶斯优化过程</h1><p>首先，假设有一个这样的函数<span class="math inline">\(c(x)\)</span>，我们需要找到他的最小值，如下图所示，这也是我们所需要优化的目标函数，但是我们并不能够知道他的具体形状以及表达形式是怎么样的。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862207488-61095c80-c151-4cb2-848d-74c44f23901f.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u10477eea" alt="image.png" /><br />贝叶斯优化是通过一种叫做代理优化的方式来进行的，就是不知道真实的目标函数长什么样，我们就用一个代理函数（surrogate function）来代替目标函数，而这个代理函数就可以通过先采样几个点，再通过这几个点来给他拟合出来，如下图虚线所示：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862350662-cfeab91b-ee4f-4395-8cae-1c9c1df4d8c1.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u93bcdc6a" alt="image.png" /><br />基于构造的代理函数，我们就可以在可能是最小值的点附近采集更多的点，或者在还没有采样过的区域来采集更多的点，有了更多点，就可以更新代理函数，使之更逼近真实的目标函数的形状，这样的话也更容易找到目标函数的最小值，这个采样的过程同样可以通过构建一个采集函数来表示，也就是知道了当前代理函数的形状，如何选择下一个<span class="math inline">\(x\)</span>使得收益最大。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862456555-21005371-dd1b-419e-9c9c-2c4e28d83ddf.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=uc7daab55" alt="image.png" /><br />然后重复以上过程，最终就可以找到函数的最小值点了，这大致就是贝叶斯优化的一个过程：</p><ol type="1"><li>初始化一个代理函数的先验分布</li><li>选择数据点<span class="math inline">\(x\)</span>，使得采集函数<span class="math inline">\(a(x)\)</span>取最大值</li><li>在目标函数 <span class="math inline">\(c(x)\)</span>中评估数据点<span class="math inline">\(x\)</span>并获取其结果 <span class="math inline">\(y\)</span></li><li>使用新数据<span class="math inline">\((x,y)\)</span>更新代理函数，得到一个后验分布（作为下一步的先验分布）</li><li>重复2-4步，直到达到最大迭代次数</li></ol><p>举个例子，如图所示，一开始只有两个点（t=2），代理函数的分布是紫色的区域那块，然后根据代理函数算出一个采集函数（绿色线），取采集函数的最大值所在的<span class="math inline">\(x\)</span>（红色三角处），算出<span class="math inline">\(y\)</span>，然后根据新的点<span class="math inline">\((x,y)\)</span>更新代理函数和采集函数（t=3），继续重复上面步骤，选择新的采集函数最大值所在的<span class="math inline">\(x\)</span>，算出<span class="math inline">\(y\)</span>，再更新代理函数和采集函数，然后继续迭代<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642870783116-4e643df2-ec8f-4b2c-84e2-f5d11c2cfac2.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u912a5632" alt="image.png" /><br />问题的核心就在于代理函数和采集函数如何构建，常用的代理函数有：</p><ol type="1"><li>高斯过程（Gaussian processes）</li><li>Tree Parzer Estimator</li><li>概率随机森林：针对类别型变量</li></ol><p>采集函数则需要兼顾两方面的性质：</p><ol type="1"><li>利用当前已开发的区域（Exploitation）：即在当前最小值附近继续搜索</li><li>探索尚未开发的区域（Exploration）：即在还没有搜索过的区域里面搜索，可能那里才是全局最优解</li></ol><p>常用的采集函数有：</p><ol type="1"><li><p>Probability of improvement（PI）</p></li><li><p>Expected improvement（EI）</p></li><li><p>Confidence bound criteria，包括LCB和UCB <a name="lYgNb"></a></p></li></ol><h1 id="可用的贝叶斯优化框架">可用的贝叶斯优化框架</h1><ol type="1"><li>BayesianOptimization：<a href="https://github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a></li><li>清华开源的openbox：<a href="https://open-box.readthedocs.io/zh_CN/latest/index.html">https://open-box.readthedocs.io/zh_CN/latest/index.html</a></li><li>华为开源的HEBO：<a href="https://github.com/huawei-noah/HEBO">https://github.com/huawei-noah/HEBO</a></li><li>Hyperopt：<a href="http://hyperopt.github.io/hyperopt/">http://hyperopt.github.io/hyperopt/</a></li></ol><p>个人觉得1和2就挺好用的 <a name="ZDkC6"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f">https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f</a></li><li><a href="https://chengfeng96.com/blog/2019/09/08/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/">https://chengfeng96.com/blog/2019/09/08/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</a></li><li><a href="https://zhuanlan.zhihu.com/p/459110020">https://zhuanlan.zhihu.com/p/459110020</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;贝叶斯优化（Bayesian Optimization），主要用来解决计算成本昂贵的黑盒优化问题，这种问题有着以下两个特点：&lt;/p&gt;</summary>
    
    
    
    <category term="最优化" scheme="https://lukan217.github.io/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="贝叶斯优化" scheme="https://lukan217.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="最优化" scheme="https://lukan217.github.io/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>一个简约的beamer模板</title>
    <link href="https://lukan217.github.io/2021/12/05/%E4%B8%80%E4%B8%AA%E7%AE%80%E7%BA%A6%E7%9A%84beamer%E6%A8%A1%E6%9D%BF/"/>
    <id>https://lukan217.github.io/2021/12/05/%E4%B8%80%E4%B8%AA%E7%AE%80%E7%BA%A6%E7%9A%84beamer%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-12-05T10:24:04.588Z</published>
    <updated>2022-05-14T16:25:34.024Z</updated>
    
    <content type="html"><![CDATA[<p>开组会啥的，要做ppt，但感觉还是用beamer比较方便，用默认的模板改了改，看着也还行 ​</p><p>项目链接：<a href="https://github.com/Smallviller/beamer_template">https://github.com/lukan217/beamer_template</a> <img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638592-6975409f-6254-4735-8062-0b436f49051b.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u2147a6de&amp;margin=%5Bobject%20Object%5D&amp;name=0001.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=82132&amp;status=done&amp;style=none&amp;taskId=u14532634-3b60-409f-9605-bb6f6f05ca2" alt="0001.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638348-e5ff3e5b-f79d-4782-8e55-0c30b903ba5f.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u3af9e72d&amp;margin=%5Bobject%20Object%5D&amp;name=0002.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=54464&amp;status=done&amp;style=none&amp;taskId=u06378bdd-14f5-4d70-8234-2de0e293ae7" alt="0002.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638612-1fe8e3f9-6a92-4e37-ae33-b5b6ab56f8e2.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u8c014218&amp;margin=%5Bobject%20Object%5D&amp;name=0003.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=93115&amp;status=done&amp;style=none&amp;taskId=u2ea290d5-3e0e-4325-8026-ef36ee6ca4f" alt="0003.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638621-2ea8a6c3-280f-42aa-b3c0-ad26cfaa4c76.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u36cfdf6d&amp;margin=%5Bobject%20Object%5D&amp;name=0004.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=78021&amp;status=done&amp;style=none&amp;taskId=u1217dd56-073b-4fa2-b583-b8fb2918500" alt="0004.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638656-448960af-454b-40ab-8d10-cd79c533bf18.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u3b90e5c0&amp;margin=%5Bobject%20Object%5D&amp;name=0005.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=91360&amp;status=done&amp;style=none&amp;taskId=u9030a713-073c-4dd9-8d48-af400e2ad07" alt="0005.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638958-0c02aa64-99e9-4ea3-ba8c-531ac03c087b.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u80084e0b&amp;margin=%5Bobject%20Object%5D&amp;name=0006.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=57523&amp;status=done&amp;style=none&amp;taskId=ua805a004-b4b3-4624-a5b3-87eea69d808" alt="0006.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639346-c7384931-218f-4b0b-b9f3-d022f11872fe.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=ufb0038af&amp;margin=%5Bobject%20Object%5D&amp;name=0007.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=82696&amp;status=done&amp;style=none&amp;taskId=ud7d2d344-949b-44c7-a352-376c052870f" alt="0007.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639446-6f42c330-ccdb-401e-93dc-4be4c74550e2.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u79b63874&amp;margin=%5Bobject%20Object%5D&amp;name=0008.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=70224&amp;status=done&amp;style=none&amp;taskId=u33f6e90c-dad6-4134-ab5e-d189eff53c1" alt="0008.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639692-a5053fac-411d-40d1-8501-6c7fe318e305.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=ud5f6369d&amp;margin=%5Bobject%20Object%5D&amp;name=0009.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=51952&amp;status=done&amp;style=none&amp;taskId=u6b9d7536-8999-465a-9817-ce762930acf" alt="0009.jpg" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;开组会啥的，要做ppt，但感觉还是用beamer比较方便，用默认的模板改了改，看着也还行 ​&lt;/p&gt;</summary>
    
    
    
    <category term="计算机" scheme="https://lukan217.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="beamer" scheme="https://lukan217.github.io/tags/beamer/"/>
    
    <category term="latex" scheme="https://lukan217.github.io/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle M5时间序列预测比赛的发现总结</title>
    <link href="https://lukan217.github.io/2021/11/27/Kaggle%20M5%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B%E7%9A%84%E5%8F%91%E7%8E%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2021/11/27/Kaggle%20M5%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B%E7%9A%84%E5%8F%91%E7%8E%B0%E6%80%BB%E7%BB%93/</id>
    <published>2021-11-27T08:52:00.501Z</published>
    <updated>2022-06-11T09:07:59.170Z</updated>
    
    <content type="html"><![CDATA[<p>谈到时序预测，2020年kaggle上举办的那场M5沃尔玛销量预测比赛肯定是最值得学习的，有人甚至直接根据这场比赛写了两篇论文（见文末）发在 International Journal of Forecasting上，来总结这场比赛用到的一些方法，这里就把论文中的一些关键发现列出来供学习参考。 <a name="Y2CJP"></a></p><h1 id="机器学习方法的优越性"><strong>机器学习方法的优越性</strong></h1><p>多年来，经验发现，简单的方法与复杂的或统计上复杂的方法一样准确。由于有限的数据可用性、算法的低效性、预处理的需要以及受限的计算能力，此前机器学习方法和统计学方法相比准确性都有所不足。M4是第一个发现两个ML方法明显比简单的统计方法更准确的预测竞赛，突出了ML方法对更准确预测的潜在价值。赢得M4竞赛的第一名是混合了RNN和指数平滑的混合方法，而排名第二的方法是使用XGBoost对标准时间序列预测方法产生的预测进行优化加权。尽管M4的两个获奖作品在本质上都是ML，但它们都建立在统计学、系列的特定功能上，同时也与四种统计学方法的简单组合有着差不多的准确性。然而，M5是第一个所有表现最好的方法都是 "纯 "ML方法，并且明显优于所有统计基准及其组合的比赛。LightGBM证明了它可以有效地用于处理大量相关的时间序列序列和外生变量，并减少预测误差。此外，像DeepAR和N-BEATS这样的深度学习方法，也显示出预测的潜力。 <a name="scDUJ"></a></p><h1 id="模型融合的价值"><strong>模型融合的价值</strong></h1><p>M5 Accuracy竞赛表明融合不同方法的预测，即使是相对简单的方法，也能从总体上提高精确度。M5 Accuracy竞赛的冠军采用了一个非常简单的简单平均融合，涉及6个模型，每个模型利用不同的学习方法和训练集。同样，亚军采用了5个模型的简单平均融合，每个模型对趋势有不同的估计，而第三名的方法，是43个NN的简单平均融合。排名14th, 17th, 21st, 24th, 25th和44th方法也使用了模型的简单平均融合。 在这些融合方法中，只有排名25的方法考虑了对单个方法的不平等加权融合。模型融合的价值也被竞争的baleline之间的比较所支持：指数平滑模型和ARIMA模型的融合比单个方法的表现更好，自上而下和自下而上的调和预测的融合则比自上而下和自下而上的表现都好。 <a name="KXkQA"></a></p><h1 id="时间序列交叉学习-的价值"><strong>"时间序列交叉学习 "的价值</strong></h1><p>在以前的M系列竞赛中，大多数时间序列都是不相关的，属于不同的频率和领域，而且时间上也不对齐。因此，尽管M4比赛中表现最好的两个参赛者都同时考虑了从多个序列中"交叉学习"，而不是一次一个系列，但他们的方法在实践中难以有效实施，也没有充分展示出该方法的潜力。相比之下，由于M5是由排列整齐、高度相关的时间序列组成的，而且是分层结构的，所以 "交叉学习 "更容易应用，与逐个时间序列训练的方法相比，取得了很好的效果。同时，除了可以提高准确率，"交叉学习 "意味着可以在多个时间序列上只使用一个模型，因为这些序列是一起训练的，而不用针对每个时间序列都训练一个模型，因此减少了整体的计算成本，缓解了与有限的历史观察相关的困难。基本上，M5中所有表现最好的50个方法都使用了 "交叉学习"，利用了数据集提供的所有信息。 <a name="R2rMu"></a></p><h1 id="获胜的方法和用于销售预测的baseline之间存在重大差异"><strong>获胜的方法和用于销售预测的baseline之间存在重大差异</strong></h1><p>M5 Accuracy竞赛考虑了24个通常用于销售预测应用的各种类型的baseline，包括传统的和最先进的统计方法、ML方法和他们的融合。但与这些baseline相比，获胜的作品明显提供了更准确的预测，并且在WRMSSE方面也平均高出20%以上。尽管较低的聚集水平上表现差不多，但结果仍清楚地表明了它们的优越性，并促使在ML预测方法领域进行更多的研究，这些方法可用于预测时间序列之间复杂的非线性关系，同时也能够方便的引入外生/解释变量。 <a name="HxvtW"></a></p><h1 id="从模型外部校正预测的价值"><strong>从模型外部校正预测的价值</strong></h1><p>当预测者利用外部信息、内部知识以及他们的专业知识来提高预测精度时，通常会使用预测调整。在M5 Accuracy竞赛中，一些表现最好的方法，如排名第2和第5的方法，以乘数的形式利用这种调整来提高ML模型的预测（即在原先的预测结果上乘以一个系数，如0.95-1.05来校正预测结果，这个实际上是因为lightgbm外推能力差，在有趋势的序列上容易高估预测或者低估预测）。尽管这些调整并不完全基于判断，而是基于最低聚合水平产生的预测与较高聚合水平的预测之间的分析进行调整，但这些调整被证明是有用的，能够帮助预测模型减少偏差，更好地说明在较高聚合水平上更容易观察到的长期趋势。 然后，这种调整的实际价值需要进一步调查，但在预测领域，调和在不同聚集水平产生的预测的概念并不新鲜，许多研究实证证明了它的好处，特别是当预测和来自完整层次的信息被利用时。 <a name="LOU5s"></a></p><h1 id="有效的cv交叉验证策略的价值"><strong>有效的CV（交叉验证）策略的价值</strong></h1><p>在处理复杂的预测任务时，采用有效的CV策略对于提高样本外预测的准确性、避免过拟合和减轻不确定性至关重要。采用这种策略的重要性在M5 Accuracy竞赛的结果中得到了证明，选择进行CV的时间段，验证时间窗口的大小，这些窗口的更新方式，以及用于衡量预测表现的标准，都是预测者必须考虑的一些因素。在M5精确度竞赛中，表现最好的四种方法和前50名提交的绝大多数都考虑了这样的一种CV策略，即至少使用最后四个28天长的可用的数据窗口来做交叉验证以评估预测精确度，从而对样本外的表现提供一个合理的近似。除了这个CV方案之外，第一名的方案还同时测量了他所开发的模型CV结果的平均值和标准偏差。根据他的验证，他的方法中的递归模型被发现平均比非递归模型更准确，但更不稳定。因此，他决定将这两个模型结合起来，以确保产生的预测既是准确和稳定。在评估预测方法时，必须考虑到预测误差的全部分布，特别是其尾部，这表明稳健性是实现高准确度的前提条件。我们希望M5的结果将鼓励在这一领域的更多研究，并有助于开发更强大的CV策略。 <a name="VfP42"></a></p><h1 id="外生解释变量的重要性"><strong>外生/解释变量的重要性</strong></h1><p>时间序列方法通常足以识别和捕捉其历史数据模式（水平、趋势和季节性），通过推断这种模式并产生准确的预测。然而，仅仅依靠历史数据的时间序列方法不能有效地说明节假日、特殊事件、促销活动、价格以及可能的天气的影响。在这种情况下，来自外生/解释变量的信息对于提高预测精度显得至关重要。在M5 Accuracy预测竞赛中，所有获奖作品都利用外部信息来提高其模型的预测性能。例如，monsaraida和其他团队发现，几个与价格相关的特征对于提高他们模型结果的准确性具有重要意义。此外，外生/解释变量的重要性也在几个简单统计学模型中得到的支持，例如，使用促销信息和特殊事件作为外生变量的指数平滑模型比普通的指数平滑模型精确度要高6%。ARIMA模型的情况也是如此，ARIMAX的精度比普通的ARIMA要高13%。 <a name="mDxVC"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://www.researchgate.net/publication/344487258_The_M5_Accuracy_competition_Results_findings_and_conclusions">The M5 Accuracy competition: Results, findings and conclusions</a></li><li><a href="https://www.researchgate.net/publication/346493740_The_M5_Uncertainty_competition_Results_findings_and_conclusions">The M5 Uncertainty competition: Results, findings and conclusions</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;谈到时序预测，2020年kaggle上举办的那场M5沃尔玛销量预测比赛肯定是最值得学习的，有人甚至直接根据这场比赛写了两篇论文（见文末）发在 International Journal of Forecasting上，来总结这场比赛用到的一些方法，这里就把论文中的一些关键发现列出来供学习参考。 &lt;a name=&quot;Y2CJP&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="比赛" scheme="https://lukan217.github.io/categories/%E6%AF%94%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="时间序列" scheme="https://lukan217.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>Git常用操作总结</title>
    <link href="https://lukan217.github.io/2021/08/29/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2021/08/29/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-28T18:31:42.115Z</published>
    <updated>2022-05-14T16:21:32.743Z</updated>
    
    <content type="html"><![CDATA[<p>最近工作上会用到Git，这里记录一下一些常用操作供参考！</p><h1 id="git配置">Git配置</h1><ol type="1"><li><p>安装Git：<a href="https://git-scm.com/">https://git-scm.com/</a></p></li><li><p>本地命令行生成密钥绑定GitHub账号 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 输入命令生成密钥对，替换成自己邮箱，然后一路回车</span></span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将生成的公钥打印出来复制，将这串文本复制粘贴到GitHub的Setting-&gt;SSH and GPG keys中</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 输入命令检查是否绑定成功,输入yes后，如果出现Hi,xxx!则绑定成功</span></span><br><span class="line">ssh -T git@github.com</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li><li><p>配置用户名和邮箱信息： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看配置信息，一开始为空</span></span><br><span class="line">git config --list</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 全局配置，对所有代码库生效</span></span><br><span class="line">git config --global user.name &quot;你的名字&quot;</span><br><span class="line">git config --global user.email &quot;你的邮箱&quot;</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 局部配置，只对当前的代码库有效</span></span><br><span class="line">git config --local user.name &quot;你的名字&quot;</span><br><span class="line">git config --local user.email &quot;你的邮箱&quot;</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置后，远程仓库提交的commit里对应的用户即为 user.name</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li></ol><h1 id="git基本概念">Git基本概念</h1><ol type="1"><li>本地仓库：本地仓库上存放所有相关的文件，具体可分为工作区、暂存区和仓库区，工作区即项目文件夹下不包含<code>.git</code>文件夹的所有文件，暂存区和仓库区则在<code>.git</code>文件夹下<ol type="1"><li>工作区：即我们工作的文件夹，在里面进行文件的增删改操作</li><li>暂存区：临时保存工作区上的改动，通过<code>git add</code>操作将工作区的修改同步到暂存区</li><li>仓库区：当执行<code>git commit</code>操作时，将暂存区上的所有变动同步到本地仓库</li></ol></li><li>远程仓库：GitHub/GitLab上保存的仓库，通过<code>git push</code>将本地仓库同步到远程仓库，也可以通过<code>git fetch/pull</code>将远程仓库同步到本地仓库</li></ol><figure><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1630159880578-b47a2dae-9236-4c17-8de3-f4d7155ac69f.png" alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h1 id="git基本操作">Git基本操作</h1><h2 id="创建版本库">创建版本库</h2><p>创建版本库有两种方式，一种是将本地的文件夹直接变成一个git仓库，另一种是直接将远程的仓库克隆到本地 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git init # 将本地文件夹变为一个git仓库</span><br><span class="line">git clone &lt;url&gt; #将远程仓库克隆到本地</span><br></pre></td></tr></table></figure></p><h2 id="修改与提交操作">修改与提交操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git add &lt;file&gt; # 将单个文件从工作区添加到暂存区</span><br><span class="line">git add . # 将所有文件添加到暂存区</span><br><span class="line">git commit -m &quot;messenge&quot; # 将暂存区文件提交到本地仓库</span><br><span class="line">git status # 查看工作区状态，显示有变更的文件。</span><br><span class="line">git diff # 比较文件的不同，即暂存区和工作区的差异。</span><br></pre></td></tr></table></figure><h2 id="远程操作">远程操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git push origin master # 将本地的master分支推送到远程对应的分支</span><br><span class="line">git pull  # 下载远程代码并合并，相当于git fetch + git pull</span><br><span class="line">git fetch# 从远程获取代码库，但不进行合并操作</span><br><span class="line"></span><br><span class="line">git remote add origin &lt;url&gt; # 将远程仓库与本地仓库关联起来</span><br><span class="line">git remote -v # 查看远程库信息</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="撤销与回退操作">撤销与回退操作</h2><p>撤销操作：当修改了工作区/暂存区的文件，但是还没有commit时，想要撤销之前的操作： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 场景1：当你改乱了工作区某个文件的内容，但还没有add到暂存区</span></span><br><span class="line">git checkout &lt;file&gt; # 撤销工作区的某个文件到和暂存区一样的状态</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 场景2：当乱改了工作区某个文件的内容，并且git add到了暂存区</span></span><br><span class="line">git reset HEAD &lt;file&gt; # 第1步，将暂存区的文件修改撤销掉</span><br><span class="line">git checkout &lt;file&gt; # 第2步，将工作区的文件修改撤销掉</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 场景3：乱改了很多文件，想回到最新一次提交时的状态</span></span><br><span class="line">git reset --hard HEAD # 撤销工作区中所有未提交文件的修改内容</span><br><span class="line"></span><br></pre></td></tr></table></figure> 回退操作：当已经进行了commit操作，需要回退到之前的版本： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD^ # 回退到上次提交的状态</span><br><span class="line">git reset --hard HEAD~n # 回退到n个版本前的状态</span><br><span class="line">git reset --hard HEAD commitid # 回退到某一个commitid的状态</span><br></pre></td></tr></table></figure></p><h1 id="git分支管理">Git分支管理</h1><p>git的最强大之处就在于分支管理了，具体有两种应用场景：</p><ol type="1"><li>多人协作：每个人都基于主分支创建一个自己的分支，在分支上进行开发，然后再不断将写好的代码合并到主分支</li><li>自己修复bug/增加feature：创建一个bug分支或者feature分支，写好代码后合并到自己的分支然后删除bug/feature分支 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git branch &lt;name&gt; # 创建分支</span><br><span class="line">git checkout &lt;name&gt; # 切换到某个分支</span><br><span class="line">git checkout -b newtest # 创建并切换到新分支，相当于同时执行了以上两个命令</span><br><span class="line">git merge &lt;name&gt; # 合并某个分支到当前分支中，默认fast forward</span><br><span class="line">git branch -a # 查看所有分支</span><br><span class="line">git branch -d &lt;name&gt; # 删除分支</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h1 id="git多人协作">Git多人协作</h1><p>多人协作在同一个分支上进行开发的工作模式：</p><ol type="1"><li>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</li><li>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并；</li><li>如果合并有冲突，则解决冲突，并在本地提交；</li><li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</li><li>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。<br /></li></ol><h1 id="参考">参考</h1><ol type="1"><li><a href="https://www.runoob.com/git/git-tutorial.html">Git教程 | 菜鸟教程</a></li><li><a href="https://www.liaoxuefeng.com/wiki/896043488029600">Git教程 - 廖雪峰的官方网站</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近工作上会用到Git，这里记录一下一些常用操作供参考！&lt;/p&gt;</summary>
    
    
    
    <category term="计算机" scheme="https://lukan217.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="git" scheme="https://lukan217.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>科大讯飞|线下商店销量预测挑战赛top7方案</title>
    <link href="https://lukan217.github.io/2021/08/21/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E7%BA%BF%E4%B8%8B%E5%95%86%E5%BA%97%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9Btop7%E6%96%B9%E6%A1%88/"/>
    <id>https://lukan217.github.io/2021/08/21/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E7%BA%BF%E4%B8%8B%E5%95%86%E5%BA%97%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9Btop7%E6%96%B9%E6%A1%88/</id>
    <published>2021-08-21T11:10:08.518Z</published>
    <updated>2022-05-14T16:23:20.895Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了科大讯飞的线下商店销量预测挑战赛，线上成绩0.66，最终排名第七，这里把自己的方案分享出来，欢迎大家交流讨论！代码和数据均已上传到GitHub：<br /><a href="https://github.com/Smallviller/KDXF_sales_forecast_competition">https://github.com/Smallviller/KDXF_sales_forecast_competition</a> <a name="hjSac"></a></p><h1 id="赛题说明">赛题说明</h1><p>比赛传送门：<a href="https://challenge.xfyun.cn/topic/info?type=offline-store-sales-forecast">https://challenge.xfyun.cn/topic/info?type=offline-store-sales-forecast</a> <a name="WNhyD"></a></p><h2 id="赛题任务">赛题任务</h2><p>给定商店销量历史相关数据和时间等信息，预测商店对应商品的周销量。 <a name="j2N5j"></a></p><h2 id="数据说明">数据说明</h2><p>训练集：33周的历史销量数据<br />测试集：34周的销量<br />数据字段：字段shop_id（店铺id）、 item_id（商品id）、week（周标识）、item_price（商品价格）、item_category_id（商品品类id）、weekly_sales（周销量）组成。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1629537615118-1f4a390d-5a87-4d37-8629-ffc0700e24ed.png" alt="image.png" /><br />可以发现这里的shop_id、item_id、week和item_category_id进行了脱敏处理，经过简单探索，发现：</p><ol type="1"><li><p>shop_id共有32个，item_id共有523个、item_category_id共有34个，shop和item是多对多的关系，通过shop*item可标识唯一商品。</p></li><li><p>item_price存在大量空值，比率达73%</p></li><li><p>weekly_sales大多偏低，集中在0和1，存在间歇性需求的问题 <a name="mQdqi"></a></p></li></ol><h1 id="特征工程">特征工程</h1><p>一般销量预测，特征工程主要从这几个方面入手：时间相关特征、历史销量相关特征、价格相关特征...不过这里的时间特征被脱敏了，用不到，所以特征工程主要从销量和价格入手。 <a name="mib5L"></a></p><h2 id="销量相关特征">销量相关特征</h2><ol type="1"><li><p>滞后特征：滞后 1-14周的销量</p></li><li><p>滑动特征：滑动2-14周销量的min/max/median/mean/std</p></li><li><p>类别encoding特征：每个item、shop、item_category、shop*item_category、shop*item销量的mean和std</p></li><li><p>类别滞后特征：每个item、shop、item_category滞后1-14周的销量 <a name="m6Ypo"></a></p></li></ol><h2 id="价格相关特征">价格相关特征</h2><ol type="1"><li><p>价格原始值：包含原始特征和填充特征，填充策略采用先向前填补再向后填补，最后没填补到的在用众数填补</p></li><li><p>类别encoding特征：每个item、shop、item_category、shop*item_category、shop*item价格的mean和std</p></li><li><p>价格差异特征：当前价格与shop、item、item_cat、shop_cat、shop_item的价格均值的差值</p></li><li><p>价格变动特征：当前价格与上周价格/上个月平均价格的差值 <a name="Ty2SW"></a></p></li></ol><h1 id="模型">模型</h1><p><a name="Fxz2m"></a></p><h2 id="模型-1">模型</h2><p>只使用了lightgbm <a name="JQtAZ"></a></p><h2 id="损失函数">损失函数</h2><p>训练的损失函数采用tweedie，由于存在间歇性需求的问题，很多商品的销量的销量为0，满足tweedie分布，因此采用tweedie作为损失函数效果要比mse要更好 <a name="Ls2BR"></a></p><h2 id="交叉验证策略">交叉验证策略</h2><p>由于时间序列的数据存在先后，只能用历史来预测未来，因此在交叉验证的时候就得格外小心，不能使用随机划分，因为这样会泄露未来的数据，但是时序也有自己的一套交叉验证方法，我这里使用了三折交叉。<br />使用三折交叉验证，建立三个lgb模型：</p><ol type="1"><li>模型1：训练集使用1-30周数据，验证集使用31周数据，早停100轮</li><li>模型2：训练集使用1-31周数据，验证集使用32周数据，早停100轮</li><li>模型3：训练集使用1-32周数据，验证集使用33周数据，早停100轮</li></ol><p>特征工程、模型的调参等都是基于这个交叉策略来做的，最后将这三个模型取简单平均。<br />为什么不用五折交叉？五折交叉融合的效果不太好，经试验3折融合的成绩是最好的 <a name="wELKW"></a></p><h2 id="后处理">后处理</h2><p>由于树模型无法捕捉到趋势，只能学习到历史的东西，不能外推，预测的时候就容易偏高或者偏低，所以提交的时候其实还试着给结果乘上了一个系数1.025，这也是kaggle上很多时序比赛用的一个trick，结果大概能提升0.005个点吧 <a name="okZbV"></a></p><h1 id="一些本地有效果但线上不能提分的尝试">一些本地有效果但线上不能提分的尝试</h1><ol type="1"><li><p>分shop、item、item_category建模，以及这几种方式得到结果的简单平均融合，按理来说在数据量足够的情况下，对每个类别分别建一个模型应该是比全部数据一起建模效果要好的，不过线上无提升</p></li><li><p>去掉部分重要度特征不高的特征后建模，用到的特征有159个之多，试着使用特征过滤的手段去掉部分无用特征，仍然是本地有提升，线上无提升</p></li><li><p>训练集删掉前15周的数据进行建模，由于构造了很多lag特征，导致了前15周一些特征都是空值的情况，试着把这部分数据删掉，并且越早的数据对于之后的预测越没用，所以按理删掉这些数据应该是能有所提升的，但是还是本地有提升，线上无提升</p></li><li><p>... <a name="Q9Xyh"></a></p></li></ol><h1 id="总结">总结</h1><p>第一次比较投入的去参加这种比赛，感觉还是蛮靠运气和一些trick的，最后怎么弄都上不了分，不知道瓶颈卡在哪了，或许对数据做更多的探索，以及换一些深度的模型能上分吧，再接再励！ <a name="f2Vxd"></a></p><h1 id="代码">代码</h1><p><a name="qjE5G"></a></p><h2 id="数据预处理">数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并训练测试</span></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/train.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/test.csv&#x27;</span>)</span><br><span class="line">df=pd.concat([train,test]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df=df.sort_values([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用来做滑动和滞后特征的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makelag</span>(<span class="params">data,values,shift</span>):</span></span><br><span class="line">    lags=[i+shift <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>)]</span><br><span class="line">    rollings=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">15</span>)]</span><br><span class="line">    <span class="keyword">for</span> lag <span class="keyword">in</span> lags:</span><br><span class="line">        data[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=values.shift(lag)</span><br><span class="line">    <span class="keyword">for</span> rolling <span class="keyword">in</span> rollings:</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_min&#x27;</span>]=values.shift(shift).rolling(window=rolling).<span class="built_in">min</span>()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_max&#x27;</span>]=values.shift(shift).rolling(window=rolling).<span class="built_in">max</span>()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_median&#x27;</span>]=values.shift(shift).rolling(window=rolling).median()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_std&#x27;</span>]=values.shift(shift).rolling(window=rolling).std()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_mean&#x27;</span>]=values.shift(shift).rolling(window=rolling).mean()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个item都做滞后和滑动特征</span></span><br><span class="line">df=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>]).apply(<span class="keyword">lambda</span> x:makelag(x,x[<span class="string">&#x27;weekly_sales&#x27;</span>],<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 价格填充特征，先用前一个值填补，再向后填补，最后没填补到的用那个item的价格众数填补</span></span><br><span class="line">df[<span class="string">&#x27;item_price_fill&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.ffill().bfill())</span><br><span class="line">df[<span class="string">&#x27;item_price_fill&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.fillna(x.mode()[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># 对于每个shop,item,item_cat,shop*item_cat,shop*item分别做价格和销量的mean/std encoding，</span></span><br><span class="line"><span class="keyword">for</span> func <span class="keyword">in</span> [<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;std&#x27;</span>]:</span><br><span class="line">    df[<span class="string">f&#x27;shop_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;category_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;item_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_cat_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_item_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;category_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_cat_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;item_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_item_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].transform(func)</span><br><span class="line"><span class="comment"># 价格差异特征，当前价格与shop、item、item_cat、shop_cat、shop_item的价格均值的差值</span></span><br><span class="line">df[<span class="string">&#x27;shop_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;item_price_diff&#x27;</span>]=df[<span class="string">&#x27;item_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;cat_price_diff&#x27;</span>]=df[<span class="string">&#x27;category_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;shop_cat_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_cat_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;shop_item_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_item_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line"><span class="comment"># 当前价格与上周价格的差值，当前价格与上个月价格均值的差值</span></span><br><span class="line">df[<span class="string">&#x27;week_price_diff&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x-x.shift(<span class="number">1</span>))</span><br><span class="line">df[<span class="string">&#x27;month_price_diff&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x-x.shift(<span class="number">1</span>).rolling(<span class="number">4</span>).mean())</span><br><span class="line"><span class="comment"># 销量的滞后特征，对于每个item、item_cat、shop的聚合平均值</span></span><br><span class="line"><span class="keyword">for</span> lag <span class="keyword">in</span> [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)]:</span><br><span class="line">    df[<span class="string">f&#x27;item_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">    df[<span class="string">f&#x27;cat_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">    df[<span class="string">f&#x27;shop_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df.to_pickle(<span class="string">&#x27;data.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p><a name="UicNz"></a></p><h2 id="模型-2">模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df=pd.read_pickle(<span class="string">&#x27;data.pkl&#x27;</span>)</span><br><span class="line"><span class="comment"># 三折交叉</span></span><br><span class="line">cvs=[<span class="number">32</span>,<span class="number">31</span>,<span class="number">30</span>]</span><br><span class="line">params = &#123;</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;tweedie&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;tweedie_variance_power&#x27;</span>:<span class="number">1.6</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">2</span>**<span class="number">7</span>-<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">50</span>,</span><br><span class="line">        <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.6</span>,</span><br><span class="line">        <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.6</span>,</span><br><span class="line">        <span class="string">&#x27;subsample_freq&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.015</span>,</span><br><span class="line">        <span class="string">&#x27;n_estimators&#x27;</span>:<span class="number">2000</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1024</span>,</span><br><span class="line">        <span class="string">&#x27;n_jobs&#x27;</span>:-<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;silent&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">    &#125;</span><br><span class="line">y_preds=[]</span><br><span class="line">scores=[]</span><br><span class="line"><span class="keyword">for</span> cv <span class="keyword">in</span> cvs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">10</span>+<span class="built_in">str</span>(cv)+<span class="string">&#x27;=&#x27;</span>*<span class="number">10</span>)</span><br><span class="line">    train=df[df[<span class="string">&#x27;week&#x27;</span>]&lt;cv]</span><br><span class="line">    val=df[df[<span class="string">&#x27;week&#x27;</span>]==cv]</span><br><span class="line">    test=df[df[<span class="string">&#x27;week&#x27;</span>]==<span class="number">33</span>]</span><br><span class="line">    X_train=train.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>])</span><br><span class="line">    y_train=train[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    X_test=test.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    y_test=test[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    X_val=val.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>])</span><br><span class="line">    y_val=val[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    model=lgb.LGBMRegressor(**params)</span><br><span class="line">    model.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_val,y_val)],eval_metric=[<span class="string">&#x27;mse&#x27;</span>],verbose=<span class="literal">False</span>,categorical_feature=[<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>],early_stopping_rounds=<span class="number">100</span>)</span><br><span class="line">    val_pred=model.predict(X_val)*<span class="number">0.995</span></span><br><span class="line">    mse=mean_squared_error(y_val,val_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;MSE: <span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line">    scores.append(mse)</span><br><span class="line">    y_pred=model.predict(X_test)</span><br><span class="line">    y_preds.append(y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;三折交叉的score<span class="subst">&#123;scores&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;三折交叉平均score<span class="subst">&#123;np.mean(scores)&#125;</span>&#x27;</span>)</span><br><span class="line">y_pred=np.zeros_like(y_pred)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> y_preds:</span><br><span class="line">    y_pred+=t*<span class="number">1</span>/<span class="number">3</span></span><br><span class="line">sample_submit = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/sample_submit.csv&#x27;</span>)</span><br><span class="line">sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>] = y_pred</span><br><span class="line">sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>] = sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>].apply(<span class="keyword">lambda</span> x:x <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>).values</span><br><span class="line">sample_submit.to_csv(<span class="string">&#x27;submit.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近参加了科大讯飞的线下商店销量预测挑战赛，线上成绩0.66，最终排名第七，这里把自己的方案分享出来，欢迎大家交流讨论！代码和数据均已上传到GitHub：&lt;br&gt;&lt;a href=&quot;https://github.com/Smallviller/KDXF_sales_forecast_competition&quot;&gt;https://github.com/Smallviller/KDXF_sales_forecast_competition&lt;/a&gt; &lt;a name=&quot;hjSac&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="比赛" scheme="https://lukan217.github.io/categories/%E6%AF%94%E8%B5%9B/"/>
    
    
    <category term="时间序列" scheme="https://lukan217.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="销量预测" scheme="https://lukan217.github.io/tags/%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归的交叉熵损失函数原理</title>
    <link href="https://lukan217.github.io/2021/08/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86/"/>
    <id>https://lukan217.github.io/2021/08/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86/</id>
    <published>2021-08-06T17:54:19.354Z</published>
    <updated>2022-05-14T16:23:26.955Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子两次面试，都被问到了逻辑回归的损失函数是什么，我知道是交叉熵，也很顺利的脱口说出了他的函数表达式，但是接下来被问到了为什么要用这个损失函数，我之前见过那张图，就是这个交叉熵函数的曲面是比平方损失函数（MSE）的曲面要陡峭，更方便梯度下降算法的迭代求解，但是再被往下深挖，问还有别的原因吗，这背后的存在的数学逻辑是什么？接着又被问了一堆的极大似然估计啥啥啥数理统计的东西，就有点说不出来了，所以查了一些资料，顺便写篇文章总结一下加深下理解。 <a name="lnsnU"></a></p><h1 id="交叉熵损失函数">交叉熵损失函数</h1><p>先来熟悉下他的定义和函数形式，交叉熵（Cross Entropy）损失函数，也被称为对数损失函数，logloss，表现形式如下：</p><p><span class="math display">\[L=-[y\log(\hat y)+(1-y)log(1-\hat y)]\\\]</span></p><p>这里的<span class="math inline">\(y\)</span>代表真实值，0或1，$y $代表预测值/估计值，值为一个概率，取值范围0~1，一般大于0.5我们就判定这个样本为1，小于0.5就把这个样本归为0。</p><p>从函数的形式上，我们可以看出，无论真实的<span class="math inline">\(y\)</span>取0或1，这个加号两边的取值必有一个为0，假设<span class="math inline">\(y=1\)</span>，那么此时<span class="math inline">\(L=-\log(\hat y)\)</span>，此时损失函数代表预测为1的概率取负，如果<span class="math inline">\(y=0\)</span>，那么<span class="math inline">\(L=-log(1-\hat y)\)</span>，此时损失函数代表预测为0的概率取负，那么问题就简单了，直观上来理解这个损失函数，就是，要使得每一个样本属于其真实值的概率最大化。</p><p>虽然直观上理解这个损失函数代表的意义没有问题，但是其是怎么推导出来的呢？这样的形式会有什么样的优点呢？这里就有两种方式来理解这个损失函数了，一个是从数理统计的极大似然估计出发，另一个是从KL散度的角度出发。</p><p><a name="9rx1K"></a></p><h1 id="从极大似然估计角度理解">从极大似然估计角度理解</h1><h2 id="极大似然估计">极大似然估计</h2><p>首先需要复习一下极大似然估计是什么玩意？这个东西虽然在本科的概率论和数理统计课程中就学过了，但是还是有那么一点一知半解。</p><p>要理解极大似然估计，就得先知道这个似然函数<span class="math inline">\(p(x|\theta)\)</span>的概念，这个比较容易和概率函数搞混，因为表达式都是：<span class="math inline">\(p(x|\theta)\)</span>，但实际上似然函数（likelihood function ）与概率函数（probability function）是完全不一样的两个东西。</p><p>如果<span class="math inline">\(p(x|\theta)\)</span>中<span class="math inline">\(\theta\)</span>是已知确定的，<span class="math inline">\(x\)</span>是变量的话，那么这个函数就叫做概率函数，他描述在给定的模型参数<span class="math inline">\(\theta\)</span>下，对于不同的样本点<span class="math inline">\(x\)</span>，其出现的概率是多少，比如对于身高的正态函数，给定参数均值170和标准差10，那么就可以计算出现身高为180的人的概率有多少。</p><p>反过来，如果<span class="math inline">\(p(x|\theta)\)</span>中<span class="math inline">\(x\)</span>是已知确定的，<span class="math inline">\(\theta\)</span>是变量的话，那么这个函数就叫做似然函数，他描述对于不同的模型参数<span class="math inline">\(\theta\)</span>，出现$x $这个样本的概率是多少，还是身高的那个例子，如果给定一个样本身高为180，那么就可以计算不同的均值和标准差参数组合下出现这个样本的概率。</p><p>那么，极大似然估计是什么意思呢？就是<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，</strong>举个例子，我给了一堆人的身高，这些样本都是独立同分布的，然后知道身高是符合正态分布的，我想要推出人群中身高的均值和标准差是多少，那么就可以通过遍历每一个参数值，然后根据似然函数算出每一个人身高对应的概率是多少，因为是这些人是独立同分布的，所以就可以通过把这些概率乘起来的方式，来计算出一个出现这些样本的概率，然后选取最大概率对应的那个均值和标准差，这个均值和标准差就是想要的结果了。 <a name="rH0z9"></a></p><h2 id="逻辑回归参数的极大似然估计">逻辑回归参数的极大似然估计</h2><p>了解了极大似然估计，接下来就可以说一下啊逻辑回归的参数是怎么通过极大似然估计来进行估计的了。首先，根据逻辑回归的计算公式，我们可以知道对应为1和0的样本的概率：</p><p><span class="math display">\[\begin{align*}P(Y=1|x)&amp;=\frac{e^{wx+b}}{1+e^{wx+b}}=p(x)\\P(Y=0|x)&amp;=\frac{1}{1+e^{wx+b}}=1-p(x)\end{align*}\]</span></p><p>然后就可以计算出现这些样本的似然函数，就是把每一个样本的概率乘起来：</p><p><span class="math display">\[L(w;b)=\prod_{i=1}^{n}[p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\]</span></p><p>但是这个形式是连乘的，并不好求，所以一般我们会把他取对数，转化为累加的形式，就得到对数似然函数：</p><p><span class="math display">\[L&#39;(w;b)=\sum_{i=1}^{n}[y_i\log(p(x_i))+(1-y_i)log(1-p(x_i))]\\\]</span></p><p>这时候呢，我们就可以通过最大化这个对数似然函数的方式来求得逻辑回归模型中的<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>，把上面的式子加个负号，就是通过最小化这个负对数似然函数来求得<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>，就可以通过梯度下降法来进行求解了。</p><p>可以发现，通过数理统计中的极大似然估计方法，也可以得到逻辑回归的损失函数。 <a name="8zkfK"></a></p><h1 id="从kl散度的角度理解">从KL散度的角度理解</h1><p>交叉熵是信息论里面的概念，要理解这里的交叉熵是怎么推出来的，就得先理解以下一个叫做KL散度（相对熵）的东西。</p><p>如果对于同一个随机变量<span class="math inline">\(X\)</span>有两个单独的概率分布<span class="math inline">\(p(X)\)</span>和<span class="math inline">\(q(X)\)</span>，那么我们就可以<strong>用KL散度来衡量这两个分布的差异</strong>：</p><p><span class="math display">\[D_{KL}(p||q)=\sum_{i=1}^{n}p(x_i)\log(\frac{p(x_i)}{q(x_i)})\\\]</span></p><p>我们将<span class="math inline">\(p(x)\)</span>定义为真实的概率分布，<span class="math inline">\(q(x)\)</span>定义为模型预测的概率分布，我们希望预测的概率分布与真实的概率分布差异越小越好，也就是使得KL散度越小越好，而<span class="math inline">\(p(x)\)</span>是在数据集确定之后就确定下来的了，所以我们只要使得<span class="math inline">\(q(x)\)</span>尽可能地接近<span class="math inline">\(p(x)\)</span>就可以了。</p><p>将这个KL散度的公式展开可以得到：</p><p><span class="math display">\[\begin{align*}D_{KL}(p||q)&amp;=\sum_{i=1}^{n}p(x_i)\log(\frac{p(x_i)}{q(x_i)})\\&amp;=\sum_{i=1}^{n}p(x_i)\log(p(x_i))-\sum_{i=1}^{n}p(x_i)\log(q(x_i))\\&amp;=-H(p(x))-\sum_{i=1}^{n}p(x_i)\log(q(x_i))\end{align*}\]</span></p><p>学过信息论的可能会知道，<span class="math inline">\(-\log(p(x))\)</span>代表的就是<strong>信息量</strong>，某一随机事件发生的概率越小，反映的信息量就越大，比如新冠疫情的发生，概率很小，但是蕴含的信息量就很大，而这个<span class="math inline">\(-\sum_{i=1}^{n}p(x)\log(p(x))\)</span>代表的就是信息量的期望，也就是<strong>信息熵</strong>，然后如果把这个<span class="math inline">\(log\)</span>里面的<span class="math inline">\(p(x)\)</span>换成另一个分布的概率<span class="math inline">\(q(x)\)</span>，也就是<span class="math inline">\(-\sum_{i=1}^{n}p(x)\log(q(x))\)</span>，这个就是<strong>交叉熵。</strong></p><p>所以根据上面那个展开的公式，就可以发现<strong>KL散度=交叉熵-真实分布的信息熵</strong>，而这个真实分布的信息熵是根据<span class="math inline">\(p(x)\)</span>计算得到的，而这个<span class="math inline">\(p(x)\)</span>是在数据集确定之后就确定下来的了，这一项就可以当成一个常数项，所以我们如果想让KL散度越小，只需要让交叉熵越小越好了，因此就可以直接将逻辑回归的损失函数直接定义为交叉熵。 <a name="pihcD"></a></p><h1 id="使用交叉熵作为损失函数的好处">使用交叉熵作为损失函数的好处</h1><p>从上面的两个角度，我们就可以理解为什么逻辑回归要用交叉熵来作为损失函数了，但是，使用交叉熵背后的数学逻辑是明白了，那么，反映到实际里面，交叉熵到底有着什么样的优越性呢？</p><p>这里使用之前自己上数据挖掘课程ppt里的一张图来说明这个问题，可以看到，交叉熵函数的曲面是非常陡峭的，在模型效果差的时候学习速度比较快，是非常有利于梯度下降的迭代的，所以逻辑回归里面使用交叉熵作为损失函数而不是使用均方误差作为损失函数，这个也可以通过求导的方式来证明，不过限于个人水平，这里就不展开了，具体可以间文末列出的的第三篇参考资料。<img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1625298163597-63b86d09-180b-4c0d-a6e3-ab1cab0f20d9.png" alt="image.png" /> <a name="I1fZO"></a></p><h1 id="总结">总结</h1><p>本文主要从两个角度——数理统计的极大似然估计以及信息论中的KL散度，来说明逻辑回归中交叉熵函数背后的数学逻辑，同时也简单说明了交叉熵函数在逻辑回归中相对于均方误差函数的优势。 <a name="7WUkf"></a></p><h1 id="参考">参考</h1><p><a href="https://mp.weixin.qq.com/s/LPfrzLCVBj3SUQAf9fnlmA">为什么逻辑回归的损失函数是交叉熵？</a></p><p><a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计</a></p><p><a href="https://zhuanlan.zhihu.com/p/35709485">损失函数|交叉熵损失函数</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前阵子两次面试，都被问到了逻辑回归的损失函数是什么，我知道是交叉熵，也很顺利的脱口说出了他的函数表达式，但是接下来被问到了为什么要用这个损失函数，我之前见过那张图，就是这个交叉熵函数的曲面是比平方损失函数（MSE）的曲面要陡峭，更方便梯度下降算法的迭代求解，但是再被往下深挖，问还有别的原因吗，这背后的存在的数学逻辑是什么？接着又被问了一堆的极大似然估计啥啥啥数理统计的东西，就有点说不出来了，所以查了一些资料，顺便写篇文章总结一下加深下理解。 &lt;a name=&quot;lnsnU&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="逻辑回归" scheme="https://lukan217.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    <category term="损失函数" scheme="https://lukan217.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>时间序列的多步预测方法总结</title>
    <link href="https://lukan217.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-06T17:54:19.202Z</published>
    <updated>2022-05-14T16:24:12.613Z</updated>
    
    <content type="html"><![CDATA[<p>在时间序列预测中，预测的horizon往往是一段时间，比如下一周的股票价格、销量、天气等等，但是，在将时间序列数据转化为有监督学习时，往往会构造很多特征，其中一个很重要的就是滞后值特征和滑动窗口统计特征，一旦加入这些特征，就会导致有监督学习的多步预测出现问题，比如，我需要构造了一个滞后一天的特征lag1，需要预测接下来两天的值，那么，第一天的是很好预测的，因为我有昨天的值，但是第二天的预测就有问题了，因为昨天的观测值是不知道的啊，在上一篇文章中，我提到了一个递归预测法，但这两天看了一下，其实解决这个问题的方法还不少，所以写篇文章总结下吧。 <a name="kfo8C"></a></p><h1 id="直接预测法">直接预测法</h1><p>直接预测法（Direct Multi-step Forecast Strategy），这种方法的思路呢就是，如果不能使用lag特征，那我干脆就不用了。这种方法的可操作空间还是挺大的，可以分为只使用1个模型，使用n个模型（n为需要预测的天数），使用1-n个模型。接下来详细说明下每一种方法。 <a name="bDO6i"></a></p><h2 id="只使用一个模型">只使用一个模型</h2><p>举个例子，现有7月10号-7月15号的数据，需要预测未来3天的销量，那么，我就不能用lag1和lag2作为特征，但是可以用lag3呀，所以就用lag3作为特征构建一个模型：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626280659659-db52b363-6f31-44e1-96a7-5d33e8b327f5.png#align=left&amp;display=inline&amp;height=224" alt="image.png" /><br />这种是只使用一个模型来预测的，但是呢，缺点是特征居然要构造到lag3，lag1和lag2的信息完全没用到，所以就有人提出了一种思路，就是对于每一天都构建一个模型。 <a name="bxmLJ"></a></p><h2 id="使用n个模型">使用n个模型</h2><p>这个的思路呢，就是想能够尽可能多的用到lag的信息，所以，对于每一天都构建一个模型，比如对于15号，构建模型1，使用了lag1，lag2和lag3作为特征来训练，然后对于16号，因为不能用到lag1的信息了，但是lag2和lag3还是能用到的，所以就用lag2和lag3作为特征，再训练一个模型2，17号的话，就只有lag3能用了，所以就直接用lag3作为特征来训练一个模型3，然后模型123分别就可以输出每一天的预测值了。<br /><strong><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626281132143-f39be164-ed9e-4fe7-815e-97fa8cd78b73.png#align=left&amp;display=inline&amp;height=227" alt="image.png" /></strong><br />这种方法的优势是最大可能的用到了lag的信息，但是缺陷也非常明显，就是因为对于每一天都需要构建一个模型的话，那预测的天数一长，数据一多，那计算量是没法想象的，所以也有人提出了一个这种的方案，就不是对每一天构建一个模型了，而是每几天构建一个模型。 <a name="yyygU"></a></p><h2 id="使用1-n个模型">使用1-n个模型</h2><p>还是上面那个例子，这次把数据改变一下，预测四天吧，有10号-15号的数据，构建了lag1-5的特征，需要预测16号-19号的数据，那么我们知道16号和17号是都可以用到lag2和lag3的特征的，那么为这两天构建一个模型1，而18号和19号是可以用到lag4和lag5的特征的，那么为这两天构建一个模型2，所以最后就是模型1输出16号和17号的预测值，模型2输出18号和19号的值。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626282065731-86deeff3-8259-4320-998c-5fc639e8b0a4.png" alt="image.png" /><br />可以发现，这样的话，我们虽然没有尽最大可能的去使用lag特征，但是，计算量相比于使用n个模型直接小了一半。这是<a href="https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216">kaggle M5比赛第四名</a>的思路。 <a name="M1d8v"></a></p><h1 id="递归预测法">递归预测法</h1><p>然后是递归预测法（Recursive Multi-step Forecast），不知道预测值对应的滞后值怎么办？就用之前的预测值当真实值呗！举个例子，有10号-15号的数据，构建了lag1特征，需要预测未来3天的销量，那么15号的lag1特征可以直接用14号的值，假设预测出来结果是150，那么，在16号，lag1的真实值也就是15号的值虽然不知道，但是可以直接用15号的预测值填充呀，依次类推，17号的lag1也可以直接用16号的预测值填充，这就是递归预测法。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626279980227-c5384d4d-b533-45e2-89d6-37a16c66b09d.png" alt="image.png" /><br />但是，这种方法有一个缺陷就是会造成误差累计，还是上面那个例子，假设我15号那天预测错了，那么16号那天的输入就也是错的，那用来预测就更错了啊，所以，使用这种方法的话，一旦预测出错就会越错越离谱，这种方法会有着较高的偏差。 <a name="p5e7m"></a></p><h1 id="直接-递归混合预测法">直接-递归混合预测法</h1><p>直接预测法使用到的lag信息少，并且需要建的模型多，方差较大，递归预测法只使用了一个模型，并且lag的信息也全用上了，但是容易造成误差累计，偏差较大。所以，有人把上面两种方法直接结合了起来，试图平衡方差和偏差，这里就叫直接-递归混合预测法吧，混合的方式还挺多的，我看到的就三种了。 <a name="4dR72"></a></p><h2 id="混合一">混合一</h2><p>同时使用直接法和递归法，分别得出一个预测值，然后做个简单平均，这个思路也就是采用了模型融合的平均法的思想，一个高方差，一个高偏差，那么我把两个合起来取个平均方差和偏差不就小了吗，这个方法是<a href="https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163684">kaggle M5比赛top1</a>用的解决方案。 <a name="HVhMA"></a></p><h2 id="混合二">混合二</h2><p>这种方法是这篇论文提出的：《Recursive and direct multi-step forecasting: the best of both worlds》，有兴趣可以自己去读下，大概说的就是先使用递归法进行预测，然后再用直接法去训练递归法的残差，有点像boosting的思想，论文花了挺大篇幅说了这种方法的无偏性，不过，这种方法也就是存在论文中，暂时没见到人使用，具体效果还不知道。<br /> <a name="XT4Vm"></a></p><h2 id="混合三">混合三</h2><p>简单来说就是使用到了所有的lag信息，同时也建立了很多模型，还是这个例子，首先用10号-14号的数据训练模型1，得到15号的预测值，然后将15号的预测值作为16号的特征，同时用10号-15号的数据训练模型2，得到16号的预测值，最后使用16号的预测值作为17号的特征，使用10号-16号的数据训练模型3，得到17号的预测值。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626279980227-c5384d4d-b533-45e2-89d6-37a16c66b09d.png#align=left&amp;display=inline&amp;height=228、" alt="image.png" /><br />这种方法说实话我不能很get到他的好处在哪，相比于递归预测法，不就是训练时多了几条数据吗？还是会有误差累计的问题吧，或许是我没有理解明白吧，<a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582">kaggle favorita-grocery第一名</a>的方案好像也使用的这个 <a name="mlt1S"></a></p><h1 id="多输出法">多输出法</h1><p>在传统的机器学习中，是无法实现多输出的，只能输出一个值，但是在深度学习的模型中，就可以通过调节输出神经元的个数，从而实现多输出的功能，还有一些是使用seq2seq结构的，深度这块的时间序列预测目前了解的比较少，这里不再展开了。 <a name="h7wpx"></a></p><h1 id="总结">总结</h1><p>目前针对时间序列预测的多步输出问题大概就这几种方法，其中针对机器学习的直接法、递归法还有直接-递归混合法，这几种方法在kaggle上都有应用，也没有说哪种方法就一定好，这个需要就具体问题具体分析，多尝试一下才能知道在某种问题上哪种方法表现更好。 <a name="mSH2v"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting/">4 Strategies for Multi-Step Time Series Forecasting</a></li><li><a href="https://zhuanlan.zhihu.com/p/308764952">时间序列多步预测的五种策略</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在时间序列预测中，预测的horizon往往是一段时间，比如下一周的股票价格、销量、天气等等，但是，在将时间序列数据转化为有监督学习时，往往会构造很多特征，其中一个很重要的就是滞后值特征和滑动窗口统计特征，一旦加入这些特征，就会导致有监督学习的多步预测出现问题，比如，我需要构造了一个滞后一天的特征lag1，需要预测接下来两天的值，那么，第一天的是很好预测的，因为我有昨天的值，但是第二天的预测就有问题了，因为昨天的观测值是不知道的啊，在上一篇文章中，我提到了一个递归预测法，但这两天看了一下，其实解决这个问题的方法还不少，所以写篇文章总结下吧。 &lt;a name=&quot;kfo8C&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="时间序列" scheme="https://lukan217.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>时间序列数据的特征工程总结</title>
    <link href="https://lukan217.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan217.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-06T17:54:19.175Z</published>
    <updated>2022-05-14T16:24:18.293Z</updated>
    
    <content type="html"><![CDATA[<p>当下时间序列预测的方法主要有三种吧，第一个是传统的时间序列预测方法，典型代表有ARIMA和指数平滑法；第二个是基于机器学习的方法，目前用的最多的是lightgbm和xgboost，在很多时序预测比赛前几名的方案都可以看到这两种方法；第三个是基于深度学习的方法，如RNN、LSTM等。现在传统时序预测的方法的预测精度都已经不如基于机器学习和深度学习的方法了，但是后者依赖于特征工程，特征调教的好的话是可以达到很高的预测精度的，因此，本文就总结下时间序列数据常见的特征工程方法。<br /></p><p>一个典型的时间序列数据，会包含以下几列：时间戳，时序值，序列的属性变量，比如下图，日期就是时间戳，销量就是时序值，如果是多序列的话可能还会有序列的属性变量，如城市、产品、价格等。<br /> <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1625930948808-5fa93c14-0cbd-4aa6-bcb3-72da3ce43176.png#align=left&amp;display=inline&amp;height=151" alt="image.png" /><br /> 因此，时间序列的特征工程也大多是基于这三个数据衍生出来的： <img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1625934882012-f1feb04c-b9a7-4dda-85db-53aef26727ca.jpeg" />接下来将一一展开： <a name="ayMJm"></a></p><h1 id="时间戳衍生的特征">时间戳衍生的特征</h1><p>时间戳虽然只有一列，但是也可以根据这个就衍生出很多很多变量了，具体可以分为三大类：时间特征、布尔特征，时间差特征 <a name="JLCjk"></a></p><h2 id="时间特征">时间特征</h2><ul><li>年</li><li>季度</li><li>月</li><li>周</li><li>天：一年、一月、一周的第几天</li><li>小时</li><li>分钟</li><li>... <a name="YHwlX"></a></li></ul><h2 id="布尔特征">布尔特征</h2><ul><li>是否年初/年末</li><li>是否月初/月末</li><li>是否周末</li><li>是否节假日</li><li>是否特殊日期</li><li>是否早上/中午/晚上</li><li>... <a name="8UvOr"></a></li></ul><h2 id="时间差特征">时间差特征</h2><ul><li>距离年初/年末的天数</li><li>距离月初/月末的天数</li><li>距离周末的天数</li><li>距离节假日的天数</li><li>距离特殊日期的天数</li><li>... <a name="wnRgL"></a></li></ul><h1 id="时序值衍生的特征">时序值衍生的特征</h1><p>因为时间序列是通过历史来预测未来，那么，这个时序值的历史数据，也就是当前时间点之前的信息就非常有用，通过他可以发现时间序列的趋势因素、季节性周期性因素以及一些不规则的变动，具体来说这部分特征可以分为三种：滞后值、滑动窗口统计和拓展窗口统计。 <a name="YXNqf"></a></p><h2 id="滞后值">滞后值</h2><p>也称lag feature，比如对于t时刻的数据，我们认为他是跟昨天的数据、上周同一天的数据、上个月同一天的数据、去年同期的数据是高度相关的，那么，我们就可以将t-1、t-7、t-30、t-365的数据用来做特征。<br />但是在使用滞后值作为特征时需要注意一点，就是当在进行多步预测的时候，如果预测的horizon超过了滞后的期数，那么这时候就得使用递归的方式，将先前预测的值作为特征，举个例子，使用滞后一期的值作为特征，当前时间点为2021-07-10，我要预测2021-07-11和2021-07-12的股票价格，那么2021-07-11的值是可以预测的，因为我有2021-07-10的数据，但是2021-07-12的数据就不行了，因为我没有2021-07-11的数据，所以这时候一种做法就是将先前2021-07-11的预测值直接作为特征的输入，对于这种预测就得一行一行来，预测一行，拿预测值作为输入，再预测一行，再得到预测值，再预测一行，以此类推... <a name="P4fWX"></a></p><h2 id="滑动窗口统计">滑动窗口统计</h2><p>除了使用原始Lag值作为特征，还可以使用先前时间观察值的统计信息作为特征，这种类型的特征叫做滑动窗口统计，Rolling Window Statistics。比如对于t时刻，我们可以取前七天的统计值作为特征，也就是将t-1~t-8这个时间段数据的平均数、中位数、标准差、最大值、最小值等作为特征，这里指定的window就是7，也可以根据需要指定14，30等，可以发现，上面说的滞后值特征其实就是一种特殊的滑动窗口，他的window=1，然后滑动窗口统计也是可以指定滞后的期数来衍生出更多的特征的，比如七天前那个时刻的前七天数据的统计量。<br />同理，在构造这种特征的时候，也需要注意一下在多步预测时可能出现的问题。 <a name="pxjRE"></a></p><h2 id="扩展窗口统计">扩展窗口统计</h2><p>另一种特征叫做扩展窗口统计（Expanding Window Statistics），其实也算是一种特殊的滑动窗口统计，不过他用来统计的数据是整个序列全部的数据，统计值可以是平均数、中位数、标准差、最大值、最小值等，这种特征一般是用在多序列建模，比如不同的股票价格，可能会有着不同的内在属性，在预测的时候用这个特征作为区分也是一种方式。 <a name="KCYYO"></a></p><h1 id="序列属性衍生的特征">序列属性衍生的特征</h1><p><a name="CpZXA"></a></p><h2 id="连续变量衍生">连续变量衍生</h2><p>一个序列可能会伴有多个连续变量的特征，比如说对于股票数据，除了收盘价，可能还会有成交量、开盘价等伴随的特征，对于销量数据，可能还会伴随有价格的特征。对于这种连续变量，可以直接作为一个特征，也可以像之前时序值衍生的特征那样做处理，或者也可以与先前的数据做差值，比如t时刻的价格减去t-1时刻的价格。但是一般这种连续变量使用不多，因为这些值在未来也很可能是不可知的，那怎么能当成造特征呢？比如我要预测明天股票的收盘价，要用成交量作为一个特征，但是我怎么知道明天的成交量呢？这又是一个预测问题了。 <a name="Ui5Tb"></a></p><h2 id="类别变量encoding">类别变量Encoding</h2><p>对于类别型变量，如果类别比较少，一般在机器学习里做的处理是one-hot encoding，但是如果类别一多，那么生成的特征是会很多的，容易造成维度灾难，但是也不能随便用label encoding，因为很多时候类别是不反应顺序的，如果给他编码成1、2、3、4、5，对于一些树模型来说，在分裂节点的时候可不管这些是类别型还是连续型，通通当作连续型来处理，这是有先后顺序的，肯定不能这么做。所以就有这么一种方式，就是和y做特征交互，比如预测销量，有一个特征是产品类别，那么就可以统计下这个产品类别下的销量均值、标准差等，这种其实也算是上面扩展窗口统计的一种。 <a name="J9fMZ"></a></p><h1 id="参考">参考</h1><ol type="1"><li>《美团机器学习实践》</li><li><a href="https://mp.weixin.qq.com/s/dUdGhWY8l77f1TiPsnjMQA">一度让我怀疑人生的时间戳特征处理技巧。</a></li><li><a href="https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/">Basic Feature Engineering With Time Series Data in Python</a></li><li><a href="https://blog.csdn.net/fitzgerald0/article/details/104029842">时间序列树模型特征工程汇总</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;当下时间序列预测的方法主要有三种吧，第一个是传统的时间序列预测方法，典型代表有ARIMA和指数平滑法；第二个是基于机器学习的方法，目前用的最多的是lightgbm和xgboost，在很多时序预测比赛前几名的方案都可以看到这两种方法；第三个是基于深度学习的方法，如RNN、LSTM等。现在传统时序预测的方法的预测精度都已经不如基于机器学习和深度学习的方法了，但是后者依赖于特征工程，特征调教的好的话是可以达到很高的预测精度的，因此，本文就总结下时间序列数据常见的特征工程方法。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="时间序列" scheme="https://lukan217.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="特征工程" scheme="https://lukan217.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>云服务器配置远程jupyter notebook环境</title>
    <link href="https://lukan217.github.io/2021/08/07/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/"/>
    <id>https://lukan217.github.io/2021/08/07/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/</id>
    <published>2021-08-06T17:54:19.076Z</published>
    <updated>2022-07-10T10:19:29.344Z</updated>
    
    <content type="html"><![CDATA[<p>去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：</p><ol type="1"><li>通过网址随时随地都能打开编程</li><li>配置了适合编程的主题色调</li><li>加入了插件补全功能</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624879339187-e21ea69c-5a13-4f9e-9a78-278e3a86edb6.png#height=365&amp;id=dR0gt&amp;" alt="image.png" /><br />前几天因为折腾自己的服务器环境给搞崩了，数据库出了点问题，所以只能重装系统，导致jupyter notebook又要重装一遍，然后几个月后服务器到期，估计又要重新配一遍环境，就索性写一篇教程，供自己日后和有需要的人参考。</p><h1 id="云服务器选购">云服务器选购</h1><p>首先需要选购一个云服务器，推荐腾讯云或者阿里云，有学生认证的话一年大概100左右，操作系统推荐是用目前主流的两个Linux发行版，ubuntu和cent OS，两个系统在一些安装软件的命令上会有小差异，我这里用的是ubuntu。</p><h1 id="安装anaconda">安装Anaconda</h1><p>在买好云服务器后，就通过ssh连接，就可以用命令行进行操作了，首先第一步是安装anaconda，先要下载anaconda的安装包，输入命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>下载好后直接安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898431857-7b2a0ec6-2970-4c6f-9af3-07dc336f9a3d.png#height=72&amp;id=DlHXa&amp;" alt="image.png" /><br />会弹出这样一个界面，直接一直回车，然后输入yes继续回车，等待安装完成即可，安装完成会有这样一个界面，就代表安装完成了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898994446-2e991619-5f16-4b5f-bd61-3292b2fc53f7.png#height=126&amp;id=P7Dy3" alt="image.png" /></p><h1 id="配置jupyter-notebook环境">配置jupyter notebook环境</h1><p>接下来就可以配置jupyter notebook环境了，首先需要生成一个配置文件，输入命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>因为服务器的安全性，配置远程访问是需要设置一个密码的，输入命令，生成密钥：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure><p>输入两次密码，这里就会生成一个密钥放在用户文件夹的.jupyter文件夹下，和刚刚的配置文件路径一样，这两个文件会自动关联起来，在修改配置文件的时候就不需要加跟密钥相关的命令了。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624899427258-a0a09979-4f70-4935-9933-d058534df757.png#height=52&amp;id=i9wwI" alt="image.png" /><br />接下来就可以直接修改刚刚生成的那个配置文件了，使用vim打开，输入命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>按键盘的i键进入编辑模式，直接在开头添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&#x27;*&#x27; # 代表任意ip是都可以访问jupyter</span><br><span class="line">c.NotebookApp.notebook_dir=&#x27;/home/ubuntu/jupyter&#x27; # notebook的工作目录，可以自己的实际情况修改，注意要确保目录存在</span><br><span class="line">c.NotebookApp.open_browser = False # 不打开浏览器</span><br><span class="line">c.NotebookApp.port =8888  #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure><p>按Esc键退出编辑模式，然后输入:wq保存即可。</p><h1 id="开启远程访问">开启远程访问</h1><p>我们在上一步中指定了端口为8888，也让所有ip都能够访问这个端口了，但是在云服务器中还需要把这个端口开启起来，以腾讯云为例，进入安全组中，添加入站规则，按如下设置，然后在出站规则里点击一键放通，入站规则和出站规则都需要配置好<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624900683679-df0724ff-dcb9-4091-bf70-b9cb926120f2.png#height=205&amp;id=p7ggP" alt="image.png" /><br />接下来就可以将jupyter notebook打开了，不过我们需要能够将notebook一直在后台挂着，所以这里就输入这个命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>这里nohup（no hang up）是不挂起的意思，用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行，最后面的<strong>&amp;</strong>是让命令在后台执行，终端退出后命令仍旧执行，&gt; jupyter.log 2&gt;&amp;1是输出日志的意思，把命令的输出和错误都写到jupyter.log这个文件中，方便监控。<br />接下来我们在浏览器中输入：服务器公网ip:端口号，即可访问jupyter，如图所示，再输入刚刚设置的密码就行了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903214506-5212ba31-e39b-497e-bbb4-200892853bb9.png#height=390&amp;id=y6mOt" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903977925-09aa1daf-f1f3-40e2-ad21-8d1f42269d66.png#height=229&amp;id=SPHn7" alt="image.png" /></p><h1 id="装代码补全插件与更换主题">装代码补全插件与更换主题</h1><p>在上一步中，我们已经配置好了一个可以远程访问的jupyter notebook，但是呢，这个notebook的主题是默认的，白色太亮眼不适合编程，而且，默认的jupyter notebook也没有补全代码的功能，所以就通过插件的方式来解决这两个问题。</p><h2 id="补全代码插件">补全代码插件</h2><p>依次执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions </span><br><span class="line">jupyter contrib nbextension install --user</span><br><span class="line">pip install jupyter_nbextensions_configurator</span><br><span class="line">jupyter nbextensions_configurator enable --user</span><br></pre></td></tr></table></figure><p>这样插件就装好了</p><h2 id="更换主题">更换主题</h2><p>首先安装jupyterthemes：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyterthemes</span><br></pre></td></tr></table></figure><p>jupyterthemes是一个为jupyter notebook设置主题的插件，可以在github上查看他们的使用手册，<br /> 这里推荐自己的一套配置方案，在命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jt -t chesterish -f roboto -fs 12 -ofs 105 -dfs 95 -lineh 150</span><br></pre></td></tr></table></figure><h2 id="重启jupyter-notebook">重启jupyter notebook</h2><p>如果你的notebook在运行中，就需要重启一下才能使得上一步的修改生效，首先找到运行jupyter notebook的进程id，然后杀掉这个进程，再重启就可以了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ps -aux|grep jupyter</span><br><span class="line">sudo kill -9 进程id</span><br><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>可以发现，现在界面已经跟刚才的不一样了：<br /></p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904807402-493d550d-ae8c-4963-82bf-69a99fff310e.png#height=435&amp;id=gz8pT" alt="image.png" /><br /></p><p>然后还需要在Nbextensions中开启下Hinterland，也就是我们的补全插件<br /> <br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624384565647-dab6f490-7909-4501-a903-1cab72df72e0.png?x-oss-process=image%2Fresize%2Cw_1404#height=722&amp;id=AIyRA&amp;originHeight=722&amp;originWidth=1404&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1404" /></p><p>然后就大功告成了，有一个养眼的界面和补全代码的功能，就可以随时随地都能用搭建好的这个环境写一些代码了</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904949851-5191b2d7-4550-423d-b53c-71f609b93a10.png#height=404&amp;id=QiYzW" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：&lt;/p&gt;</summary>
    
    
    
    <category term="计算机" scheme="https://lukan217.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="服务器" scheme="https://lukan217.github.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    <category term="jupyter notebook" scheme="https://lukan217.github.io/tags/jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title>GBDT梯度提升树算法原理</title>
    <link href="https://lukan217.github.io/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"/>
    <id>https://lukan217.github.io/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/</id>
    <published>2021-06-16T16:00:00.000Z</published>
    <updated>2022-07-10T10:18:44.967Z</updated>
    
    <content type="html"><![CDATA[<p>GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！</p><h1 id="gbdt核心思想">GBDT核心思想</h1><p>GBDT，全称为Gradient Boosting Decision Tree，顾名思义，分为两部分， Gradient Boosting和Decision Tree，具体来说，就是使用<strong>决策树</strong>为基学习器的<strong>梯度提升算法</strong>，因此分别了解下这两部分的原理就可以很好的学习GBDT的思想。</p><h2 id="decision-tree">Decision Tree</h2><p>先说决策树部分，由于在GBDT中，每次迭代都需要用到<strong>负梯度</strong>，求<strong>负梯度</strong>就需要用到连续值，因此GBDT用的决策树都是CART（classification and regression tree）回归树，而不是分类树，CART回归树一般使用的节点分裂准则为平均平方误差MSE，即在节点分裂时穷举每一个特征的每一个阈值，来寻找最优切分特征和最优切分点，衡量的方法是平方误差最小化，这部分算法在很多机器学习的书中都有涉及，这里不详细展开。</p><h2 id="gradient-boosting">Gradient Boosting</h2><p>在说Gradient Boosting之前，先讲下boosting算法，所谓的boosting，就是使用前向分布算法（Forward Stagewise Algorithm）来训练加法模型，这个加法模型表现如下：<br /><span class="math display">\[f_M(x)=\sum_{i=1}^{M}T_(x,\Theta)\\\]</span><br />这里<span class="math inline">\(f_M(x)\)</span>表示的就是经过M次迭代后最终得到的模型，而<span class="math inline">\(T_(x,\Theta)\)</span>代表的就是单颗CART回归树，这里的<span class="math inline">\(\Theta\)</span>代表树的参数，这个模型的意义就是每次训练一个弱学习器<span class="math inline">\(T(x,\Theta)\)</span>，经过M次之后给他们加总起来得到一个强的学习器<span class="math inline">\(f_M(x)\)</span>，那么怎么来训练这个加法模型呢？<br />我们的目标是使得最后的预测误差尽可能的小，但是预测集我们事先是不知道的，我们只能使得训练集的训练误差最小，那么如何使得这种加法模型的训练误差最小？有一个方法就是在每次训练模型的时候都拟合上一个模型预测值与真实值之间的差值<span class="math inline">\(y-\hat{y}\)</span>，也就是残差，然后在把这个模型加到原来的模型中，这样就可以使得更新后的模型的预测误差更小，这种方法也被称为前向分布算法，如果基学习器是线性回归的话就是前向分布回归，关于前向分布回归可以参考我上一篇文章：<a href="https://zhuanlan.zhihu.com/p/369732767">前向分步回归Forward Stagewise Regression原理及Python实现</a>，而这里我们用的基学习器是树模型，这种方法也被称为提升树（boosting tree），具体步骤如下：</p><ol type="1"><li>初始化<span class="math inline">\(f_0(x)=0\)</span></li><li>对于<span class="math inline">\(m=1,2,3...M\)</span><ol type="1"><li>计算残差：<span class="math inline">\(r_{mi}=y_i-f_{m-1}(x),i=1,2,3,..N\)</span></li><li>以残差<span class="math inline">\(r_{mi}\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></li><li>更新<span class="math inline">\(f_m(x)=f_{m-1}(x)+T_m(x,\Theta)\)</span></li></ol></li><li>经过M次迭代后得到最终的提升树模型：<span class="math inline">\(f_{M}(x)=\sum_{i=1}^{M}T(x,\Theta)\)</span></li></ol><p>提升树每次迭代都拟合上一次模型中所没有能够学习到的东西，也就是残差，这使得提升树是肯定比一般决策树要强的，但是呢，这种拟合残差的方式也会存在一定的缺陷，这个先留到后面说，因此，基于提升树的改进算法就诞生了，也就是GBDT。<br />从上述提升树算法中，我们可以发现，boosting的核心思想就是在每次训练模型的时候，都用新的模型来弥补之前模型的不足，使得模型越来越好，这实际上就是一个优化问题，提升树每次迭代的时候优化的就是残差，那有没有存在更好的优化方式呢？当然有，就是将这个模型的“不足”用损失函数的方式来表示，在每次迭代时优化这个损失函数就行了！<br />损失函数可以用<span class="math inline">\(L(y,f(x))\)</span>，这里先用一个通用的形式来表示，具体的损失函数可以是各种各样的，比如回归问题的MSE,MAE，分类问题的交叉熵，然后我们的目标就是使得这个损失函数最小，问题就来了，怎么优化这个损失函数使他变小呢？<br />根据凸优化的理论，梯度的方向是函数值增长最快的方向，那么负梯度的方向自然就是函数值下降最快的方向，所以根据负梯度就可以找到函数值最小的地方，那么，我们就可以通过拟合负梯度的形式来优化损失函数，也就是说，<strong>在每次训练一个新模型的时候，都拟合上一个模型的损失函数的负梯度值，然后把这个新模型加到原来的模型中，这样必然使得更新后的模型的损失函数更小</strong>，这个就是Gradient Boosting了，具体步骤如下：</p><ol type="1"><li><p>初始化：$f_0(x)=0 $</p></li><li><p>对于<span class="math inline">\(m=1,2,3...M\)</span></p><ol type="1"><li><p>计算负梯度：<span class="math display">\[-g_m(x_i)=-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\\\]</span></p></li><li><p>以负梯度<span class="math inline">\(-g_m(x_i)\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></p></li><li><p>更新<span class="math display">\[f_m(x)=f_{m-1}(x)+\rho T_m(x,\Theta)\]</span></p></li></ol></li><li><p>经过M次迭代后得到最终的提升树模型：<span class="math display">\[f_{M}(x)=\sum_{i=1}^{M}\rho T(x,\Theta)\]</span></p></li></ol><p>注意到这里对于每次迭代的决策树都加了一个系数<span class="math inline">\(\rho\)</span>，这个<span class="math inline">\(\rho\)</span>也被称为学习率（learning rate），为什么要加这个学习率呢？是因为如果将学习率设为1，就等于一下子学习全部的不足，会导致过拟合。据说在调参的时候，可以通过设置较小的学习率和较大的迭代次数以得到一个较优的模型，当然，这是以计算效率为代价的。<br /></p><h2 id="gbdt为什么使用负梯度而不是残差">GBDT为什么使用负梯度而不是残差？</h2><p>好了，现在可以说说为什么GBDT要使用负梯度而不是直接使用残差了，这也是我之前比较困惑的点，原因就是残差只是损失函数负梯度的一种特例，而采用负梯度更加通用，可以使用不同的损失函数。<br />为什么说残差只是一种特例呢？我们考虑下面这个损失函数：<br /><span class="math display">\[L(y,f(x))=\frac{(y-f(x))^2}{2}\\\]</span><br />这也就是平均平方误差MSE，然后他的负梯度就是：<br /><span class="math display">\[-[\frac{\partial L(y,f(x))}{\partial f(x)}]=y-f(x)\\\]</span><br />这不就是上面说的残差吗？也就是说，残差就等价于当损失函数采用MSE时的负梯度，而损失函数采用MSE其实会有一些缺点，比如对异常值敏感，所以在实际问题中我们有可能采用MAE或者更为折中的Huber损失函数以避免这一缺陷，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917120462-f7060d81-1511-4423-b70b-7d647b78711d.png#align=left&amp;display=inline&amp;height=368&amp;margin=%5Bobject%20Object%5D&amp;originHeight=490&amp;originWidth=728&amp;size=0&amp;status=done&amp;style=none&amp;width=546" /><br />并且呢，这些损失函数的负梯度其实也可以看作是残差的一个近似，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917260219-846072de-d101-46ff-9674-9907085d1ff0.png#align=left&amp;display=inline&amp;height=286" alt="image.png" /><br />而且，由于决策树容易过拟合的特点，我们通常也会采用一些正则化的手段控制模型的复杂度来改进损失函数，如控制树的深度等<br /><span class="math display">\[obj=L(y,f(x))+\Omega(\Theta)\\\]</span><br />这里的<span class="math inline">\(\Omega(\Theta)\)</span>是正则项，用来控制模型的复杂度，使得模型在保持良好的性能的同时尽可能简单，以防止过拟合，这也是xgboost相对于GBDT的优化手段之一。<br />综上，由于负梯度可以应付各种各样千奇百怪的损失函数，所以GBDT用负梯度来拟合明显相比于残差是有着巨大的优势的。</p><h1 id="总结">总结</h1><p>本文主要介绍了GBDT的核心思想，先引入基于加法模型和前向分布算法的提升树模型，提升树模型每次迭代的拟合的是残差，然后介绍了GBDT，GBDT每次迭代拟合的是负梯度，通过拟合负梯度使得每次迭代后的损失函数值都能下降最快，最后说明了为什么负梯度与残差的关联以及为什么拟合负梯度更好。</p><h1 id="参考">参考</h1><ol type="1"><li>《The Elements of Statistical Learning Data Mining,Inference,and Prediction》</li><li><a href="https://mp.weixin.qq.com/s/9SrciKW-nJJMA2BbTs5CLg">GBDT算法原理以及代码实现</a></li><li><a href="https://blog.csdn.net/shine19930820/article/details/65633436">『机器学习笔记 』GBDT原理-Gradient Boosting Decision Tree</a></li><li><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting</a></li><li><a href="https://www.zhihu.com/question/63560633">gbdt的残差为什么用负梯度代替？</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Ridge和Lasso回归与代码实践</title>
    <link href="https://lukan217.github.io/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/"/>
    <id>https://lukan217.github.io/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-05-22T16:00:00.000Z</published>
    <updated>2022-06-11T09:08:06.848Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。</p><h1 id="ridge-lasso回归">Ridge &amp; Lasso回归</h1><p>在传统的线性回归中，是使用最小二乘法来估计参数的，通过最小化残差平方和来估计参数的，这个在机器学习领域也被称为损失函数：<br /><span class="math display">\[LOSS_{ols}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2\\\]</span><br />这种最小二乘估计的方法被证明了具有最佳线性无偏估计（Best Linear Unbias Estimator, BLUE）的性质，所谓的最佳，就是方差最小，但这是在线性无偏估计的前提下，在有偏的情况下方差就不一定是最小了，设想一下，如果牺牲这个有偏的性质来使得方差变小呢，根据bias-variance trade-off，会不会有可能使得整体的预测误差进一步降低呢？<br />于是Ridge和Lasso的形式就被提出来了，通过牺牲传统ols回归中无偏的性质来使得方差降低，以寻求更低的预测误差，这两者的损失函数分别如下：<br /><span class="math display">\[LOSS_{Ridge}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}\beta_j^2\\\]</span><br /><span class="math display">\[LOSS_{Lasso}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|\\\]</span><br />可以发现，这两个损失函数呢，就是在原来的ols的损失函数上加了一个系数惩罚项，因为我们求解时是让损失函数最小，加了后面这个惩罚项呢，会使得系数变小，这个<span class="math inline">\(\lambda\)</span>就用来控制惩罚的力度，如果为0的话就和传统的线性回归没有差异了，如果是无穷大的话，那么所有的回归系数都会被弄到0，最后的所有的预测结果就是样本的均值了，但在实践中，我们可以通过交叉验证的方式调节<span class="math inline">\(\lambda\)</span>的大小，选取最优的惩罚力度，就可以使得最终的预测误差达到最小。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621696524735-840f5da7-5fe4-4149-80a7-13ef9c3cbec6.png#align=left&amp;display=inline&amp;height=249&amp;id=jCBob" alt="image.png" /><br />Ridge和Lasso这种加惩罚项的方式叫做正则化（Regularization），在机器学习的应用很广，比如神经网络中就有应用。因此，Ridge也被称为<span class="math inline">\(L_2\)</span>正则化，后者被称为<span class="math inline">\(L_1\)</span>正则化。<br />虽然两者的加的惩罚项看起来差不多，其实是有着非常大的区别的，具体表现为Lasso可以使得系数压缩到0，而Ridge则不会有这种效果，把系数压缩到0的话就可以起到降维和变量选择的作用，因此Lasso在高维的数据中表现更好。<br />那么为啥会有这样的差别呢，首先我们来看他们的惩罚项的形式，一个用的是平方的形式，另一个用的是绝对值的形式，我们把之前的那个损失函数转化成一个优化问题：<br /><span class="math display">\[Ridge: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}\beta_j^2 \le s\\Lasso: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}|\beta_j| \le s\\\]</span><br />假设只有两个系数，我们用几何的方式来表达这个优化问题，Ridge的约束条件是一个平方的形式，可行域就是一个圆，而Lasso的约束条件是绝对值的形式，可行域则是一个菱形，而目标函数在求解时，肯定是跟这个约束条件的可行域相切的，而Lasso由于他是一个菱形，那么他就更容易切到菱形的顶点，因此也会使得系数为0，而Ridge是一个圆，就不容易切到系数为0的地方，因此这就使得Lasso在压缩系数时会更倾向于压缩为0。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621697160270-8a07af21-ed57-4ad0-b2ca-de6eed23bcf8.png#align=left&amp;display=inline&amp;height=240&amp;id=hG528" alt="image.png" /></p><h1 id="代码实践">代码实践</h1><p>使用sklearn自带的波斯顿房价数据集做个试验，分别跑一遍Ridge和Lasso回归，并且通过交叉验证来选取<span class="math inline">\(\lambda\)</span>，将之与线性回归进行对比，结果如下：</p><table><thead><tr class="header"><th></th><th>MSE</th></tr></thead><tbody><tr class="odd"><td>线性回归</td><td>21.8977</td></tr><tr class="even"><td>Ridge回归</td><td>21.7536</td></tr><tr class="odd"><td>Lasso回归</td><td>21.8752</td></tr></tbody></table><p>可以发现，两者的预测效果较线性回归都有一定提升，其中Lasso回归提升较小，这是因为数据集的原因，只有13个变量，并且每个变量都make sense，因此效果就一般了，在高维的数据集中Lasso从理论上 讲应该就会有较好的表现了。<br />具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV,LassoCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">y_pred_lr = lr.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;线性回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lr, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ridge=RidgeCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>) <span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line">y_pred_ridge = ridge.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Ridge回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_ridge, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">lasso=LassoCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>)<span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">lasso.fit(X_train, y_train)</span><br><span class="line">y_pred_lasso = lasso.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Lasso回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lasso, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="总结">总结</h1><p>本文对Ridge和Lasso回归做了一个总结，并通过一个简单数据集做了实践。在写的同时发现需要再去看和学习的东西很多，一个流程下来对于算法原理的理解更加透彻了，这对于搭建自己的知识体系是很有帮助的，希望以后能够坚持学完一个新的东西就写篇总结。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="https://lukan217.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>前向分步回归Forward Stagewise Regression原理及Python实现</title>
    <link href="https://lukan217.github.io/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/"/>
    <id>https://lukan217.github.io/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-05-03T16:00:00.000Z</published>
    <updated>2022-05-14T16:24:00.099Z</updated>
    
    <content type="html"><![CDATA[<p>最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。</p><h1 id="算法原理">算法原理</h1><p>这个算法的思想与boosting类似，每次迭代时都挑选出一个最优的变量来拟合残差，具体步骤如下：</p><ol type="1"><li>首先将截距项<span class="math inline">\(\beta _0\)</span>设置为<span class="math inline">\(\bar{y}\)</span>，所有的自变量系数<span class="math inline">\(\beta\)</span>都设为0，残差项设置为<span class="math inline">\(r=y-\bar y\)</span></li><li>挑选出与残差项最相关的自变量<span class="math inline">\(x_j\)</span></li><li>更新<span class="math inline">\(\beta _j\)</span>的值：，其中<span class="math inline">\(\delta_j=\epsilon \times \text{sign}[\langle x_j,r \rangle]\)</span>，这个<span class="math inline">\(\text{sign}[\langle x_j,r \rangle]\)</span>代表相关性的正负，<span class="math inline">\(\epsilon\)</span>代表步长。再更新下残差项的值：<span class="math inline">\(r=r-\delta_j x_j\)</span></li><li>重复步骤2，3，直到达到最大迭代次数或者所有的变量都与残差项无关。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620114719031-b1f1c1e8-155e-4258-a114-0d63a13b6a42.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=259&amp;id=uaa9a13b9" alt="image.png" /> 这个算法的优点在于与Lasso回归有着异曲同工之妙，通过选择合适的迭代次数和步长，可以使得部分变量的系数压缩为0，就可以起到变量选择和降低方差的作用，因此在高维数据的场景下会有较好的表现，再偷一张《The Elements of Statistical Learning》的变量系数路径图来说明这一点，左图的横轴为Lasso的L1范式，右图的横轴为前向分布回归的迭代次数，可以看到，变量系数的压缩路径大体上是一致的。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620118711730-c7178912-4b0e-447d-8355-2bdae92fcc77.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=330&amp;id=uff065e10" alt="image.png" /></li></ol><h1 id="python实现">Python实现</h1><p>用波斯顿房价的数据集来做个测试，将迭代次数设为2000的时候，mse要略小于线性回归： <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620121931201-e7594c64-9878-47d3-a851-0285bf12f751.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=44&amp;id=j2U1Z" alt="image.png" /> 因为这个数据集只有13个变量，而且每个变量都很重要，所以前向分布回归的优势并没有很明显，不过通过调参效果还是可以比普通的线性回归好那么一点，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ForwardStagewise</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">0.01</span>, max_iter=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="comment"># 初始化两个参数，eps步长和max_iter迭代次数</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 将X，y转化为数组形式</span></span><br><span class="line">        y = np.asarray(y)</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X = (X - X_mean) / X_std</span><br><span class="line">        self.y_mean = np.mean(y) <span class="comment"># 截距项，也就是y的平均</span></span><br><span class="line">        residual = y - self.y_mean <span class="comment"># 初始化残差项</span></span><br><span class="line">        x_num = np.shape(X)[<span class="number">1</span>] <span class="comment"># 变量数</span></span><br><span class="line">        self.beta = np.zeros((x_num)) <span class="comment"># 用来存储每一次系数更新的数组</span></span><br><span class="line">        self.betas = np.zeros((self.max_iter, x_num))  <span class="comment"># 用来存储每一迭代的系数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            c_hat = <span class="number">0</span></span><br><span class="line">            sign = <span class="number">0</span></span><br><span class="line">            best_feat = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x_num):</span><br><span class="line">                c_temp = X[:, j].T.dot(residual) <span class="comment"># 用来表示x与残差项的相关性</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(c_temp) &gt; c_hat:</span><br><span class="line">                    c_hat = <span class="built_in">abs</span>(c_temp)</span><br><span class="line">                    sign = np.sign(c_temp)</span><br><span class="line">                    best_feat = j</span><br><span class="line">            self.beta[best_feat] += sign * self.eps <span class="comment"># 更新系数</span></span><br><span class="line">            residual -= (self.eps * sign) * X[:, best_feat] <span class="comment"># 更新残差项</span></span><br><span class="line">            self.betas[i] = self.beta</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 预测</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 先标准化</span></span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_test = (X - X_mean) / X_std</span><br><span class="line">        y_pred = X_test.dot(self.beta) + self.y_mean</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">    boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">    X = boston.data</span><br><span class="line">    y = boston.target</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1</span>)</span><br><span class="line">    fs = ForwardStagewise(eps=<span class="number">0.01</span>, max_iter=<span class="number">2000</span>)</span><br><span class="line">    fs.fit(X_train, y_train)</span><br><span class="line">    y_pred = fs.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;前向逐步回归MSE：<span class="subst">&#123;mean_squared_error(y_pred, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line">    lg = LinearRegression()</span><br><span class="line">    lg.fit(X_train, y_train)</span><br><span class="line">    y_pred_lg = lg.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;线性回归回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lg, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="总结">总结</h1><p>前向分布回归和Lasso回归本质上其实差不多，而且两者好像都是最小角回归（Least angle regression）的一个变种，具体可以参见ESL这本书（太难了我看不懂），这两张回归算法都能起到压缩系数和变量选择的作用，但是前向分布回归的计算效率比较差，所以Lasso似乎更为人熟知，不过前者为我们学习boosting相关算法提供了一个不错的切入点。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://lukan217.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="https://lukan217.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    <category term="boosting" scheme="https://lukan217.github.io/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（二）</title>
    <link href="https://lukan217.github.io/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://lukan217.github.io/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2020-12-10T16:00:00.000Z</published>
    <updated>2022-05-14T16:23:47.504Z</updated>
    
    <content type="html"><![CDATA[<p>书接上文：<a href="https://zhuanlan.zhihu.com/p/329185040">爬取厦门58同城二手房数据进行数据分析（一）</a> 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。</p><h1 id="数据分析及可视化">数据分析及可视化</h1><h2 id="数据预处理">数据预处理</h2><p>首先导包，由于<code>seaborn</code>画图不支持中文显示，因此还需要加几行代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 中文字体设置-黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line">sns.<span class="built_in">set</span>(font=<span class="string">&#x27;SimHei&#x27;</span>)  <span class="comment"># 解决Seaborn中文显示问题</span></span><br></pre></td></tr></table></figure> 读入数据，删除不需要分析的字段，以及删除存在缺失值的数据： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">data = data.drop(columns=[<span class="string">&#x27;Unnamed: 0&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;产权年限&#x27;</span>, <span class="string">&#x27;location2&#x27;</span>])</span><br><span class="line">data = data[data[<span class="string">&#x27;location1&#x27;</span>] != <span class="string">&#x27;厦门周边&#x27;</span>] <span class="comment"># 删除厦门周边的数据</span></span><br><span class="line">data = data.dropna()</span><br><span class="line">data</span><br></pre></td></tr></table></figure> 最终得到的数据像这样子，去除缺失值后一共749行*16列： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620860545-327f70ad-5f36-4bb7-bb43-d88b1d34f9b1.png" /></p><p>为了方便后续的工作，我们在将数据做一些简单的处理： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;室&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;厅&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;卫&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>]))</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>] = data[<span class="string">&#x27;均价&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;房本面积&#x27;</span>] = data[<span class="string">&#x27;房本面积&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;建筑年代&#x27;</span>] = data[<span class="string">&#x27;建筑年代&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;总楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>:-<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;所在楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">&#x27;小区均价&#x27;</span>] = data[<span class="string">&#x27;小区均价&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;物业费&#x27;</span>] = data[<span class="string">&#x27;物业费&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;绿化率&#x27;</span>] = data[<span class="string">&#x27;绿化率&#x27;</span>].apply(<span class="built_in">float</span>)</span><br><span class="line">data[<span class="string">&#x27;车位信息&#x27;</span>] = data[<span class="string">&#x27;车位信息&#x27;</span>].apply(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure></p><h2 id="单变量可视化">单变量可视化</h2><p><strong>价格分布</strong></p><p>厦门市的房价总体来说还是非常贵的，一平方米平均要四万多，一套下来得四百多万，买不起买不起 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">&#x27;均价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>].mean()</span><br><span class="line">sns.distplot(data[<span class="string">&#x27;总价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;总价&#x27;</span>].mean()</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607617912033-306d3e0a-325a-4ae7-bbff-1d91d715aa58.png" /></p><p><strong>房屋区域分布</strong></p><p>有将近一半的二手房都在岛内（思明和湖里)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;位置1&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607619988435-7f18f6b6-fa96-4fa9-b889-e04cedf4e344.png" /> <strong>房屋朝向分布</strong></p><p>选取前五种最受欢迎的房屋朝向，可以看出，有2/3的房子都是南北朝向：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋朝向&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620480819-0821a116-4e07-45f8-90cb-dcbe6a31ce44.png" /></p><p><strong>房屋户型分布</strong></p><p>同样选取前五种最受欢迎的房屋朝向，可以发现3室2厅2卫的户型最受欢迎：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋户型&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620697989-2b958314-3805-4be7-8094-2e0237bc6f5d.png" /></p><p><strong>装修情况分布</strong></p><p>二手房基本上都是装修好了的，只有不到10%的是毛坯（为啥二手房还有毛坯的？）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;装修情况&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621101526-c6d1f02d-3bc1-4008-a9f0-7dbc1dbba179.png" /></p><h2 id="多变量间关系及可视化">多变量间关系及可视化</h2><p><strong>地域与房价</strong></p><p>画出各个区域的每平方米价格的箱型图，果然，岛内的房价更可怕了，思明区接近6万/平米，更有12万/平米的天价房，湖里区也接近5万/平米，就算在同安和翔安这两个鸟不拉屎的地方一平米也要两万多了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(data=data, x=<span class="string">&#x27;位置1&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621274809-4304e87f-f8bf-4720-b874-7cf6192373e5.png" /> 地域与其他变量 将数据做一个聚合，取平均，可以发现，岛内的房子都比较老，大概都在2000年上下（因为没地方可建了吧)，而岛外基本上都在2010年左右，而且岛内的房子就只有十三四层，而岛外的房子有二十层左右，面积也相对来说比岛内的小一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=[<span class="string">&#x27;位置1&#x27;</span>])[<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;房本面积&#x27;</span>,<span class="string">&#x27;建筑年代&#x27;</span>,<span class="string">&#x27;总楼层&#x27;</span>].mean()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621696738-bc35f9ce-7418-460b-a873-a5bf5001c76c.png" /></p><p><strong>建筑年代与房价</strong></p><p>看上去好像越老的房子越贵，上世纪末建的房子最值钱，而最近几年建的房子都不怎么值钱，当然这也跟我们之前分析的区域有关，因为最近建的房子基本都在岛外，所以当然不怎么值钱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;建筑年代&#x27;</span>)[<span class="string">&#x27;均价&#x27;</span>].mean().plot()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607623479793-fe610cab-0954-4669-afff-3824eabb0d12.png" /></p><p><strong>所在楼层与房价</strong></p><p>一般来说，大家都不太喜欢低楼层的房子，因为太吵了，当然太高也不行，这种关系，也反映在房价中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;所在楼层&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607622112702-1862764d-a020-4303-8938-244724f22518.png" /></p><p>再来看看厦门哪个小区的房子最贵吧，这里选取小区均价最高的15个小区：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;小区名&#x27;</span>)[<span class="string">&#x27;小区均价&#x27;</span>].mean().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">15</span>).plot(kind=<span class="string">&#x27;barh&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607651743576-9563d729-5c92-4f5c-a10d-1d85574a67fe.png" /></p><h2 id="地理可视化">地理可视化</h2><p>前阵子刚好接触到百度地图的API，非常强大，就顺手做个地图可视化吧！ 首先需要去百度地图开发者官网（ <a href="https://link.jianshu.com/?t=http://lbsyun.baidu.com/">http://lbsyun.baidu.com/</a>）注册一个密钥，然后创建两个应用，一个是服务端的，用来使用Python获取小区坐标，一个是浏览器端的，用来通过修改html源代码创建热力图，具体实现可以参考这篇文章：<a href="https://blog.csdn.net/ebzxw/article/details/80265796">Python使用百度地图API实现地点信息转换及房价指数热力地图</a> 最后生成的效果如下图所示，可以看出，厦门市最贵的地段基本上就在火车站周围那一块： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607655308085-a39295fa-8bad-495b-80b8-8fedada0be79.png" /> <em>ps: 这里可视化原本想使用 folium，但是存在 folium包存在两个问题，一个是热力图存在 bug，没有渐变效果，另外一个是因为我坐标采用的是百度的坐标，百度的坐标是经过加密的，用在 folium上会存在坐标偏移的情况，故弃用</em></p><h1 id="预测模型">预测模型</h1><p>以每平方米价格为因变量，其余变量为自变量，并将分类变量使用 LabelEncoder 编码，将测试集与训练集以2：8的比例分割： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=data.drop(columns=[<span class="string">&#x27;小区均价&#x27;</span>,<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;均价&#x27;</span>,<span class="string">&#x27;房屋户型&#x27;</span>,<span class="string">&#x27;小区名&#x27;</span>])</span><br><span class="line">y=data[<span class="string">&#x27;均价&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">&#x27;位置1&#x27;</span>,<span class="string">&#x27;房屋朝向&#x27;</span>,<span class="string">&#x27;一手房源&#x27;</span>,<span class="string">&#x27;所在楼层&#x27;</span>,<span class="string">&#x27;装修情况&#x27;</span>]:</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    x[col]=le.fit_transform(x[col])</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure> 由于自变量中存在很多分类变量，因此考虑使用树模型进行预测，由于树模型本身就有着特征选择的功能，因此，不做特征选择，直接跑模型</p><p><strong>决策树</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeRegressor()</span><br><span class="line">dt.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;决策树绝对值误差：<span class="subst">&#123;mean_absolute_error(dt.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>随机森林</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">2000</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">rf.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;随机森林绝对值误差：<span class="subst">&#123;mean_absolute_error(rf.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>Catboost</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cb=CatBoostRegressor()</span><br><span class="line">cb.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Catboost绝对值误差：<span class="subst">&#123;mean_absolute_error(cb.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>结果对比</strong></p><table><thead><tr class="header"><th></th><th>决策树</th><th>随机森林</th><th>catboost</th></tr></thead><tbody><tr class="odd"><td>绝对值误差</td><td>2885.81</td><td>2286.76</td><td>2347.04</td></tr></tbody></table><p><strong>特征重要性</strong></p><p>用随机森林输出特征重要性看看： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fi = pd.DataFrame(</span><br><span class="line">    &#123;<span class="string">&#x27;x&#x27;</span>: x.columns, <span class="string">&#x27;feature_importance&#x27;</span>: rf.feature_importances_&#125;)</span><br><span class="line">fi = fi.sort_values(by=<span class="string">&#x27;feature_importance&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;feature_importance&#x27;</span>, y=<span class="string">&#x27;x&#x27;</span>, data=fi)</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607667452772-a2ae6636-935b-411c-b9fa-864472bf30a5.png" /> 啊这，小区均价一枝独秀，解释力度太大了，把其他特征的信息都全部吃下去了，为了更好的解释其他特征与每平方米价格的关系，我们考虑把它排除在外，再输出一次特征重要性： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607668343722-a06abc5d-4305-4e67-a2ea-ee97a3a68a0b.png" /> 这次就好点了，预测的绝对值误差虽然变成了四千，预测效果变差了，但是解释力度提高了，对房价影响最大的前五个特征为：位置1（区域）、物业费（反映小区的质量）、容积率（反映小区的居住的舒适度)、总楼层、建筑年代，而房屋朝向、所在楼层和装修情况这些特征居然没有想象中的那么重要，看来在厦门，<strong>决定一套房子价格的是房子所在小区的属性，而不是你这套房子本身的属性</strong>。</p><h1 id="小结">小结</h1><p>好了，又一篇文章水完了，这篇文章还是花了我不少时间的，尤其是在研究怎么画图上，看来可视化这方面还是得继续学习一下啊！这个月总体来说还是比较忙的，希望能够坚持每周写一篇吧，下周可能会开始写一些算法的学习笔记。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;书接上文：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/329185040&quot;&gt;爬取厦门58同城二手房数据进行数据分析（一）&lt;/a&gt; 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。&lt;/p&gt;</summary>
    
    
    
    <category term="数据分析" scheme="https://lukan217.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="数据分析" scheme="https://lukan217.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（一）</title>
    <link href="https://lukan217.github.io/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://lukan217.github.io/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2020-12-03T16:00:00.000Z</published>
    <updated>2022-05-14T16:23:39.332Z</updated>
    
    <content type="html"><![CDATA[<p>最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。</p><h1 id="网页分析">网页分析</h1><p>首先，点进去首页，是一行行的信息，一页有120条： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010708512-8e3cc62c-fd97-4df6-a329-014b77fc3cf8.png#align=left&amp;display=inline&amp;height=419" alt="image.png" /> 点进去链接后是详情页的信息，主要分为两部分信息：房子属性和小区信息 <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011106024-d85dbb2a-08eb-47dd-b954-c89b1c253faa.png#align=left&amp;display=inline&amp;height=388" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010951590-61f97fe9-5be9-40f0-9eb2-47ef2046d244.png#align=left&amp;display=inline&amp;height=251" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010981428-3ead9cef-a4d1-4950-a749-ea116896ad27.png#align=left&amp;display=inline&amp;height=310" alt="image.png" /> 这里需要注意的是，虽然这些信息都可以直接用xpath获取，但是详情页里面的价格居然用了字体加密！！！ <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011288804-77aaeae6-90d6-4295-8486-30459b7c8642.png" alt="image.png" /> 虽然网上有很多这种破解办法，但其实根本不需要那么复杂，回到我们的首页，那边不也有价格嘛，而且，这个价格是没有加密的哦！（58同城程序员故意留的后门？？？)，所以我们爬虫思路大概就分三步走：</p><ol type="1"><li>先爬取首页一行行的信息，用xpath获取标题，价格，链接，一条信息用一个字典存储，然后将这条字典并存储在一个列表里面</li><li>第一步爬完后，然后开始遍历我们的列表，进入详情页的链接，把详情页的信息也用xpath一个个扒到我们的字典里，最终返回一个具有完整信息的列表</li><li>使用pandas的Dataframe函数就可以将第二部返回的列表直接转化为一个Dataframe，就能直接导出csv了。 原以为这样就能直接收工了，没想到小看58同城了，还没爬几条就给你来了个人机验证，访问再频繁点就需要登录了，然后就啥信息都爬不到了，因为自己之前爬的都是比较简单的网站，从来就没出现过这个问题，看来还是自己naive了啊，所以前几天也花了一整天的时间来解决这个问题，接下来就看看如何解决这个问题吧！</li></ol><h1 id="ip访问频繁问题">ip访问频繁问题</h1><p>一般这种问题的解决方案有两种：</p><ol type="1"><li>使用代理ip访问。</li><li>设置程序休眠时间和断点续传机制。</li></ol><p>第一种解决方案的话，一般来说是需要自己花钱去买付费ip的，因为大多数免费ip都被人用烂了，考虑到穷和需要花时间去学习怎么构建代理池的问题，于是第一种方案就被我放弃了。 第二种方案算是一种土办法吧，只要我访问足够慢，跟正常用户一样，那么它就不会封我ip（所以这次我爬500条数据都用了两个小时），但是，为了避免还是弹出人机验证的问题，我们需要一个断点续传机制，即当我知道自己的ip被封了的时候，就马上停止访问，并且把已经爬取的数据全部保存下来，然后手动用浏览器去网站上点一下验证码，然后再从之前停止的地方继续开始，这就需要我们给每条信息编个号，当检测到被封ip的时候输出这个编号，手动验证之后继续从这个编号处开始。 说了这么多都是废话，接下来直接上代码吧！ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="comment"># 输入链接，返回解析后的html</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36 Edg/86.0.622.63&quot;</span>&#125;</span><br><span class="line">    response = requests.get(url=url, headers=headers)</span><br><span class="line">    content = response.content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    html = etree.HTML(content)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_base_info</span>(<span class="params">page_url</span>):</span></span><br><span class="line">    <span class="comment"># 获取基础信息</span></span><br><span class="line">    html = parse_url(page_url)</span><br><span class="line">    titles = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/text()&#x27;</span>)  <span class="comment"># 标题</span></span><br><span class="line">    urls = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/@href&#x27;</span>)  <span class="comment"># 链接</span></span><br><span class="line">    total_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;sum&quot;]/b/text()&#x27;</span>)  <span class="comment"># 总价</span></span><br><span class="line">    unit_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;unit&quot;]/text()&#x27;</span>)  <span class="comment"># 均价</span></span><br><span class="line">    base_infos = []  <span class="comment"># 使用一个列表存储所有信息</span></span><br><span class="line">    <span class="keyword">for</span> title, url, total_price, unit_price <span class="keyword">in</span> <span class="built_in">zip</span>(titles, urls, total_prices, unit_prices):</span><br><span class="line">        <span class="comment"># 将信息写入一个字典中</span></span><br><span class="line">        info = &#123;&#125;</span><br><span class="line">        info[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">        <span class="keyword">if</span> url[<span class="number">0</span>:<span class="number">5</span>] != <span class="string">&#x27;https&#x27;</span>:  <span class="comment"># 有的链接不是https开头的，手动加上</span></span><br><span class="line">            url = <span class="string">&#x27;https:&#x27;</span>+url</span><br><span class="line">        info[<span class="string">&#x27;url&#x27;</span>] = url.split(<span class="string">&#x27;?&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># 删掉链接后面跟的cookie参数什么乱七八糟的东西</span></span><br><span class="line">        info[<span class="string">&#x27;total_price&#x27;</span>] = total_price</span><br><span class="line">        info[<span class="string">&#x27;unit_price&#x27;</span>] = unit_price</span><br><span class="line">        base_infos.append(info)</span><br><span class="line">    <span class="keyword">return</span> base_infos</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_extra_info</span>(<span class="params">info</span>):</span></span><br><span class="line">    <span class="comment"># 进入详情页获取更多信息</span></span><br><span class="line">    info_url = info[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">    html = parse_url(info_url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[1]/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[2]/text()&#x27;</span>)[<span class="number">0</span>].replace(<span class="string">&#x27;－&#x27;</span>, <span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 获取详情页表格中的信息</span></span><br><span class="line">    info_keys = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;mr_25 c_999&quot;]/text()&#x27;</span>)[<span class="number">1</span>:]</span><br><span class="line">    info_values = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;c_000&quot;]&#x27;</span>)</span><br><span class="line">    info_values = [v.text <span class="keyword">for</span> v <span class="keyword">in</span> info_values]</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> <span class="built_in">zip</span>(info_keys, info_values):</span><br><span class="line">        info[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取小区及周边信息</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/h3/a/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[1]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[3]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[4]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[5]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[6]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://xm.58.com/ershoufang/pn&#x27;</span></span><br><span class="line">infos = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))  <span class="comment"># 设置休息时间应对反爬</span></span><br><span class="line">    page_url = base_url+<span class="built_in">str</span>(i)</span><br><span class="line">    results = get_base_info(page_url)</span><br><span class="line">    infos.extend(results)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;爬取页面<span class="subst">&#123;i&#125;</span>的基础信息成功！&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(infos)):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))</span><br><span class="line">    infos[i] = get_extra_info(infos[i])</span><br><span class="line">    <span class="keyword">if</span> infos[i][<span class="string">&#x27;location1&#x27;</span>] == <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> infos[i][<span class="string">&#x27;xiaoqu_name&#x27;</span>] == <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 如果这两个值都为空值，说明开始人机验证了</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;爬取第<span class="subst">&#123;i&#125;</span>条信息失败,请进行人机验证! &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(infos[i][<span class="string">&#x27;url&#x27;</span>])</span><br><span class="line">        <span class="comment"># 及时保存数据</span></span><br><span class="line">        data = pd.DataFrame(infos)</span><br><span class="line">        data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取第&#123;&#125;条信息成功：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, infos[i][<span class="string">&#x27;title&#x27;</span>]))</span><br><span class="line">        </span><br><span class="line">data = pd.DataFrame(infos)</span><br><span class="line">data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)  <span class="comment"># 导出到csv文件</span></span><br></pre></td></tr></table></figure></p><h1 id="总结">总结</h1><p>这次爬虫主要学了三个东西吧：</p><ol type="1"><li>字典是个很有用的数据类型，不仅存储效率高，而且将多个字典放在列表里可以直接转化为pandas的Dataframe，还就能直接导出，不需要读写文件那么麻烦。</li><li>xpath的异常处理机制很重要。有些信息可能会在某个页面上神奇的消失了，所以最好事先做好异常处理机制，爬不到就置为空值，不然程序一报错之前爬的数据全没掉了。</li><li>断点续传机制也很重要。天知道网站会在什么时候给你跳出验证码，所以最好有断点续传机制，防止你要从头来过。</li></ol><p>好了，第一篇技术性的文章就这样水成了，第二篇过几天有空写。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。&lt;/p&gt;</summary>
    
    
    
    <category term="数据分析" scheme="https://lukan217.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="爬虫" scheme="https://lukan217.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
