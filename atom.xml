<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lukan&#39;s Blog</title>
  
  <subtitle>记录点滴成长</subtitle>
  <link href="https://lukan797.github.io/atom.xml" rel="self"/>
  
  <link href="https://lukan797.github.io/"/>
  <updated>2022-02-08T13:46:55.475Z</updated>
  <id>https://lukan797.github.io/</id>
  
  <author>
    <name>Lukan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>贝叶斯优化基本原理总结</title>
    <link href="https://lukan797.github.io/2022/01/23/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan797.github.io/2022/01/23/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/</id>
    <published>2022-01-22T17:50:03.926Z</published>
    <updated>2022-02-08T13:46:55.475Z</updated>
    
    <content type="html"><![CDATA[<p>贝叶斯优化（Bayesian Optimization），主要用来解决计算成本昂贵的黑盒优化问题，这种问题有着以下两个特点：</p><ol type="1"><li>目标函数<span class="math inline">\(f(x)\)</span>及其导数未知，否则就可以用梯度下降等方法求解</li><li>计算目标函数时间成本大，意味着像蚁群算法、遗传算法这种方法也失效了，因为计算一次要花费很多时间</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642518880682-ca49f801-c6dd-406d-965d-9ba3690c74a6.png#clientId=u3fe6f5be-444f-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=52&amp;id=SNVtL" alt="image.png" /><br />这种问题最典型的就是机器学习里面的超参数优化，使用的模型为 <span class="math inline">\(f\)</span>，超参数为输入的 <span class="math inline">\(x\)</span>，评估指标（MSE, AUC等）为输出的目标函数值，在这个场景下，很多机器学习的入门课程都会提到网格搜索和随机搜索，但是这两个其实本质上也是一种类似于穷举的方式，随便选取一组可能的<span class="math inline">\(x\)</span>，然后分别计算目标值，最后对比所有的结果得到最好的解，可以看出来这种求解是很低效的，因此，解决这种问题需要设计一种高效的算法，来在有限的时间里面找到一个相对不错的解，这就是贝叶斯优化。<br />贝叶斯优化，是一种使用贝叶斯定理来指导搜索以找到目标函数的最小值或最大值的方法，就是在每次迭代的时候，利用之前观测到的历史信息（先验知识)来进行下一次优化，通俗点讲，就是在进行一次迭代的时候，先回顾下之前的迭代结果，结果太差的<span class="math inline">\(x\)</span>附近就不去找了，尽量往结果好一点的<span class="math inline">\(x\)</span>附近去找最优解，这样一来搜索的效率就大大提高了，这其实和人的思维方式也有点像，每次在学习中试错，并且在下次的时候根据这些经验来找到最优的策略。 <a name="raVsx"></a></p><h1 id="贝叶斯优化过程">贝叶斯优化过程</h1><p>首先，假设有一个这样的函数<span class="math inline">\(c(x)\)</span>，我们需要找到他的最小值，如下图所示，这也是我们所需要优化的目标函数，但是我们并不能够知道他的具体形状以及表达形式是怎么样的。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862207488-61095c80-c151-4cb2-848d-74c44f23901f.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u10477eea" alt="image.png" /><br />贝叶斯优化是通过一种叫做代理优化的方式来进行的，就是不知道真实的目标函数长什么样，我们就用一个代理函数（surrogate function）来代替目标函数，而这个代理函数就可以通过先采样几个点，再通过这几个点来给他拟合出来，如下图虚线所示：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862350662-cfeab91b-ee4f-4395-8cae-1c9c1df4d8c1.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u93bcdc6a" alt="image.png" /><br />基于构造的代理函数，我们就可以在可能是最小值的点附近采集更多的点，或者在还没有采样过的区域来采集更多的点，有了更多点，就可以更新代理函数，使之更逼近真实的目标函数的形状，这样的话也更容易找到目标函数的最小值，这个采样的过程同样可以通过构建一个采集函数来表示，也就是知道了当前代理函数的形状，如何选择下一个<span class="math inline">\(x\)</span>使得收益最大。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642862456555-21005371-dd1b-419e-9c9c-2c4e28d83ddf.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=uc7daab55" alt="image.png" /><br />然后重复以上过程，最终就可以找到函数的最小值点了，这大致就是贝叶斯优化的一个过程：</p><ol type="1"><li>初始化一个代理函数的先验分布</li><li>选择数据点<span class="math inline">\(x\)</span>，使得采集函数<span class="math inline">\(a(x)\)</span>取最大值</li><li>在目标函数 <span class="math inline">\(c(x)\)</span>中评估数据点<span class="math inline">\(x\)</span>并获取其结果 <span class="math inline">\(y\)</span></li><li>使用新数据<span class="math inline">\((x,y)\)</span>更新代理函数，得到一个后验分布（作为下一步的先验分布）</li><li>重复2-4步，直到达到最大迭代次数</li></ol><p>举个例子，如图所示，一开始只有两个点（t=2），代理函数的分布是紫色的区域那块，然后根据代理函数算出一个采集函数（绿色线），取采集函数的最大值所在的<span class="math inline">\(x\)</span>（红色三角处），算出<span class="math inline">\(y\)</span>，然后根据新的点<span class="math inline">\((x,y)\)</span>更新代理函数和采集函数（t=3），继续重复上面步骤，选择新的采集函数最大值所在的<span class="math inline">\(x\)</span>，算出<span class="math inline">\(y\)</span>，再更新代理函数和采集函数，然后继续迭代<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/764062/1642870783116-4e643df2-ec8f-4b2c-84e2-f5d11c2cfac2.png#clientId=u86233cce-d6bd-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;id=u912a5632" alt="image.png" /><br />问题的核心就在于代理函数和采集函数如何构建，常用的代理函数有：</p><ol type="1"><li>高斯过程（Gaussian processes）</li><li>Tree Parzer Estimator</li><li>概率随机森林：针对类别型变量</li></ol><p>采集函数则需要兼顾两方面的性质：</p><ol type="1"><li>利用当前已开发的区域（Exploitation）：即在当前最小值附近继续搜索</li><li>探索尚未开发的区域（Exploration）：即在还没有搜索过的区域里面搜索，可能那里才是全局最优解</li></ol><p>常用的采集函数有：</p><ol type="1"><li><p>Probability of improvement（PI）</p></li><li><p>Expected improvement（EI）</p></li><li><p>Confidence bound criteria，包括LCB和UCB <a name="lYgNb"></a></p></li></ol><h1 id="可用的贝叶斯优化框架">可用的贝叶斯优化框架</h1><ol type="1"><li>BayesianOptimization：<a href="https://github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a></li><li>清华开源的openbox：<a href="https://open-box.readthedocs.io/zh_CN/latest/index.html">https://open-box.readthedocs.io/zh_CN/latest/index.html</a></li><li>华为开源的HEBO：<a href="https://github.com/huawei-noah/HEBO">https://github.com/huawei-noah/HEBO</a></li><li>Hyperopt：<a href="http://hyperopt.github.io/hyperopt/">http://hyperopt.github.io/hyperopt/</a></li></ol><p>个人觉得1和2就挺好用的 <a name="ZDkC6"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f">https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f</a></li><li><a href="https://chengfeng96.com/blog/2019/09/08/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/">https://chengfeng96.com/blog/2019/09/08/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</a></li><li><a href="https://zhuanlan.zhihu.com/p/459110020">https://zhuanlan.zhihu.com/p/459110020</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;贝叶斯优化（Bayesian Optimization），主要用来解决计算成本昂贵的黑盒优化问题，这种问题有着以下两个特点：&lt;/p&gt;</summary>
    
    
    
    <category term="教程" scheme="https://lukan797.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="最优化" scheme="https://lukan797.github.io/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>一个简约的beamer模板</title>
    <link href="https://lukan797.github.io/2021/12/05/%E4%B8%80%E4%B8%AA%E7%AE%80%E7%BA%A6%E7%9A%84beamer%E6%A8%A1%E6%9D%BF/"/>
    <id>https://lukan797.github.io/2021/12/05/%E4%B8%80%E4%B8%AA%E7%AE%80%E7%BA%A6%E7%9A%84beamer%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-12-05T10:24:04.588Z</published>
    <updated>2021-12-05T10:28:32.190Z</updated>
    
    <content type="html"><![CDATA[<p>开组会啥的，要做ppt，但感觉还是用beamer比较方便，用默认的模板改了改，看着也还行 ​</p><p>项目链接：<a href="https://github.com/Smallviller/beamer_template">https://github.com/Smallviller/beamer_template</a> <img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638592-6975409f-6254-4735-8062-0b436f49051b.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u2147a6de&amp;margin=%5Bobject%20Object%5D&amp;name=0001.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=82132&amp;status=done&amp;style=none&amp;taskId=u14532634-3b60-409f-9605-bb6f6f05ca2" alt="0001.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638348-e5ff3e5b-f79d-4782-8e55-0c30b903ba5f.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u3af9e72d&amp;margin=%5Bobject%20Object%5D&amp;name=0002.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=54464&amp;status=done&amp;style=none&amp;taskId=u06378bdd-14f5-4d70-8234-2de0e293ae7" alt="0002.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638612-1fe8e3f9-6a92-4e37-ae33-b5b6ab56f8e2.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u8c014218&amp;margin=%5Bobject%20Object%5D&amp;name=0003.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=93115&amp;status=done&amp;style=none&amp;taskId=u2ea290d5-3e0e-4325-8026-ef36ee6ca4f" alt="0003.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638621-2ea8a6c3-280f-42aa-b3c0-ad26cfaa4c76.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u36cfdf6d&amp;margin=%5Bobject%20Object%5D&amp;name=0004.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=78021&amp;status=done&amp;style=none&amp;taskId=u1217dd56-073b-4fa2-b583-b8fb2918500" alt="0004.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638656-448960af-454b-40ab-8d10-cd79c533bf18.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u3b90e5c0&amp;margin=%5Bobject%20Object%5D&amp;name=0005.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=91360&amp;status=done&amp;style=none&amp;taskId=u9030a713-073c-4dd9-8d48-af400e2ad07" alt="0005.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691638958-0c02aa64-99e9-4ea3-ba8c-531ac03c087b.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u80084e0b&amp;margin=%5Bobject%20Object%5D&amp;name=0006.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=57523&amp;status=done&amp;style=none&amp;taskId=ua805a004-b4b3-4624-a5b3-87eea69d808" alt="0006.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639346-c7384931-218f-4b0b-b9f3-d022f11872fe.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=ufb0038af&amp;margin=%5Bobject%20Object%5D&amp;name=0007.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=82696&amp;status=done&amp;style=none&amp;taskId=ud7d2d344-949b-44c7-a352-376c052870f" alt="0007.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639446-6f42c330-ccdb-401e-93dc-4be4c74550e2.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=u79b63874&amp;margin=%5Bobject%20Object%5D&amp;name=0008.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=70224&amp;status=done&amp;style=none&amp;taskId=u33f6e90c-dad6-4134-ab5e-d189eff53c1" alt="0008.jpg" /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1638691639692-a5053fac-411d-40d1-8501-6c7fe318e305.jpeg#clientId=uca4f431b-7c8d-4&amp;from=drop&amp;id=ud5f6369d&amp;margin=%5Bobject%20Object%5D&amp;name=0009.jpg&amp;originHeight=1063&amp;originWidth=1890&amp;originalType=binary&amp;ratio=1&amp;size=51952&amp;status=done&amp;style=none&amp;taskId=u6b9d7536-8999-465a-9817-ce762930acf" alt="0009.jpg" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;开组会啥的，要做ppt，但感觉还是用beamer比较方便，用默认的模板改了改，看着也还行 ​&lt;/p&gt;</summary>
    
    
    
    <category term="教程" scheme="https://lukan797.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="beamer" scheme="https://lukan797.github.io/tags/beamer/"/>
    
    <category term="latex" scheme="https://lukan797.github.io/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle M5时间序列预测比赛的发现总结</title>
    <link href="https://lukan797.github.io/2021/11/27/Kaggle%20M5%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B%E7%9A%84%E5%8F%91%E7%8E%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan797.github.io/2021/11/27/Kaggle%20M5%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B%E7%9A%84%E5%8F%91%E7%8E%B0%E6%80%BB%E7%BB%93/</id>
    <published>2021-11-27T08:52:00.501Z</published>
    <updated>2021-11-28T08:51:21.530Z</updated>
    
    <content type="html"><![CDATA[<p>谈到时序预测，2020年kaggle上举办的那场M5沃尔玛销量预测比赛肯定是最值得学习的，有人甚至直接根据这场比赛写了两篇论文（见文末）发在 International Journal of Forecasting上，来总结这场比赛用到的一些方法，这里就把论文中的一些关键发现列出来供学习参考。 <a name="Y2CJP"></a></p><h1 id="机器学习方法的优越性"><strong>机器学习方法的优越性</strong></h1><p>多年来，经验发现，简单的方法与复杂的或统计上复杂的方法一样准确。由于有限的数据可用性、算法的低效性、预处理的需要以及受限的计算能力，此前机器学习方法和统计学方法相比准确性都有所不足。M4是第一个发现两个ML方法明显比简单的统计方法更准确的预测竞赛，突出了ML方法对更准确预测的潜在价值。赢得M4竞赛的第一名是混合了RNN和指数平滑的混合方法，而排名第二的方法是使用XGBoost对标准时间序列预测方法产生的预测进行优化加权。尽管M4的两个获奖作品在本质上都是ML，但它们都建立在统计学、系列的特定功能上，同时也与四种统计学方法的简单组合有着差不多的准确性。然而，M5是第一个所有表现最好的方法都是 "纯 "ML方法，并且明显优于所有统计基准及其组合的比赛。LightGBM证明了它可以有效地用于处理大量相关的时间序列序列和外生变量，并减少预测误差。此外，像DeepAR和N-BEATS这样的深度学习方法，也显示出预测的潜力。 <a name="scDUJ"></a></p><h1 id="模型融合的价值"><strong>模型融合的价值</strong></h1><p>M5 Accuracy竞赛表明融合不同方法的预测，即使是相对简单的方法，也能从总体上提高精确度。M5 Accuracy竞赛的冠军采用了一个非常简单的简单平均融合，涉及6个模型，每个模型利用不同的学习方法和训练集。同样，亚军采用了5个模型的简单平均融合，每个模型对趋势有不同的估计，而第三名的方法，是43个NN的简单平均融合。排名14th, 17th, 21st, 24th, 25th和44th方法也使用了模型的简单平均融合。 在这些融合方法中，只有排名25的方法考虑了对单个方法的不平等加权融合。模型融合的价值也被竞争的baleline之间的比较所支持：指数平滑模型和ARIMA模型的融合比单个方法的表现更好，自上而下和自下而上的调和预测的融合则比自上而下和自下而上的表现都好。 <a name="KXkQA"></a></p><h1 id="时间序列交叉学习-的价值"><strong>"时间序列交叉学习 "的价值</strong></h1><p>在以前的M系列竞赛中，大多数时间序列都是不相关的，属于不同的频率和领域，而且时间上也不对齐。因此，尽管M4比赛中表现最好的两个参赛者都同时考虑了从多个序列中"交叉学习"，而不是一次一个系列，但他们的方法在实践中难以有效实施，也没有充分展示出该方法的潜力。相比之下，由于M5是由排列整齐、高度相关的时间序列组成的，而且是分层结构的，所以 "交叉学习 "更容易应用，与逐个时间序列训练的方法相比，取得了很好的效果。同时，除了可以提高准确率，"交叉学习 "意味着可以在多个时间序列上只使用一个模型，因为这些序列是一起训练的，而不用针对每个时间序列都训练一个模型，因此减少了整体的计算成本，缓解了与有限的历史观察相关的困难。基本上，M5中所有表现最好的50个方法都使用了 "交叉学习"，利用了数据集提供的所有信息。 <a name="R2rMu"></a></p><h1 id="获胜的方法和用于销售预测的baseline之间存在重大差异"><strong>获胜的方法和用于销售预测的baseline之间存在重大差异</strong></h1><p>M5 Accuracy竞赛考虑了24个通常用于销售预测应用的各种类型的baseline，包括传统的和最先进的统计方法、ML方法和他们的融合。但与这些baseline相比，获胜的作品明显提供了更准确的预测，并且在WRMSSE方面也平均高出20%以上。尽管较低的聚集水平上表现差不多，但结果仍清楚地表明了它们的优越性，并促使在ML预测方法领域进行更多的研究，这些方法可用于预测时间序列之间复杂的非线性关系，同时也能够方便的引入外生/解释变量。 <a name="HxvtW"></a></p><h1 id="从模型外部校正预测的价值"><strong>从模型外部校正预测的价值</strong></h1><p>当预测者利用外部信息、内部知识以及他们的专业知识来提高预测精度时，通常会使用预测调整。在M5 Accuracy竞赛中，一些表现最好的方法，如排名第2和第5的方法，以乘数的形式利用这种调整来提高ML模型的预测（即在原先的预测结果上乘以一个系数，如0.95-1.05来校正预测结果，这个实际上是因为lightgbm外推能力差，在有趋势的序列上容易高估预测或者低估预测）。尽管这些调整并不完全基于判断，而是基于最低聚合水平产生的预测与较高聚合水平的预测之间的分析进行调整，但这些调整被证明是有用的，能够帮助预测模型减少偏差，更好地说明在较高聚合水平上更容易观察到的长期趋势。 然后，这种调整的实际价值需要进一步调查，但在预测领域，调和在不同聚集水平产生的预测的概念并不新鲜，许多研究实证证明了它的好处，特别是当预测和来自完整层次的信息被利用时。 <a name="LOU5s"></a></p><h1 id="有效的cv交叉验证策略的价值"><strong>有效的CV（交叉验证）策略的价值</strong></h1><p>在处理复杂的预测任务时，采用有效的CV策略对于提高样本外预测的准确性、避免过拟合和减轻不确定性至关重要。采用这种策略的重要性在M5 Accuracy竞赛的结果中得到了证明，选择进行CV的时间段，验证时间窗口的大小，这些窗口的更新方式，以及用于衡量预测表现的标准，都是预测者必须考虑的一些因素。在M5精确度竞赛中，表现最好的四种方法和前50名提交的绝大多数都考虑了这样的一种CV策略，即至少使用最后四个28天长的可用的数据窗口来做交叉验证以评估预测精确度，从而对样本外的表现提供一个合理的近似。除了这个CV方案之外，第一名的方案还同时测量了他所开发的模型CV结果的平均值和标准偏差。根据他的验证，他的方法中的递归模型被发现平均比非递归模型更准确，但更不稳定。因此，他决定将这两个模型结合起来，以确保产生的预测既是准确和稳定。在评估预测方法时，必须考虑到预测误差的全部分布，特别是其尾部，这表明稳健性是实现高准确度的前提条件。我们希望M5的结果将鼓励在这一领域的更多研究，并有助于开发更强大的CV策略。 <a name="VfP42"></a></p><h1 id="外生解释变量的重要性"><strong>外生/解释变量的重要性</strong></h1><p>时间序列方法通常足以识别和捕捉其历史数据模式（水平、趋势和季节性），通过推断这种模式并产生准确的预测。然而，仅仅依靠历史数据的时间序列方法不能有效地说明节假日、特殊事件、促销活动、价格以及可能的天气的影响。在这种情况下，来自外生/解释变量的信息对于提高预测精度显得至关重要。在M5 Accuracy预测竞赛中，所有获奖作品都利用外部信息来提高其模型的预测性能。例如，monsaraida和其他团队发现，几个与价格相关的特征对于提高他们模型结果的准确性具有重要意义。此外，外生/解释变量的重要性也在几个简单统计学模型中得到的支持，例如，使用促销信息和特殊事件作为外生变量的指数平滑模型比普通的指数平滑模型精确度要高6%。ARIMA模型的情况也是如此，ARIMAX的精度比普通的ARIMA要高13%。 <a name="mDxVC"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://www.researchgate.net/publication/344487258_The_M5_Accuracy_competition_Results_findings_and_conclusions">The M5 Accuracy competition: Results, findings and conclusions</a></li><li><a href="https://www.researchgate.net/publication/346493740_The_M5_Uncertainty_competition_Results_findings_and_conclusions">The M5 Uncertainty competition: Results, findings and conclusions</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;谈到时序预测，2020年kaggle上举办的那场M5沃尔玛销量预测比赛肯定是最值得学习的，有人甚至直接根据这场比赛写了两篇论文（见文末）发在 International Journal of Forecasting上，来总结这场比赛用到的一些方法，这里就把论文中的一些关键发现列出来供学习参考。 &lt;a name=&quot;Y2CJP&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="时间序列" scheme="https://lukan797.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Git常用操作总结</title>
    <link href="https://lukan797.github.io/2021/08/29/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan797.github.io/2021/08/29/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-28T18:31:42.115Z</published>
    <updated>2021-08-28T18:43:37.696Z</updated>
    
    <content type="html"><![CDATA[<p>最近工作上会用到Git，这里记录一下一些常用操作供参考！</p><h1 id="git配置">Git配置</h1><ol type="1"><li><p>安装Git：<a href="https://git-scm.com/">https://git-scm.com/</a></p></li><li><p>本地命令行生成密钥绑定GitHub账号 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 输入命令生成密钥对，替换成自己邮箱，然后一路回车</span></span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将生成的公钥打印出来复制，将这串文本复制粘贴到GitHub的Setting-&gt;SSH and GPG keys中</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 输入命令检查是否绑定成功,输入yes后，如果出现Hi,xxx!则绑定成功</span></span><br><span class="line">ssh -T git@github.com</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li><li><p>配置用户名和邮箱信息： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看配置信息，一开始为空</span></span><br><span class="line">git config --list</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 全局配置，对所有代码库生效</span></span><br><span class="line">git config --global user.name &quot;你的名字&quot;</span><br><span class="line">git config --global user.email &quot;你的邮箱&quot;</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 局部配置，只对当前的代码库有效</span></span><br><span class="line">git config --local user.name &quot;你的名字&quot;</span><br><span class="line">git config --local user.email &quot;你的邮箱&quot;</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置后，远程仓库提交的commit里对应的用户即为 user.name</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li></ol><h1 id="git基本概念">Git基本概念</h1><ol type="1"><li>本地仓库：本地仓库上存放所有相关的文件，具体可分为工作区、暂存区和仓库区，工作区即项目文件夹下不包含<code>.git</code>文件夹的所有文件，暂存区和仓库区则在<code>.git</code>文件夹下<ol type="1"><li>工作区：即我们工作的文件夹，在里面进行文件的增删改操作</li><li>暂存区：临时保存工作区上的改动，通过<code>git add</code>操作将工作区的修改同步到暂存区</li><li>仓库区：当执行<code>git commit</code>操作时，将暂存区上的所有变动同步到本地仓库</li></ol></li><li>远程仓库：GitHub/GitLab上保存的仓库，通过<code>git push</code>将本地仓库同步到远程仓库，也可以通过<code>git fetch/pull</code>将远程仓库同步到本地仓库</li></ol><figure><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1630159880578-b47a2dae-9236-4c17-8de3-f4d7155ac69f.png" alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h1 id="git基本操作">Git基本操作</h1><h2 id="创建版本库">创建版本库</h2><p>创建版本库有两种方式，一种是将本地的文件夹直接变成一个git仓库，另一种是直接将远程的仓库克隆到本地 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git init # 将本地文件夹变为一个git仓库</span><br><span class="line">git clone &lt;url&gt; #将远程仓库克隆到本地</span><br></pre></td></tr></table></figure></p><h2 id="修改与提交操作">修改与提交操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git add &lt;file&gt; # 将单个文件从工作区添加到暂存区</span><br><span class="line">git add . # 将所有文件添加到暂存区</span><br><span class="line">git commit -m &quot;messenge&quot; # 将暂存区文件提交到本地仓库</span><br><span class="line">git status # 查看工作区状态，显示有变更的文件。</span><br><span class="line">git diff # 比较文件的不同，即暂存区和工作区的差异。</span><br></pre></td></tr></table></figure><h2 id="远程操作">远程操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git push origin master # 将本地的master分支推送到远程对应的分支</span><br><span class="line">git pull  # 下载远程代码并合并，相当于git fetch + git pull</span><br><span class="line">git fetch# 从远程获取代码库，但不进行合并操作</span><br><span class="line"></span><br><span class="line">git remote add origin &lt;url&gt; # 将远程仓库与本地仓库关联起来</span><br><span class="line">git remote -v # 查看远程库信息</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="撤销与回退操作">撤销与回退操作</h2><p>撤销操作：当修改了工作区/暂存区的文件，但是还没有commit时，想要撤销之前的操作： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 场景1：当你改乱了工作区某个文件的内容，但还没有add到暂存区</span></span><br><span class="line">git checkout &lt;file&gt; # 撤销工作区的某个文件到和暂存区一样的状态</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 场景2：当乱改了工作区某个文件的内容，并且git add到了暂存区</span></span><br><span class="line">git reset HEAD &lt;file&gt; # 第1步，将暂存区的文件修改撤销掉</span><br><span class="line">git checkout &lt;file&gt; # 第2步，将工作区的文件修改撤销掉</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 场景3：乱改了很多文件，想回到最新一次提交时的状态</span></span><br><span class="line">git reset --hard HEAD # 撤销工作区中所有未提交文件的修改内容</span><br><span class="line"></span><br></pre></td></tr></table></figure> 回退操作：当已经进行了commit操作，需要回退到之前的版本： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD^ # 回退到上次提交的状态</span><br><span class="line">git reset --hard HEAD~n # 回退到n个版本前的状态</span><br><span class="line">git reset --hard HEAD commitid # 回退到某一个commitid的状态</span><br></pre></td></tr></table></figure></p><h1 id="git分支管理">Git分支管理</h1><p>git的最强大之处就在于分支管理了，具体有两种应用场景：</p><ol type="1"><li>多人协作：每个人都基于主分支创建一个自己的分支，在分支上进行开发，然后再不断将写好的代码合并到主分支</li><li>自己修复bug/增加feature：创建一个bug分支或者feature分支，写好代码后合并到自己的分支然后删除bug/feature分支 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git branch &lt;name&gt; # 创建分支</span><br><span class="line">git checkout &lt;name&gt; # 切换到某个分支</span><br><span class="line">git checkout -b newtest # 创建并切换到新分支，相当于同时执行了以上两个命令</span><br><span class="line">git merge &lt;name&gt; # 合并某个分支到当前分支中，默认fast forward</span><br><span class="line">git branch -a # 查看所有分支</span><br><span class="line">git branch -d &lt;name&gt; # 删除分支</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h1 id="git多人协作">Git多人协作</h1><p>多人协作在同一个分支上进行开发的工作模式：</p><ol type="1"><li>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</li><li>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并；</li><li>如果合并有冲突，则解决冲突，并在本地提交；</li><li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</li><li>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。<br /></li></ol><h1 id="参考">参考</h1><ol type="1"><li><a href="https://www.runoob.com/git/git-tutorial.html">Git教程 | 菜鸟教程</a></li><li><a href="https://www.liaoxuefeng.com/wiki/896043488029600">Git教程 - 廖雪峰的官方网站</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近工作上会用到Git，这里记录一下一些常用操作供参考！&lt;/p&gt;</summary>
    
    
    
    <category term="教程" scheme="https://lukan797.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="git" scheme="https://lukan797.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>科大讯飞|线下商店销量预测挑战赛top7方案</title>
    <link href="https://lukan797.github.io/2021/08/21/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E7%BA%BF%E4%B8%8B%E5%95%86%E5%BA%97%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9Btop7%E6%96%B9%E6%A1%88/"/>
    <id>https://lukan797.github.io/2021/08/21/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E7%BA%BF%E4%B8%8B%E5%95%86%E5%BA%97%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9Btop7%E6%96%B9%E6%A1%88/</id>
    <published>2021-08-21T11:10:08.518Z</published>
    <updated>2021-08-21T11:15:42.634Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了科大讯飞的线下商店销量预测挑战赛，线上成绩0.66，最终排名第七，这里把自己的方案分享出来，欢迎大家交流讨论！代码和数据均已上传到GitHub：<br /><a href="https://github.com/Smallviller/KDXF_sales_forecast_competition">https://github.com/Smallviller/KDXF_sales_forecast_competition</a> <a name="hjSac"></a></p><h1 id="赛题说明">赛题说明</h1><p>比赛传送门：<a href="https://challenge.xfyun.cn/topic/info?type=offline-store-sales-forecast">https://challenge.xfyun.cn/topic/info?type=offline-store-sales-forecast</a> <a name="WNhyD"></a></p><h2 id="赛题任务">赛题任务</h2><p>给定商店销量历史相关数据和时间等信息，预测商店对应商品的周销量。 <a name="j2N5j"></a></p><h2 id="数据说明">数据说明</h2><p>训练集：33周的历史销量数据<br />测试集：34周的销量<br />数据字段：字段shop_id（店铺id）、 item_id（商品id）、week（周标识）、item_price（商品价格）、item_category_id（商品品类id）、weekly_sales（周销量）组成。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1629537615118-1f4a390d-5a87-4d37-8629-ffc0700e24ed.png" alt="image.png" /><br />可以发现这里的shop_id、item_id、week和item_category_id进行了脱敏处理，经过简单探索，发现：</p><ol type="1"><li><p>shop_id共有32个，item_id共有523个、item_category_id共有34个，shop和item是多对多的关系，通过shop*item可标识唯一商品。</p></li><li><p>item_price存在大量空值，比率达73%</p></li><li><p>weekly_sales大多偏低，集中在0和1，存在间歇性需求的问题 <a name="mQdqi"></a></p></li></ol><h1 id="特征工程">特征工程</h1><p>一般销量预测，特征工程主要从这几个方面入手：时间相关特征、历史销量相关特征、价格相关特征...不过这里的时间特征被脱敏了，用不到，所以特征工程主要从销量和价格入手。 <a name="mib5L"></a></p><h2 id="销量相关特征">销量相关特征</h2><ol type="1"><li><p>滞后特征：滞后 1-14周的销量</p></li><li><p>滑动特征：滑动2-14周销量的min/max/median/mean/std</p></li><li><p>类别encoding特征：每个item、shop、item_category、shop*item_category、shop*item销量的mean和std</p></li><li><p>类别滞后特征：每个item、shop、item_category滞后1-14周的销量 <a name="m6Ypo"></a></p></li></ol><h2 id="价格相关特征">价格相关特征</h2><ol type="1"><li><p>价格原始值：包含原始特征和填充特征，填充策略采用先向前填补再向后填补，最后没填补到的在用众数填补</p></li><li><p>类别encoding特征：每个item、shop、item_category、shop*item_category、shop*item价格的mean和std</p></li><li><p>价格差异特征：当前价格与shop、item、item_cat、shop_cat、shop_item的价格均值的差值</p></li><li><p>价格变动特征：当前价格与上周价格/上个月平均价格的差值 <a name="Ty2SW"></a></p></li></ol><h1 id="模型">模型</h1><p><a name="Fxz2m"></a></p><h2 id="模型-1">模型</h2><p>只使用了lightgbm <a name="JQtAZ"></a></p><h2 id="损失函数">损失函数</h2><p>训练的损失函数采用tweedie，由于存在间歇性需求的问题，很多商品的销量的销量为0，满足tweedie分布，因此采用tweedie作为损失函数效果要比mse要更好 <a name="Ls2BR"></a></p><h2 id="交叉验证策略">交叉验证策略</h2><p>由于时间序列的数据存在先后，只能用历史来预测未来，因此在交叉验证的时候就得格外小心，不能使用随机划分，因为这样会泄露未来的数据，但是时序也有自己的一套交叉验证方法，我这里使用了三折交叉。<br />使用三折交叉验证，建立三个lgb模型：</p><ol type="1"><li>模型1：训练集使用1-30周数据，验证集使用31周数据，早停100轮</li><li>模型2：训练集使用1-31周数据，验证集使用32周数据，早停100轮</li><li>模型3：训练集使用1-32周数据，验证集使用33周数据，早停100轮</li></ol><p>特征工程、模型的调参等都是基于这个交叉策略来做的，最后将这三个模型取简单平均。<br />为什么不用五折交叉？五折交叉融合的效果不太好，经试验3折融合的成绩是最好的 <a name="wELKW"></a></p><h2 id="后处理">后处理</h2><p>由于树模型无法捕捉到趋势，只能学习到历史的东西，不能外推，预测的时候就容易偏高或者偏低，所以提交的时候其实还试着给结果乘上了一个系数1.025，这也是kaggle上很多时序比赛用的一个trick，结果大概能提升0.005个点吧 <a name="okZbV"></a></p><h1 id="一些本地有效果但线上不能提分的尝试">一些本地有效果但线上不能提分的尝试</h1><ol type="1"><li><p>分shop、item、item_category建模，以及这几种方式得到结果的简单平均融合，按理来说在数据量足够的情况下，对每个类别分别建一个模型应该是比全部数据一起建模效果要好的，不过线上无提升</p></li><li><p>去掉部分重要度特征不高的特征后建模，用到的特征有159个之多，试着使用特征过滤的手段去掉部分无用特征，仍然是本地有提升，线上无提升</p></li><li><p>训练集删掉前15周的数据进行建模，由于构造了很多lag特征，导致了前15周一些特征都是空值的情况，试着把这部分数据删掉，并且越早的数据对于之后的预测越没用，所以按理删掉这些数据应该是能有所提升的，但是还是本地有提升，线上无提升</p></li><li><p>... <a name="Q9Xyh"></a></p></li></ol><h1 id="总结">总结</h1><p>第一次比较投入的去参加这种比赛，感觉还是蛮靠运气和一些trick的，最后怎么弄都上不了分，不知道瓶颈卡在哪了，或许对数据做更多的探索，以及换一些深度的模型能上分吧，再接再励！ <a name="f2Vxd"></a></p><h1 id="代码">代码</h1><p><a name="qjE5G"></a></p><h2 id="数据预处理">数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并训练测试</span></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/train.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/test.csv&#x27;</span>)</span><br><span class="line">df=pd.concat([train,test]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df=df.sort_values([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用来做滑动和滞后特征的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makelag</span>(<span class="params">data,values,shift</span>):</span></span><br><span class="line">    lags=[i+shift <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>)]</span><br><span class="line">    rollings=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">15</span>)]</span><br><span class="line">    <span class="keyword">for</span> lag <span class="keyword">in</span> lags:</span><br><span class="line">        data[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=values.shift(lag)</span><br><span class="line">    <span class="keyword">for</span> rolling <span class="keyword">in</span> rollings:</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_min&#x27;</span>]=values.shift(shift).rolling(window=rolling).<span class="built_in">min</span>()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_max&#x27;</span>]=values.shift(shift).rolling(window=rolling).<span class="built_in">max</span>()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_median&#x27;</span>]=values.shift(shift).rolling(window=rolling).median()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_std&#x27;</span>]=values.shift(shift).rolling(window=rolling).std()</span><br><span class="line">        data[<span class="string">f&#x27;s_<span class="subst">&#123;shift&#125;</span>_roll_<span class="subst">&#123;rolling&#125;</span>_mean&#x27;</span>]=values.shift(shift).rolling(window=rolling).mean()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个item都做滞后和滑动特征</span></span><br><span class="line">df=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>]).apply(<span class="keyword">lambda</span> x:makelag(x,x[<span class="string">&#x27;weekly_sales&#x27;</span>],<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 价格填充特征，先用前一个值填补，再向后填补，最后没填补到的用那个item的价格众数填补</span></span><br><span class="line">df[<span class="string">&#x27;item_price_fill&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.ffill().bfill())</span><br><span class="line">df[<span class="string">&#x27;item_price_fill&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.fillna(x.mode()[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># 对于每个shop,item,item_cat,shop*item_cat,shop*item分别做价格和销量的mean/std encoding，</span></span><br><span class="line"><span class="keyword">for</span> func <span class="keyword">in</span> [<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;std&#x27;</span>]:</span><br><span class="line">    df[<span class="string">f&#x27;shop_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;category_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;item_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_cat_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_item_sale_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;weekly_sales&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;category_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_cat_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;item_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price&#x27;</span>].transform(func)</span><br><span class="line">    df[<span class="string">f&#x27;shop_item_price_<span class="subst">&#123;func&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].transform(func)</span><br><span class="line"><span class="comment"># 价格差异特征，当前价格与shop、item、item_cat、shop_cat、shop_item的价格均值的差值</span></span><br><span class="line">df[<span class="string">&#x27;shop_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;item_price_diff&#x27;</span>]=df[<span class="string">&#x27;item_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;cat_price_diff&#x27;</span>]=df[<span class="string">&#x27;category_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;shop_cat_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_cat_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;shop_item_price_diff&#x27;</span>]=df[<span class="string">&#x27;shop_item_price_mean&#x27;</span>]-df[<span class="string">&#x27;item_price_fill&#x27;</span>]</span><br><span class="line"><span class="comment"># 当前价格与上周价格的差值，当前价格与上个月价格均值的差值</span></span><br><span class="line">df[<span class="string">&#x27;week_price_diff&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x-x.shift(<span class="number">1</span>))</span><br><span class="line">df[<span class="string">&#x27;month_price_diff&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;item_price_fill&#x27;</span>].apply(<span class="keyword">lambda</span> x: x-x.shift(<span class="number">1</span>).rolling(<span class="number">4</span>).mean())</span><br><span class="line"><span class="comment"># 销量的滞后特征，对于每个item、item_cat、shop的聚合平均值</span></span><br><span class="line"><span class="keyword">for</span> lag <span class="keyword">in</span> [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">16</span>)]:</span><br><span class="line">    df[<span class="string">f&#x27;item_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">    df[<span class="string">f&#x27;cat_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;item_category_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">    df[<span class="string">f&#x27;shop_lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>]=df.groupby([<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;week&#x27;</span>])[<span class="string">f&#x27;lag_<span class="subst">&#123;lag&#125;</span>&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df.to_pickle(<span class="string">&#x27;data.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p><a name="UicNz"></a></p><h2 id="模型-2">模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df=pd.read_pickle(<span class="string">&#x27;data.pkl&#x27;</span>)</span><br><span class="line"><span class="comment"># 三折交叉</span></span><br><span class="line">cvs=[<span class="number">32</span>,<span class="number">31</span>,<span class="number">30</span>]</span><br><span class="line">params = &#123;</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;tweedie&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;tweedie_variance_power&#x27;</span>:<span class="number">1.6</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">2</span>**<span class="number">7</span>-<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">50</span>,</span><br><span class="line">        <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.6</span>,</span><br><span class="line">        <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.6</span>,</span><br><span class="line">        <span class="string">&#x27;subsample_freq&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.015</span>,</span><br><span class="line">        <span class="string">&#x27;n_estimators&#x27;</span>:<span class="number">2000</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1024</span>,</span><br><span class="line">        <span class="string">&#x27;n_jobs&#x27;</span>:-<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;silent&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">    &#125;</span><br><span class="line">y_preds=[]</span><br><span class="line">scores=[]</span><br><span class="line"><span class="keyword">for</span> cv <span class="keyword">in</span> cvs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">10</span>+<span class="built_in">str</span>(cv)+<span class="string">&#x27;=&#x27;</span>*<span class="number">10</span>)</span><br><span class="line">    train=df[df[<span class="string">&#x27;week&#x27;</span>]&lt;cv]</span><br><span class="line">    val=df[df[<span class="string">&#x27;week&#x27;</span>]==cv]</span><br><span class="line">    test=df[df[<span class="string">&#x27;week&#x27;</span>]==<span class="number">33</span>]</span><br><span class="line">    X_train=train.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>])</span><br><span class="line">    y_train=train[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    X_test=test.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    y_test=test[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    X_val=val.drop(columns=[<span class="string">&#x27;weekly_sales&#x27;</span>])</span><br><span class="line">    y_val=val[<span class="string">&#x27;weekly_sales&#x27;</span>]</span><br><span class="line">    model=lgb.LGBMRegressor(**params)</span><br><span class="line">    model.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_val,y_val)],eval_metric=[<span class="string">&#x27;mse&#x27;</span>],verbose=<span class="literal">False</span>,categorical_feature=[<span class="string">&#x27;shop_id&#x27;</span>,<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_category_id&#x27;</span>],early_stopping_rounds=<span class="number">100</span>)</span><br><span class="line">    val_pred=model.predict(X_val)*<span class="number">0.995</span></span><br><span class="line">    mse=mean_squared_error(y_val,val_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;MSE: <span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line">    scores.append(mse)</span><br><span class="line">    y_pred=model.predict(X_test)</span><br><span class="line">    y_preds.append(y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;三折交叉的score<span class="subst">&#123;scores&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;三折交叉平均score<span class="subst">&#123;np.mean(scores)&#125;</span>&#x27;</span>)</span><br><span class="line">y_pred=np.zeros_like(y_pred)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> y_preds:</span><br><span class="line">    y_pred+=t*<span class="number">1</span>/<span class="number">3</span></span><br><span class="line">sample_submit = pd.read_csv(<span class="string">&#x27;./线下商店销量预测_数据集/sample_submit.csv&#x27;</span>)</span><br><span class="line">sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>] = y_pred</span><br><span class="line">sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>] = sample_submit[<span class="string">&#x27;weekly_sales&#x27;</span>].apply(<span class="keyword">lambda</span> x:x <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>).values</span><br><span class="line">sample_submit.to_csv(<span class="string">&#x27;submit.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近参加了科大讯飞的线下商店销量预测挑战赛，线上成绩0.66，最终排名第七，这里把自己的方案分享出来，欢迎大家交流讨论！代码和数据均已上传到GitHub：&lt;br&gt;&lt;a href=&quot;https://github.com/Smallviller/KDXF_sales_forecast_competition&quot;&gt;https://github.com/Smallviller/KDXF_sales_forecast_competition&lt;/a&gt; &lt;a name=&quot;hjSac&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="时间序列" scheme="https://lukan797.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="销量预测" scheme="https://lukan797.github.io/tags/%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归的交叉熵损失函数原理</title>
    <link href="https://lukan797.github.io/2021/08/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86/"/>
    <id>https://lukan797.github.io/2021/08/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86/</id>
    <published>2021-08-06T17:54:19.354Z</published>
    <updated>2021-07-03T14:40:53.018Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子两次面试，都被问到了逻辑回归的损失函数是什么，我知道是交叉熵，也很顺利的脱口说出了他的函数表达式，但是接下来被问到了为什么要用这个损失函数，我之前见过那张图，就是这个交叉熵函数的曲面是比平方损失函数（MSE）的曲面要陡峭，更方便梯度下降算法的迭代求解，但是再被往下深挖，问还有别的原因吗，这背后的存在的数学逻辑是什么？接着又被问了一堆的极大似然估计啥啥啥数理统计的东西，就有点说不出来了，所以查了一些资料，顺便写篇文章总结一下加深下理解。 <a name="lnsnU"></a></p><h1 id="交叉熵损失函数">交叉熵损失函数</h1><p>先来熟悉下他的定义和函数形式，交叉熵（Cross Entropy）损失函数，也被称为对数损失函数，logloss，表现形式如下：</p><p><span class="math display">\[L=-[y\log(\hat y)+(1-y)log(1-\hat y)]\\\]</span></p><p>这里的<span class="math inline">\(y\)</span>代表真实值，0或1，$y $代表预测值/估计值，值为一个概率，取值范围0~1，一般大于0.5我们就判定这个样本为1，小于0.5就把这个样本归为0。</p><p>从函数的形式上，我们可以看出，无论真实的<span class="math inline">\(y\)</span>取0或1，这个加号两边的取值必有一个为0，假设<span class="math inline">\(y=1\)</span>，那么此时<span class="math inline">\(L=-\log(\hat y)\)</span>，此时损失函数代表预测为1的概率取负，如果<span class="math inline">\(y=0\)</span>，那么<span class="math inline">\(L=-log(1-\hat y)\)</span>，此时损失函数代表预测为0的概率取负，那么问题就简单了，直观上来理解这个损失函数，就是，要使得每一个样本属于其真实值的概率最大化。</p><p>虽然直观上理解这个损失函数代表的意义没有问题，但是其是怎么推导出来的呢？这样的形式会有什么样的优点呢？这里就有两种方式来理解这个损失函数了，一个是从数理统计的极大似然估计出发，另一个是从KL散度的角度出发。</p><p><a name="9rx1K"></a></p><h1 id="从极大似然估计角度理解">从极大似然估计角度理解</h1><h2 id="极大似然估计">极大似然估计</h2><p>首先需要复习一下极大似然估计是什么玩意？这个东西虽然在本科的概率论和数理统计课程中就学过了，但是还是有那么一点一知半解。</p><p>要理解极大似然估计，就得先知道这个似然函数<span class="math inline">\(p(x|\theta)\)</span>的概念，这个比较容易和概率函数搞混，因为表达式都是：<span class="math inline">\(p(x|\theta)\)</span>，但实际上似然函数（likelihood function ）与概率函数（probability function）是完全不一样的两个东西。</p><p>如果<span class="math inline">\(p(x|\theta)\)</span>中<span class="math inline">\(\theta\)</span>是已知确定的，<span class="math inline">\(x\)</span>是变量的话，那么这个函数就叫做概率函数，他描述在给定的模型参数<span class="math inline">\(\theta\)</span>下，对于不同的样本点<span class="math inline">\(x\)</span>，其出现的概率是多少，比如对于身高的正态函数，给定参数均值170和标准差10，那么就可以计算出现身高为180的人的概率有多少。</p><p>反过来，如果<span class="math inline">\(p(x|\theta)\)</span>中<span class="math inline">\(x\)</span>是已知确定的，<span class="math inline">\(\theta\)</span>是变量的话，那么这个函数就叫做似然函数，他描述对于不同的模型参数<span class="math inline">\(\theta\)</span>，出现$x $这个样本的概率是多少，还是身高的那个例子，如果给定一个样本身高为180，那么就可以计算不同的均值和标准差参数组合下出现这个样本的概率。</p><p>那么，极大似然估计是什么意思呢？就是<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，</strong>举个例子，我给了一堆人的身高，这些样本都是独立同分布的，然后知道身高是符合正态分布的，我想要推出人群中身高的均值和标准差是多少，那么就可以通过遍历每一个参数值，然后根据似然函数算出每一个人身高对应的概率是多少，因为是这些人是独立同分布的，所以就可以通过把这些概率乘起来的方式，来计算出一个出现这些样本的概率，然后选取最大概率对应的那个均值和标准差，这个均值和标准差就是想要的结果了。 <a name="rH0z9"></a></p><h2 id="逻辑回归参数的极大似然估计">逻辑回归参数的极大似然估计</h2><p>了解了极大似然估计，接下来就可以说一下啊逻辑回归的参数是怎么通过极大似然估计来进行估计的了。首先，根据逻辑回归的计算公式，我们可以知道对应为1和0的样本的概率：</p><p><span class="math display">\[\begin{align*}P(Y=1|x)&amp;=\frac{e^{wx+b}}{1+e^{wx+b}}=p(x)\\P(Y=0|x)&amp;=\frac{1}{1+e^{wx+b}}=1-p(x)\end{align*}\]</span></p><p>然后就可以计算出现这些样本的似然函数，就是把每一个样本的概率乘起来：</p><p><span class="math display">\[L(w;b)=\prod_{i=1}^{n}[p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\]</span></p><p>但是这个形式是连乘的，并不好求，所以一般我们会把他取对数，转化为累加的形式，就得到对数似然函数：</p><p><span class="math display">\[L&#39;(w;b)=\sum_{i=1}^{n}[y_i\log(p(x_i))+(1-y_i)log(1-p(x_i))]\\\]</span></p><p>这时候呢，我们就可以通过最大化这个对数似然函数的方式来求得逻辑回归模型中的<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>，把上面的式子加个负号，就是通过最小化这个负对数似然函数来求得<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>，就可以通过梯度下降法来进行求解了。</p><p>可以发现，通过数理统计中的极大似然估计方法，也可以得到逻辑回归的损失函数。 <a name="8zkfK"></a></p><h1 id="从kl散度的角度理解">从KL散度的角度理解</h1><p>交叉熵是信息论里面的概念，要理解这里的交叉熵是怎么推出来的，就得先理解以下一个叫做KL散度（相对熵）的东西。</p><p>如果对于同一个随机变量<span class="math inline">\(X\)</span>有两个单独的概率分布<span class="math inline">\(p(X)\)</span>和<span class="math inline">\(q(X)\)</span>，那么我们就可以<strong>用KL散度来衡量这两个分布的差异</strong>：</p><p><span class="math display">\[D_{KL}(p||q)=\sum_{i=1}^{n}p(x_i)\log(\frac{p(x_i)}{q(x_i)})\\\]</span></p><p>我们将<span class="math inline">\(p(x)\)</span>定义为真实的概率分布，<span class="math inline">\(q(x)\)</span>定义为模型预测的概率分布，我们希望预测的概率分布与真实的概率分布差异越小越好，也就是使得KL散度越小越好，而<span class="math inline">\(p(x)\)</span>是在数据集确定之后就确定下来的了，所以我们只要使得<span class="math inline">\(q(x)\)</span>尽可能地接近<span class="math inline">\(p(x)\)</span>就可以了。</p><p>将这个KL散度的公式展开可以得到：</p><p><span class="math display">\[\begin{align*}D_{KL}(p||q)&amp;=\sum_{i=1}^{n}p(x_i)\log(\frac{p(x_i)}{q(x_i)})\\&amp;=\sum_{i=1}^{n}p(x_i)\log(p(x_i))-\sum_{i=1}^{n}p(x_i)\log(q(x_i))\\&amp;=-H(p(x))-\sum_{i=1}^{n}p(x_i)\log(q(x_i))\end{align*}\]</span></p><p>学过信息论的可能会知道，<span class="math inline">\(-\log(p(x))\)</span>代表的就是<strong>信息量</strong>，某一随机事件发生的概率越小，反映的信息量就越大，比如新冠疫情的发生，概率很小，但是蕴含的信息量就很大，而这个<span class="math inline">\(-\sum_{i=1}^{n}p(x)\log(p(x))\)</span>代表的就是信息量的期望，也就是<strong>信息熵</strong>，然后如果把这个<span class="math inline">\(log\)</span>里面的<span class="math inline">\(p(x)\)</span>换成另一个分布的概率<span class="math inline">\(q(x)\)</span>，也就是<span class="math inline">\(-\sum_{i=1}^{n}p(x)\log(q(x))\)</span>，这个就是<strong>交叉熵。</strong></p><p>所以根据上面那个展开的公式，就可以发现<strong>KL散度=交叉熵-真实分布的信息熵</strong>，而这个真实分布的信息熵是根据<span class="math inline">\(p(x)\)</span>计算得到的，而这个<span class="math inline">\(p(x)\)</span>是在数据集确定之后就确定下来的了，这一项就可以当成一个常数项，所以我们如果想让KL散度越小，只需要让交叉熵越小越好了，因此就可以直接将逻辑回归的损失函数直接定义为交叉熵。 <a name="pihcD"></a></p><h1 id="使用交叉熵作为损失函数的好处">使用交叉熵作为损失函数的好处</h1><p>从上面的两个角度，我们就可以理解为什么逻辑回归要用交叉熵来作为损失函数了，但是，使用交叉熵背后的数学逻辑是明白了，那么，反映到实际里面，交叉熵到底有着什么样的优越性呢？</p><p>这里使用之前自己上数据挖掘课程ppt里的一张图来说明这个问题，可以看到，交叉熵函数的曲面是非常陡峭的，在模型效果差的时候学习速度比较快，是非常有利于梯度下降的迭代的，所以逻辑回归里面使用交叉熵作为损失函数而不是使用均方误差作为损失函数，这个也可以通过求导的方式来证明，不过限于个人水平，这里就不展开了，具体可以间文末列出的的第三篇参考资料。<img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1625298163597-63b86d09-180b-4c0d-a6e3-ab1cab0f20d9.png" alt="image.png" /> <a name="I1fZO"></a></p><h1 id="总结">总结</h1><p>本文主要从两个角度——数理统计的极大似然估计以及信息论中的KL散度，来说明逻辑回归中交叉熵函数背后的数学逻辑，同时也简单说明了交叉熵函数在逻辑回归中相对于均方误差函数的优势。 <a name="7WUkf"></a></p><h1 id="参考">参考</h1><p><a href="https://mp.weixin.qq.com/s/LPfrzLCVBj3SUQAf9fnlmA">为什么逻辑回归的损失函数是交叉熵？</a></p><p><a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计</a></p><p><a href="https://zhuanlan.zhihu.com/p/35709485">损失函数|交叉熵损失函数</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前阵子两次面试，都被问到了逻辑回归的损失函数是什么，我知道是交叉熵，也很顺利的脱口说出了他的函数表达式，但是接下来被问到了为什么要用这个损失函数，我之前见过那张图，就是这个交叉熵函数的曲面是比平方损失函数（MSE）的曲面要陡峭，更方便梯度下降算法的迭代求解，但是再被往下深挖，问还有别的原因吗，这背后的存在的数学逻辑是什么？接着又被问了一堆的极大似然估计啥啥啥数理统计的东西，就有点说不出来了，所以查了一些资料，顺便写篇文章总结一下加深下理解。 &lt;a name=&quot;lnsnU&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="逻辑回归" scheme="https://lukan797.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    <category term="损失函数" scheme="https://lukan797.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>时间序列的多步预测方法总结</title>
    <link href="https://lukan797.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan797.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-06T17:54:19.202Z</published>
    <updated>2021-07-16T11:15:03.894Z</updated>
    
    <content type="html"><![CDATA[<p>在时间序列预测中，预测的horizon往往是一段时间，比如下一周的股票价格、销量、天气等等，但是，在将时间序列数据转化为有监督学习时，往往会构造很多特征，其中一个很重要的就是滞后值特征和滑动窗口统计特征，一旦加入这些特征，就会导致有监督学习的多步预测出现问题，比如，我需要构造了一个滞后一天的特征lag1，需要预测接下来两天的值，那么，第一天的是很好预测的，因为我有昨天的值，但是第二天的预测就有问题了，因为昨天的观测值是不知道的啊，在上一篇文章中，我提到了一个递归预测法，但这两天看了一下，其实解决这个问题的方法还不少，所以写篇文章总结下吧。 <a name="kfo8C"></a></p><h1 id="直接预测法">直接预测法</h1><p>直接预测法（Direct Multi-step Forecast Strategy），这种方法的思路呢就是，如果不能使用lag特征，那我干脆就不用了。这种方法的可操作空间还是挺大的，可以分为只使用1个模型，使用n个模型（n为需要预测的天数），使用1-n个模型。接下来详细说明下每一种方法。 <a name="bDO6i"></a></p><h2 id="只使用一个模型">只使用一个模型</h2><p>举个例子，现有7月10号-7月15号的数据，需要预测未来3天的销量，那么，我就不能用lag1和lag2作为特征，但是可以用lag3呀，所以就用lag3作为特征构建一个模型：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626280659659-db52b363-6f31-44e1-96a7-5d33e8b327f5.png#align=left&amp;display=inline&amp;height=224" alt="image.png" /><br />这种是只使用一个模型来预测的，但是呢，缺点是特征居然要构造到lag3，lag1和lag2的信息完全没用到，所以就有人提出了一种思路，就是对于每一天都构建一个模型。 <a name="bxmLJ"></a></p><h2 id="使用n个模型">使用n个模型</h2><p>这个的思路呢，就是想能够尽可能多的用到lag的信息，所以，对于每一天都构建一个模型，比如对于15号，构建模型1，使用了lag1，lag2和lag3作为特征来训练，然后对于16号，因为不能用到lag1的信息了，但是lag2和lag3还是能用到的，所以就用lag2和lag3作为特征，再训练一个模型2，17号的话，就只有lag3能用了，所以就直接用lag3作为特征来训练一个模型3，然后模型123分别就可以输出每一天的预测值了。<br /><strong><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626281132143-f39be164-ed9e-4fe7-815e-97fa8cd78b73.png#align=left&amp;display=inline&amp;height=227" alt="image.png" /></strong><br />这种方法的优势是最大可能的用到了lag的信息，但是缺陷也非常明显，就是因为对于每一天都需要构建一个模型的话，那预测的天数一长，数据一多，那计算量是没法想象的，所以也有人提出了一个这种的方案，就不是对每一天构建一个模型了，而是每几天构建一个模型。 <a name="yyygU"></a></p><h2 id="使用1-n个模型">使用1-n个模型</h2><p>还是上面那个例子，这次把数据改变一下，预测四天吧，有10号-15号的数据，构建了lag1-5的特征，需要预测16号-19号的数据，那么我们知道16号和17号是都可以用到lag2和lag3的特征的，那么为这两天构建一个模型1，而18号和19号是可以用到lag4和lag5的特征的，那么为这两天构建一个模型2，所以最后就是模型1输出16号和17号的预测值，模型2输出18号和19号的值。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626282065731-86deeff3-8259-4320-998c-5fc639e8b0a4.png" alt="image.png" /><br />可以发现，这样的话，我们虽然没有尽最大可能的去使用lag特征，但是，计算量相比于使用n个模型直接小了一半。这是<a href="https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216">kaggle M5比赛第四名</a>的思路。 <a name="M1d8v"></a></p><h1 id="递归预测法">递归预测法</h1><p>然后是递归预测法（Recursive Multi-step Forecast），不知道预测值对应的滞后值怎么办？就用之前的预测值当真实值呗！举个例子，有10号-15号的数据，构建了lag1特征，需要预测未来3天的销量，那么15号的lag1特征可以直接用14号的值，假设预测出来结果是150，那么，在16号，lag1的真实值也就是15号的值虽然不知道，但是可以直接用15号的预测值填充呀，依次类推，17号的lag1也可以直接用16号的预测值填充，这就是递归预测法。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626279980227-c5384d4d-b533-45e2-89d6-37a16c66b09d.png" alt="image.png" /><br />但是，这种方法有一个缺陷就是会造成误差累计，还是上面那个例子，假设我15号那天预测错了，那么16号那天的输入就也是错的，那用来预测就更错了啊，所以，使用这种方法的话，一旦预测出错就会越错越离谱，这种方法会有着较高的偏差。 <a name="p5e7m"></a></p><h1 id="直接-递归混合预测法">直接-递归混合预测法</h1><p>直接预测法使用到的lag信息少，并且需要建的模型多，方差较大，递归预测法只使用了一个模型，并且lag的信息也全用上了，但是容易造成误差累计，偏差较大。所以，有人把上面两种方法直接结合了起来，试图平衡方差和偏差，这里就叫直接-递归混合预测法吧，混合的方式还挺多的，我看到的就三种了。 <a name="4dR72"></a></p><h2 id="混合一">混合一</h2><p>同时使用直接法和递归法，分别得出一个预测值，然后做个简单平均，这个思路也就是采用了模型融合的平均法的思想，一个高方差，一个高偏差，那么我把两个合起来取个平均方差和偏差不就小了吗，这个方法是<a href="https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163684">kaggle M5比赛top1</a>用的解决方案。 <a name="HVhMA"></a></p><h2 id="混合二">混合二</h2><p>这种方法是这篇论文提出的：《Recursive and direct multi-step forecasting: the best of both worlds》，有兴趣可以自己去读下，大概说的就是先使用递归法进行预测，然后再用直接法去训练递归法的残差，有点像boosting的思想，论文花了挺大篇幅说了这种方法的无偏性，不过，这种方法也就是存在论文中，暂时没见到人使用，具体效果还不知道。<br /> <a name="XT4Vm"></a></p><h2 id="混合三">混合三</h2><p>简单来说就是使用到了所有的lag信息，同时也建立了很多模型，还是这个例子，首先用10号-14号的数据训练模型1，得到15号的预测值，然后将15号的预测值作为16号的特征，同时用10号-15号的数据训练模型2，得到16号的预测值，最后使用16号的预测值作为17号的特征，使用10号-16号的数据训练模型3，得到17号的预测值。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1626279980227-c5384d4d-b533-45e2-89d6-37a16c66b09d.png#align=left&amp;display=inline&amp;height=228、" alt="image.png" /><br />这种方法说实话我不能很get到他的好处在哪，相比于递归预测法，不就是训练时多了几条数据吗？还是会有误差累计的问题吧，或许是我没有理解明白吧，<a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582">kaggle favorita-grocery第一名</a>的方案好像也使用的这个 <a name="mlt1S"></a></p><h1 id="多输出法">多输出法</h1><p>在传统的机器学习中，是无法实现多输出的，只能输出一个值，但是在深度学习的模型中，就可以通过调节输出神经元的个数，从而实现多输出的功能，还有一些是使用seq2seq结构的，深度这块的时间序列预测目前了解的比较少，这里不再展开了。 <a name="h7wpx"></a></p><h1 id="总结">总结</h1><p>目前针对时间序列预测的多步输出问题大概就这几种方法，其中针对机器学习的直接法、递归法还有直接-递归混合法，这几种方法在kaggle上都有应用，也没有说哪种方法就一定好，这个需要就具体问题具体分析，多尝试一下才能知道在某种问题上哪种方法表现更好。 <a name="mSH2v"></a></p><h1 id="参考">参考</h1><ol type="1"><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting/">4 Strategies for Multi-Step Time Series Forecasting</a></li><li><a href="https://zhuanlan.zhihu.com/p/308764952">时间序列多步预测的五种策略</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在时间序列预测中，预测的horizon往往是一段时间，比如下一周的股票价格、销量、天气等等，但是，在将时间序列数据转化为有监督学习时，往往会构造很多特征，其中一个很重要的就是滞后值特征和滑动窗口统计特征，一旦加入这些特征，就会导致有监督学习的多步预测出现问题，比如，我需要构造了一个滞后一天的特征lag1，需要预测接下来两天的值，那么，第一天的是很好预测的，因为我有昨天的值，但是第二天的预测就有问题了，因为昨天的观测值是不知道的啊，在上一篇文章中，我提到了一个递归预测法，但这两天看了一下，其实解决这个问题的方法还不少，所以写篇文章总结下吧。 &lt;a name=&quot;kfo8C&quot;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="时间序列" scheme="https://lukan797.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>时间序列数据的特征工程总结</title>
    <link href="https://lukan797.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>https://lukan797.github.io/2021/08/07/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%80%BB%E7%BB%93/</id>
    <published>2021-08-06T17:54:19.175Z</published>
    <updated>2021-07-10T17:04:17.272Z</updated>
    
    <content type="html"><![CDATA[<p>当下时间序列预测的方法主要有三种吧，第一个是传统的时间序列预测方法，典型代表有ARIMA和指数平滑法；第二个是基于机器学习的方法，目前用的最多的是lightgbm和xgboost，在很多时序预测比赛前几名的方案都可以看到这两种方法；第三个是基于深度学习的方法，如RNN、LSTM等。现在传统时序预测的方法的预测精度都已经不如基于机器学习和深度学习的方法了，但是后者依赖于特征工程，特征调教的好的话是可以达到很高的预测精度的，因此，本文就总结下时间序列数据常见的特征工程方法。<br /></p><p>一个典型的时间序列数据，会包含以下几列：时间戳，时序值，序列的属性变量，比如下图，日期就是时间戳，销量就是时序值，如果是多序列的话可能还会有序列的属性变量，如城市、产品、价格等。<br /> <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1625930948808-5fa93c14-0cbd-4aa6-bcb3-72da3ce43176.png#align=left&amp;display=inline&amp;height=151" alt="image.png" /><br /> 因此，时间序列的特征工程也大多是基于这三个数据衍生出来的： <img src="https://cdn.nlark.com/yuque/0/2021/jpeg/764062/1625934882012-f1feb04c-b9a7-4dda-85db-53aef26727ca.jpeg" />接下来将一一展开： <a name="ayMJm"></a></p><h1 id="时间戳衍生的特征">时间戳衍生的特征</h1><p>时间戳虽然只有一列，但是也可以根据这个就衍生出很多很多变量了，具体可以分为三大类：时间特征、布尔特征，时间差特征 <a name="JLCjk"></a></p><h2 id="时间特征">时间特征</h2><ul><li>年</li><li>季度</li><li>月</li><li>周</li><li>天：一年、一月、一周的第几天</li><li>小时</li><li>分钟</li><li>... <a name="YHwlX"></a></li></ul><h2 id="布尔特征">布尔特征</h2><ul><li>是否年初/年末</li><li>是否月初/月末</li><li>是否周末</li><li>是否节假日</li><li>是否特殊日期</li><li>是否早上/中午/晚上</li><li>... <a name="8UvOr"></a></li></ul><h2 id="时间差特征">时间差特征</h2><ul><li>距离年初/年末的天数</li><li>距离月初/月末的天数</li><li>距离周末的天数</li><li>距离节假日的天数</li><li>距离特殊日期的天数</li><li>... <a name="wnRgL"></a></li></ul><h1 id="时序值衍生的特征">时序值衍生的特征</h1><p>因为时间序列是通过历史来预测未来，那么，这个时序值的历史数据，也就是当前时间点之前的信息就非常有用，通过他可以发现时间序列的趋势因素、季节性周期性因素以及一些不规则的变动，具体来说这部分特征可以分为三种：滞后值、滑动窗口统计和拓展窗口统计。 <a name="YXNqf"></a></p><h2 id="滞后值">滞后值</h2><p>也称lag feature，比如对于t时刻的数据，我们认为他是跟昨天的数据、上周同一天的数据、上个月同一天的数据、去年同期的数据是高度相关的，那么，我们就可以将t-1、t-7、t-30、t-365的数据用来做特征。<br />但是在使用滞后值作为特征时需要注意一点，就是当在进行多步预测的时候，如果预测的horizon超过了滞后的期数，那么这时候就得使用递归的方式，将先前预测的值作为特征，举个例子，使用滞后一期的值作为特征，当前时间点为2021-07-10，我要预测2021-07-11和2021-07-12的股票价格，那么2021-07-11的值是可以预测的，因为我有2021-07-10的数据，但是2021-07-12的数据就不行了，因为我没有2021-07-11的数据，所以这时候一种做法就是将先前2021-07-11的预测值直接作为特征的输入，对于这种预测就得一行一行来，预测一行，拿预测值作为输入，再预测一行，再得到预测值，再预测一行，以此类推... <a name="P4fWX"></a></p><h2 id="滑动窗口统计">滑动窗口统计</h2><p>除了使用原始Lag值作为特征，还可以使用先前时间观察值的统计信息作为特征，这种类型的特征叫做滑动窗口统计，Rolling Window Statistics。比如对于t时刻，我们可以取前七天的统计值作为特征，也就是将t-1~t-8这个时间段数据的平均数、中位数、标准差、最大值、最小值等作为特征，这里指定的window就是7，也可以根据需要指定14，30等，可以发现，上面说的滞后值特征其实就是一种特殊的滑动窗口，他的window=1，然后滑动窗口统计也是可以指定滞后的期数来衍生出更多的特征的，比如七天前那个时刻的前七天数据的统计量。<br />同理，在构造这种特征的时候，也需要注意一下在多步预测时可能出现的问题。 <a name="pxjRE"></a></p><h2 id="扩展窗口统计">扩展窗口统计</h2><p>另一种特征叫做扩展窗口统计（Expanding Window Statistics），其实也算是一种特殊的滑动窗口统计，不过他用来统计的数据是整个序列全部的数据，统计值可以是平均数、中位数、标准差、最大值、最小值等，这种特征一般是用在多序列建模，比如不同的股票价格，可能会有着不同的内在属性，在预测的时候用这个特征作为区分也是一种方式。 <a name="KCYYO"></a></p><h1 id="序列属性衍生的特征">序列属性衍生的特征</h1><p><a name="CpZXA"></a></p><h2 id="连续变量衍生">连续变量衍生</h2><p>一个序列可能会伴有多个连续变量的特征，比如说对于股票数据，除了收盘价，可能还会有成交量、开盘价等伴随的特征，对于销量数据，可能还会伴随有价格的特征。对于这种连续变量，可以直接作为一个特征，也可以像之前时序值衍生的特征那样做处理，或者也可以与先前的数据做差值，比如t时刻的价格减去t-1时刻的价格。但是一般这种连续变量使用不多，因为这些值在未来也很可能是不可知的，那怎么能当成造特征呢？比如我要预测明天股票的收盘价，要用成交量作为一个特征，但是我怎么知道明天的成交量呢？这又是一个预测问题了。 <a name="Ui5Tb"></a></p><h2 id="类别变量encoding">类别变量Encoding</h2><p>对于类别型变量，如果类别比较少，一般在机器学习里做的处理是one-hot encoding，但是如果类别一多，那么生成的特征是会很多的，容易造成维度灾难，但是也不能随便用label encoding，因为很多时候类别是不反应顺序的，如果给他编码成1、2、3、4、5，对于一些树模型来说，在分裂节点的时候可不管这些是类别型还是连续型，通通当作连续型来处理，这是有先后顺序的，肯定不能这么做。所以就有这么一种方式，就是和y做特征交互，比如预测销量，有一个特征是产品类别，那么就可以统计下这个产品类别下的销量均值、标准差等，这种其实也算是上面扩展窗口统计的一种。 <a name="J9fMZ"></a></p><h1 id="参考">参考</h1><ol type="1"><li>《美团机器学习实践》</li><li><a href="https://mp.weixin.qq.com/s/dUdGhWY8l77f1TiPsnjMQA">一度让我怀疑人生的时间戳特征处理技巧。</a></li><li><a href="https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/">Basic Feature Engineering With Time Series Data in Python</a></li><li><a href="https://blog.csdn.net/fitzgerald0/article/details/104029842">时间序列树模型特征工程汇总</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;当下时间序列预测的方法主要有三种吧，第一个是传统的时间序列预测方法，典型代表有ARIMA和指数平滑法；第二个是基于机器学习的方法，目前用的最多的是lightgbm和xgboost，在很多时序预测比赛前几名的方案都可以看到这两种方法；第三个是基于深度学习的方法，如RNN、LSTM等。现在传统时序预测的方法的预测精度都已经不如基于机器学习和深度学习的方法了，但是后者依赖于特征工程，特征调教的好的话是可以达到很高的预测精度的，因此，本文就总结下时间序列数据常见的特征工程方法。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="时间序列" scheme="https://lukan797.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
    <category term="特征工程" scheme="https://lukan797.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>云服务器配置远程jupyter notebook环境</title>
    <link href="https://lukan797.github.io/2021/08/07/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/"/>
    <id>https://lukan797.github.io/2021/08/07/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8Bjupyter%20notebook%E7%8E%AF%E5%A2%83/</id>
    <published>2021-08-06T17:54:19.076Z</published>
    <updated>2021-06-30T18:36:39.236Z</updated>
    
    <content type="html"><![CDATA[<p>去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：</p><ol type="1"><li>通过网址随时随地都能打开编程</li><li>配置了适合编程的主题色调</li><li>加入了插件补全功能</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624879339187-e21ea69c-5a13-4f9e-9a78-278e3a86edb6.png#height=365&amp;id=dR0gt&amp;" alt="image.png" /><br />前几天因为折腾自己的服务器环境给搞崩了，数据库出了点问题，所以只能重装系统，导致jupyter notebook又要重装一遍，然后几个月后服务器到期，估计又要重新配一遍环境，就索性写一篇教程，供自己日后和有需要的人参考。</p><h1 id="云服务器选购">云服务器选购</h1><p>首先需要选购一个云服务器，推荐腾讯云或者阿里云，有学生认证的话一年大概100左右，操作系统推荐是用目前主流的两个Linux发行版，ubuntu和cent OS，两个系统在一些安装软件的命令上会有小差异，我这里用的是ubuntu。</p><h1 id="安装anaconda">安装Anaconda</h1><p>在买好云服务器后，就通过ssh连接，就可以用命令行进行操作了，首先第一步是安装anaconda，先要下载anaconda的安装包，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure> 下载好后直接安装： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898431857-7b2a0ec6-2970-4c6f-9af3-07dc336f9a3d.png#height=72&amp;id=DlHXa&amp;" alt="image.png" /><br />会弹出这样一个界面，直接一直回车，然后输入yes继续回车，等待安装完成即可，安装完成会有这样一个界面，就代表安装完成了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624898994446-2e991619-5f16-4b5f-bd61-3292b2fc53f7.png#height=126&amp;id=P7Dy3" alt="image.png" /></p><h1 id="配置jupyter-notebook环境">配置jupyter notebook环境</h1><p>接下来就可以配置jupyter notebook环境了，首先需要生成一个配置文件，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure> 因为服务器的安全性，配置远程访问是需要设置一个密码的，输入命令，生成密钥： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure> 输入两次密码，这里就会生成一个密钥放在用户文件夹的.jupyter文件夹下，和刚刚的配置文件路径一样，这两个文件会自动关联起来，在修改配置文件的时候就不需要加跟密钥相关的命令了。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624899427258-a0a09979-4f70-4935-9933-d058534df757.png#height=52&amp;id=i9wwI" alt="image.png" /><br />接下来就可以直接修改刚刚生成的那个配置文件了，使用vim打开，输入命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure> 按键盘的i键进入编辑模式，直接在开头添加以下内容： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&#x27;*&#x27; # 代表任意ip是都可以访问jupyter</span><br><span class="line">c.NotebookApp.notebook_dir=&#x27;/home/ubuntu/jupyter&#x27; # notebook的工作目录，可以自己的实际情况修改，注意要确保目录存在</span><br><span class="line">c.NotebookApp.open_browser = False # 不打开浏览器</span><br><span class="line">c.NotebookApp.port =8888  #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure> 按Esc键退出编辑模式，然后输入:wq保存即可。</p><h1 id="开启远程访问">开启远程访问</h1><p>我们在上一步中指定了端口为8888，也让所有ip都能够访问这个端口了，但是在云服务器中还需要把这个端口开启起来，以腾讯云为例，进入安全组中，添加入站规则，按如下设置，然后在出站规则里点击一键放通，入站规则和出站规则都需要配置好<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624900683679-df0724ff-dcb9-4091-bf70-b9cb926120f2.png#height=205&amp;id=p7ggP" alt="image.png" /><br />接下来就可以将jupyter notebook打开了，不过我们需要能够将notebook一直在后台挂着，所以这里就输入这个命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure> 这里nohup（no hang up）是不挂起的意思，用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行，最后面的<strong>&amp;</strong>是让命令在后台执行，终端退出后命令仍旧执行，&gt; jupyter.log 2&gt;&amp;1是输出日志的意思，把命令的输出和错误都写到jupyter.log这个文件中，方便监控。<br />接下来我们在浏览器中输入：服务器公网ip:端口号，即可访问jupyter，如图所示，再输入刚刚设置的密码就行了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903214506-5212ba31-e39b-497e-bbb4-200892853bb9.png#height=390&amp;id=y6mOt" alt="image.png" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624903977925-09aa1daf-f1f3-40e2-ad21-8d1f42269d66.png#height=229&amp;id=SPHn7" alt="image.png" /></p><h1 id="装代码补全插件与更换主题">装代码补全插件与更换主题</h1><p>在上一步中，我们已经配置好了一个可以远程访问的jupyter notebook，但是呢，这个notebook的主题是默认的，白色太亮眼不适合编程，而且，默认的jupyter notebook也没有补全代码的功能，所以就通过插件的方式来解决这两个问题。</p><h2 id="补全代码插件">补全代码插件</h2><p>依次执行以下命令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions </span><br><span class="line">jupyter contrib nbextension install --user</span><br><span class="line">pip install jupyter_nbextensions_configurator</span><br><span class="line">jupyter nbextensions_configurator enable --user</span><br></pre></td></tr></table></figure> 这样插件就装好了</p><h2 id="更换主题">更换主题</h2><p>首先安装jupyterthemes： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyterthemes</span><br></pre></td></tr></table></figure> jupyterthemes是一个为jupyter notebook设置主题的插件，可以在github上查看他们的使用手册，<br /> 这里推荐自己的一套配置方案，在命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jt -t chesterish -f roboto -fs 12 -ofs 105 -dfs 95 -lineh 150</span><br></pre></td></tr></table></figure><h2 id="重启jupyter-notebook">重启jupyter notebook</h2><p>如果你的notebook在运行中，就需要重启一下才能使得上一步的修改生效，首先找到运行jupyter notebook的进程id，然后杀掉这个进程，再重启就可以了 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ps -aux|grep jupyter</span><br><span class="line">sudo kill -9 进程id</span><br><span class="line">nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure> 可以发现，现在界面已经跟刚才的不一样了：<br /></p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904807402-493d550d-ae8c-4963-82bf-69a99fff310e.png#height=435&amp;id=gz8pT" alt="image.png" /><br /></p><p>然后还需要在Nbextensions中开启下Hinterland，也就是我们的补全插件<br /> <br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624384565647-dab6f490-7909-4501-a903-1cab72df72e0.png?x-oss-process=image%2Fresize%2Cw_1404#height=722&amp;id=AIyRA&amp;originHeight=722&amp;originWidth=1404&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1404" /></p><p>然后就大功告成了，有一个养眼的界面和补全代码的功能，就可以随时随地都能用搭建好的这个环境写一些代码了</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1624904949851-5191b2d7-4550-423d-b53c-71f609b93a10.png#height=404&amp;id=QiYzW" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;去年年初疫情，阿里云搞了一个在家实践的活动，就免费领了半年的云服务器，从此打开了新世界的大门，比如写一些脚本在挂服务器上跑一些代码，搭一个网站，还有一个就是可以在服务器上搭一个jupyter notebook的环境，通过网址就可以直接打开notebook写代码了，适合方便快速地写一些小型的代码，或者在手头的电脑没有anaconda环境时直接使用，甚至用ipad或者手机也可以写，大致的效果如下：&lt;/p&gt;</summary>
    
    
    
    <category term="教程" scheme="https://lukan797.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="jupyter notebook" scheme="https://lukan797.github.io/tags/jupyter-notebook/"/>
    
    <category term="云服务器" scheme="https://lukan797.github.io/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>GBDT梯度提升树算法原理</title>
    <link href="https://lukan797.github.io/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"/>
    <id>https://lukan797.github.io/2021/06/17/GBDT%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/</id>
    <published>2021-06-16T16:00:00.000Z</published>
    <updated>2021-06-30T18:35:14.700Z</updated>
    
    <content type="html"><![CDATA[<p>GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！</p><h1 id="gbdt核心思想">GBDT核心思想</h1><p>GBDT，全称为Gradient Boosting Decision Tree，顾名思义，分为两部分， Gradient Boosting和Decision Tree，具体来说，就是使用<strong>决策树</strong>为基学习器的<strong>梯度提升算法</strong>，因此分别了解下这两部分的原理就可以很好的学习GBDT的思想。</p><h2 id="decision-tree">Decision Tree</h2><p>先说决策树部分，由于在GBDT中，每次迭代都需要用到<strong>负梯度</strong>，求<strong>负梯度</strong>就需要用到连续值，因此GBDT用的决策树都是CART（classification and regression tree）回归树，而不是分类树，CART回归树一般使用的节点分裂准则为平均平方误差MSE，即在节点分裂时穷举每一个特征的每一个阈值，来寻找最优切分特征和最优切分点，衡量的方法是平方误差最小化，这部分算法在很多机器学习的书中都有涉及，这里不详细展开。</p><h2 id="gradient-boosting">Gradient Boosting</h2><p>在说Gradient Boosting之前，先讲下boosting算法，所谓的boosting，就是使用前向分布算法（Forward Stagewise Algorithm）来训练加法模型，这个加法模型表现如下：<br /><span class="math display">\[f_M(x)=\sum_{i=1}^{M}T_(x,\Theta)\\\]</span><br />这里<span class="math inline">\(f_M(x)\)</span>表示的就是经过M次迭代后最终得到的模型，而<span class="math inline">\(T_(x,\Theta)\)</span>代表的就是单颗CART回归树，这里的<span class="math inline">\(\Theta\)</span>代表树的参数，这个模型的意义就是每次训练一个弱学习器<span class="math inline">\(T(x,\Theta)\)</span>，经过M次之后给他们加总起来得到一个强的学习器<span class="math inline">\(f_M(x)\)</span>，那么怎么来训练这个加法模型呢？<br />我们的目标是使得最后的预测误差尽可能的小，但是预测集我们事先是不知道的，我们只能使得训练集的训练误差最小，那么如何使得这种加法模型的训练误差最小？有一个方法就是在每次训练模型的时候都拟合上一个模型预测值与真实值之间的差值<span class="math inline">\(y-\hat{y}\)</span>，也就是残差，然后在把这个模型加到原来的模型中，这样就可以使得更新后的模型的预测误差更小，这种方法也被称为前向分布算法，如果基学习器是线性回归的话就是前向分布回归，关于前向分布回归可以参考我上一篇文章：<a href="https://zhuanlan.zhihu.com/p/369732767">前向分步回归Forward Stagewise Regression原理及Python实现</a>，而这里我们用的基学习器是树模型，这种方法也被称为提升树（boosting tree），具体步骤如下：</p><ol type="1"><li>初始化<span class="math inline">\(f_0(x)=0\)</span></li><li>对于<span class="math inline">\(m=1,2,3...M\)</span><ol type="1"><li>计算残差：<span class="math inline">\(r_{mi}=y_i-f_{m-1}(x),i=1,2,3,..N\)</span></li><li>以残差<span class="math inline">\(r_{mi}\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></li><li>更新<span class="math inline">\(f_m(x)=f_{m-1}(x)+T_m(x,\Theta)\)</span></li></ol></li><li>经过M次迭代后得到最终的提升树模型：<span class="math inline">\(f_{M}(x)=\sum_{i=1}^{M}T(x,\Theta)\)</span></li></ol><p>提升树每次迭代都拟合上一次模型中所没有能够学习到的东西，也就是残差，这使得提升树是肯定比一般决策树要强的，但是呢，这种拟合残差的方式也会存在一定的缺陷，这个先留到后面说，因此，基于提升树的改进算法就诞生了，也就是GBDT。<br />从上述提升树算法中，我们可以发现，boosting的核心思想就是在每次训练模型的时候，都用新的模型来弥补之前模型的不足，使得模型越来越好，这实际上就是一个优化问题，提升树每次迭代的时候优化的就是残差，那有没有存在更好的优化方式呢？当然有，就是将这个模型的“不足”用损失函数的方式来表示，在每次迭代时优化这个损失函数就行了！<br />损失函数可以用<span class="math inline">\(L(y,f(x))\)</span>，这里先用一个通用的形式来表示，具体的损失函数可以是各种各样的，比如回归问题的MSE,MAE，分类问题的交叉熵，然后我们的目标就是使得这个损失函数最小，问题就来了，怎么优化这个损失函数使他变小呢？<br />根据凸优化的理论，梯度的方向是函数值增长最快的方向，那么负梯度的方向自然就是函数值下降最快的方向，所以根据负梯度就可以找到函数值最小的地方，那么，我们就可以通过拟合负梯度的形式来优化损失函数，也就是说，<strong>在每次训练一个新模型的时候，都拟合上一个模型的损失函数的负梯度值，然后把这个新模型加到原来的模型中，这样必然使得更新后的模型的损失函数更小</strong>，这个就是Gradient Boosting了，具体步骤如下：</p><ol type="1"><li>初始化：$f_0(x)=0 $</li><li>对于<span class="math inline">\(m=1,2,3...M\)</span><ol type="1"><li><p>计算负梯度：<span class="math display">\[-g_m(x_i)=-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\\\]</span></p></li><li><p>以负梯度<span class="math inline">\(-g_m(x_i)\)</span>为预测值，训练一个回归树<span class="math inline">\(T_m(x,\Theta)\)</span></p></li><li><p>更新<span class="math display">\[f_m(x)=f_{m-1}(x)+\rho T_m(x,\Theta)\]</span></p></li></ol></li><li>经过M次迭代后得到最终的提升树模型：<span class="math display">\[f_{M}(x)=\sum_{i=1}^{M}\rho T(x,\Theta)\]</span></li></ol><p>注意到这里对于每次迭代的决策树都加了一个系数<span class="math inline">\(\rho\)</span>，这个<span class="math inline">\(\rho\)</span>也被称为学习率（learning rate），为什么要加这个学习率呢？是因为如果将学习率设为1，就等于一下子学习全部的不足，会导致过拟合。据说在调参的时候，可以通过设置较小的学习率和较大的迭代次数以得到一个较优的模型，当然，这是以计算效率为代价的。<br /></p><h2 id="gbdt为什么使用负梯度而不是残差">GBDT为什么使用负梯度而不是残差？</h2><p>好了，现在可以说说为什么GBDT要使用负梯度而不是直接使用残差了，这也是我之前比较困惑的点，原因就是残差只是损失函数负梯度的一种特例，而采用负梯度更加通用，可以使用不同的损失函数。<br />为什么说残差只是一种特例呢？我们考虑下面这个损失函数：<br /><span class="math display">\[L(y,f(x))=\frac{(y-f(x))^2}{2}\\\]</span><br />这也就是平均平方误差MSE，然后他的负梯度就是：<br /><span class="math display">\[-[\frac{\partial L(y,f(x))}{\partial f(x)}]=y-f(x)\\\]</span><br />这不就是上面说的残差吗？也就是说，残差就等价于当损失函数采用MSE时的负梯度，而损失函数采用MSE其实会有一些缺点，比如对异常值敏感，所以在实际问题中我们有可能采用MAE或者更为折中的Huber损失函数以避免这一缺陷，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917120462-f7060d81-1511-4423-b70b-7d647b78711d.png#align=left&amp;display=inline&amp;height=368&amp;margin=%5Bobject%20Object%5D&amp;originHeight=490&amp;originWidth=728&amp;size=0&amp;status=done&amp;style=none&amp;width=546" /><br />并且呢，这些损失函数的负梯度其实也可以看作是残差的一个近似，如图：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1623917260219-846072de-d101-46ff-9674-9907085d1ff0.png#align=left&amp;display=inline&amp;height=286" alt="image.png" /><br />而且，由于决策树容易过拟合的特点，我们通常也会采用一些正则化的手段控制模型的复杂度来改进损失函数，如控制树的深度等<br /><span class="math display">\[obj=L(y,f(x))+\Omega(\Theta)\\\]</span><br />这里的<span class="math inline">\(\Omega(\Theta)\)</span>是正则项，用来控制模型的复杂度，使得模型在保持良好的性能的同时尽可能简单，以防止过拟合，这也是xgboost相对于GBDT的优化手段之一。<br />综上，由于负梯度可以应付各种各样千奇百怪的损失函数，所以GBDT用负梯度来拟合明显相比于残差是有着巨大的优势的。</p><h1 id="总结">总结</h1><p>本文主要介绍了GBDT的核心思想，先引入基于加法模型和前向分布算法的提升树模型，提升树模型每次迭代的拟合的是残差，然后介绍了GBDT，GBDT每次迭代拟合的是负梯度，通过拟合负梯度使得每次迭代后的损失函数值都能下降最快，最后说明了为什么负梯度与残差的关联以及为什么拟合负梯度更好。</p><h1 id="参考">参考</h1><ol type="1"><li>《The Elements of Statistical Learning Data Mining,Inference,and Prediction》</li><li><a href="https://mp.weixin.qq.com/s/9SrciKW-nJJMA2BbTs5CLg">GBDT算法原理以及代码实现</a></li><li><a href="https://blog.csdn.net/shine19930820/article/details/65633436">『机器学习笔记 』GBDT原理-Gradient Boosting Decision Tree</a></li><li><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting</a></li><li><a href="https://www.zhihu.com/question/63560633">gbdt的残差为什么用负梯度代替？</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;GBDT应该是我了解到的最能打的传统机器学习算法了，即使在当前各种基于深度学习模型泛滥的当下，依旧在一些中小型规模的数据集上有着非常亮眼的表现，如kaggle和天池上的一些比赛，高分的模型基本上都是使用的基于GBDT框架的xgboost、catboost、lightgbm。本文将介绍下这个最基础的GBDT算法，方便之后其他模型的学习，这次虽然花了很多时间看了不少资料，但是限于个人水平，只能结合自己的理解，尽力把一些最精华和比较难懂的地方稍微总结一下，有不恰当的地方还请指正！&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="GBDT" scheme="https://lukan797.github.io/tags/GBDT/"/>
    
    <category term="boosting" scheme="https://lukan797.github.io/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>Ridge和Lasso回归与代码实践</title>
    <link href="https://lukan797.github.io/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/"/>
    <id>https://lukan797.github.io/2021/05/23/Ridge%E5%92%8CLasso%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-05-22T16:00:00.000Z</published>
    <updated>2021-06-30T18:35:23.880Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。</p><h1 id="ridge-lasso回归">Ridge &amp; Lasso回归</h1><p>在传统的线性回归中，是使用最小二乘法来估计参数的，通过最小化残差平方和来估计参数的，这个在机器学习领域也被称为损失函数：<br /><span class="math display">\[LOSS_{ols}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2\\\]</span><br />这种最小二乘估计的方法被证明了具有最佳线性无偏估计（Best Linear Unbias Estimator, BLUE）的性质，所谓的最佳，就是方差最小，但这是在线性无偏估计的前提下，在有偏的情况下方差就不一定是最小了，设想一下，如果牺牲这个有偏的性质来使得方差变小呢，根据bias-variance trade-off，会不会有可能使得整体的预测误差进一步降低呢？<br />于是Ridge和Lasso的形式就被提出来了，通过牺牲传统ols回归中无偏的性质来使得方差降低，以寻求更低的预测误差，这两者的损失函数分别如下：<br /><span class="math display">\[LOSS_{Ridge}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}\beta_j^2\\\]</span><br /><span class="math display">\[LOSS_{Lasso}=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|\\\]</span><br />可以发现，这两个损失函数呢，就是在原来的ols的损失函数上加了一个系数惩罚项，因为我们求解时是让损失函数最小，加了后面这个惩罚项呢，会使得系数变小，这个<span class="math inline">\(\lambda\)</span>就用来控制惩罚的力度，如果为0的话就和传统的线性回归没有差异了，如果是无穷大的话，那么所有的回归系数都会被弄到0，最后的所有的预测结果就是样本的均值了，但在实践中，我们可以通过交叉验证的方式调节<span class="math inline">\(\lambda\)</span>的大小，选取最优的惩罚力度，就可以使得最终的预测误差达到最小。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621696524735-840f5da7-5fe4-4149-80a7-13ef9c3cbec6.png#align=left&amp;display=inline&amp;height=249&amp;id=jCBob" alt="image.png" /><br />Ridge和Lasso这种加惩罚项的方式叫做正则化（Regularization），在机器学习的应用很广，比如神经网络中就有应用。因此，Ridge也被称为<span class="math inline">\(L_2\)</span>正则化，后者被称为<span class="math inline">\(L_1\)</span>正则化。<br />虽然两者的加的惩罚项看起来差不多，其实是有着非常大的区别的，具体表现为Lasso可以使得系数压缩到0，而Ridge则不会有这种效果，把系数压缩到0的话就可以起到降维和变量选择的作用，因此Lasso在高维的数据中表现更好。<br />那么为啥会有这样的差别呢，首先我们来看他们的惩罚项的形式，一个用的是平方的形式，另一个用的是绝对值的形式，我们把之前的那个损失函数转化成一个优化问题：<br /><span class="math display">\[Ridge: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}\beta_j^2 \le s\\Lasso: \quad \min \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \quad s.t.\sum_{j=1}^{p}|\beta_j| \le s\\\]</span><br />假设只有两个系数，我们用几何的方式来表达这个优化问题，Ridge的约束条件是一个平方的形式，可行域就是一个圆，而Lasso的约束条件是绝对值的形式，可行域则是一个菱形，而目标函数在求解时，肯定是跟这个约束条件的可行域相切的，而Lasso由于他是一个菱形，那么他就更容易切到菱形的顶点，因此也会使得系数为0，而Ridge是一个圆，就不容易切到系数为0的地方，因此这就使得Lasso在压缩系数时会更倾向于压缩为0。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1621697160270-8a07af21-ed57-4ad0-b2ca-de6eed23bcf8.png#align=left&amp;display=inline&amp;height=240&amp;id=hG528" alt="image.png" /></p><h1 id="代码实践">代码实践</h1><p>使用sklearn自带的波斯顿房价数据集做个试验，分别跑一遍Ridge和Lasso回归，并且通过交叉验证来选取<span class="math inline">\(\lambda\)</span>，将之与线性回归进行对比，结果如下：</p><table><thead><tr class="header"><th></th><th>MSE</th></tr></thead><tbody><tr class="odd"><td>线性回归</td><td>21.8977</td></tr><tr class="even"><td>Ridge回归</td><td>21.7536</td></tr><tr class="odd"><td>Lasso回归</td><td>21.8752</td></tr></tbody></table><p>可以发现，两者的预测效果较线性回归都有一定提升，其中Lasso回归提升较小，这是因为数据集的原因，只有13个变量，并且每个变量都make sense，因此效果就一般了，在高维的数据集中Lasso从理论上 讲应该就会有较好的表现了。<br />具体代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV,LassoCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">y_pred_lr = lr.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;线性回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lr, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ridge=RidgeCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>) <span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line">y_pred_ridge = ridge.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Ridge回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_ridge, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">lasso=LassoCV(alphas=np.logspace(-<span class="number">5</span>,<span class="number">5</span>,<span class="number">11</span>),cv=<span class="number">5</span>)<span class="comment"># lambda选择10的-5次方到5次方，五折交叉选择</span></span><br><span class="line">lasso.fit(X_train, y_train)</span><br><span class="line">y_pred_lasso = lasso.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Lasso回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lasso, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h1 id="总结">总结</h1><p>本文对Ridge和Lasso回归做了一个总结，并通过一个简单数据集做了实践。在写的同时发现需要再去看和学习的东西很多，一个流程下来对于算法原理的理解更加透彻了，这对于搭建自己的知识体系是很有帮助的，希望以后能够坚持学完一个新的东西就写篇总结。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;线性回归作为一个非常经典的方法，被广泛应用于计量领域，用来解释变量对y的影响，但是在机器学习领域用纯粹的线性回归来做预测的好像就很少了，因为预测效果不怎么样，因此本文将对线性回归的两种改进方法做一个总结。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="回归算法" scheme="https://lukan797.github.io/tags/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>前向分步回归Forward Stagewise Regression原理及Python实现</title>
    <link href="https://lukan797.github.io/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/"/>
    <id>https://lukan797.github.io/2021/05/04/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E5%9B%9E%E5%BD%92Forward%20Stagewise%20Regression%E5%8E%9F%E7%90%86%E5%8F%8APython%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-05-03T16:00:00.000Z</published>
    <updated>2021-06-30T18:36:29.572Z</updated>
    
    <content type="html"><![CDATA[<p>最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。</p><h1 id="算法原理">算法原理</h1><p>这个算法的思想与boosting类似，每次迭代时都挑选出一个最优的变量来拟合残差，具体步骤如下：</p><ol type="1"><li>首先将截距项<span class="math inline">\(\beta _0\)</span>设置为<span class="math inline">\(\bar{y}\)</span>，所有的自变量系数<span class="math inline">\(\beta\)</span>都设为0，残差项设置为<span class="math inline">\(r=y-\bar y\)</span></li><li>挑选出与残差项最相关的自变量<span class="math inline">\(x_j\)</span></li><li>更新<span class="math inline">\(\beta _j\)</span>的值：，其中<span class="math inline">\(\delta_j=\epsilon \times \text{sign}[\langle x_j,r \rangle]\)</span>，这个<span class="math inline">\(\text{sign}[\langle x_j,r \rangle]\)</span>代表相关性的正负，<span class="math inline">\(\epsilon\)</span>代表步长。再更新下残差项的值：<span class="math inline">\(r=r-\delta_j x_j\)</span></li><li>重复步骤2，3，直到达到最大迭代次数或者所有的变量都与残差项无关。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620114719031-b1f1c1e8-155e-4258-a114-0d63a13b6a42.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=259&amp;id=uaa9a13b9" alt="image.png" /> 这个算法的优点在于与Lasso回归有着异曲同工之妙，通过选择合适的迭代次数和步长，可以使得部分变量的系数压缩为0，就可以起到变量选择和降低方差的作用，因此在高维数据的场景下会有较好的表现，再偷一张《The Elements of Statistical Learning》的变量系数路径图来说明这一点，左图的横轴为Lasso的L1范式，右图的横轴为前向分布回归的迭代次数，可以看到，变量系数的压缩路径大体上是一致的。 <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620118711730-c7178912-4b0e-447d-8355-2bdae92fcc77.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=330&amp;id=uff065e10" alt="image.png" /></li></ol><h1 id="python实现">Python实现</h1><p>用波斯顿房价的数据集来做个测试，将迭代次数设为2000的时候，mse要略小于线性回归： <img src="https://cdn.nlark.com/yuque/0/2021/png/764062/1620121931201-e7594c64-9878-47d3-a851-0285bf12f751.png#clientId=ucf785b6c-7490-4&amp;from=paste&amp;height=44&amp;id=j2U1Z" alt="image.png" /> 因为这个数据集只有13个变量，而且每个变量都很重要，所以前向分布回归的优势并没有很明显，不过通过调参效果还是可以比普通的线性回归好那么一点，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ForwardStagewise</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">0.01</span>, max_iter=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="comment"># 初始化两个参数，eps步长和max_iter迭代次数</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 将X，y转化为数组形式</span></span><br><span class="line">        y = np.asarray(y)</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>) <span class="comment"># 标准化</span></span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X = (X - X_mean) / X_std</span><br><span class="line">        self.y_mean = np.mean(y) <span class="comment"># 截距项，也就是y的平均</span></span><br><span class="line">        residual = y - self.y_mean <span class="comment"># 初始化残差项</span></span><br><span class="line">        x_num = np.shape(X)[<span class="number">1</span>] <span class="comment"># 变量数</span></span><br><span class="line">        self.beta = np.zeros((x_num)) <span class="comment"># 用来存储每一次系数更新的数组</span></span><br><span class="line">        self.betas = np.zeros((self.max_iter, x_num))  <span class="comment"># 用来存储每一迭代的系数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            c_hat = <span class="number">0</span></span><br><span class="line">            sign = <span class="number">0</span></span><br><span class="line">            best_feat = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x_num):</span><br><span class="line">                c_temp = X[:, j].T.dot(residual) <span class="comment"># 用来表示x与残差项的相关性</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(c_temp) &gt; c_hat:</span><br><span class="line">                    c_hat = <span class="built_in">abs</span>(c_temp)</span><br><span class="line">                    sign = np.sign(c_temp)</span><br><span class="line">                    best_feat = j</span><br><span class="line">            self.beta[best_feat] += sign * self.eps <span class="comment"># 更新系数</span></span><br><span class="line">            residual -= (self.eps * sign) * X[:, best_feat] <span class="comment"># 更新残差项</span></span><br><span class="line">            self.betas[i] = self.beta</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 预测</span></span><br><span class="line">        X = np.asarray(X) <span class="comment"># 先标准化</span></span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_test = (X - X_mean) / X_std</span><br><span class="line">        y_pred = X_test.dot(self.beta) + self.y_mean</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">    boston = load_boston() <span class="comment"># 导入波斯顿数据集</span></span><br><span class="line">    X = boston.data</span><br><span class="line">    y = boston.target</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1</span>)</span><br><span class="line">    fs = ForwardStagewise(eps=<span class="number">0.01</span>, max_iter=<span class="number">2000</span>)</span><br><span class="line">    fs.fit(X_train, y_train)</span><br><span class="line">    y_pred = fs.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;前向逐步回归MSE：<span class="subst">&#123;mean_squared_error(y_pred, y_test)&#125;</span>&#x27;</span>)</span><br><span class="line">    lg = LinearRegression()</span><br><span class="line">    lg.fit(X_train, y_train)</span><br><span class="line">    y_pred_lg = lg.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;线性回归回归MSE：<span class="subst">&#123;mean_squared_error(y_pred_lg, y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="总结">总结</h1><p>前向分布回归和Lasso回归本质上其实差不多，而且两者好像都是最小角回归（Least angle regression）的一个变种，具体可以参见ESL这本书（太难了我看不懂），这两张回归算法都能起到压缩系数和变量选择的作用，但是前向分布回归的计算效率比较差，所以Lasso似乎更为人熟知，不过前者为我们学习boosting相关算法提供了一个不错的切入点。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近偶然接触到一种回归算法，叫做前向分布回归（Forward Stagewise Regression），注意这不是那个向前逐步回归（Forward stepwise regression），stepwise和stagewise，还是有区别的，网上关于他的介绍非常少，中文社区基本就没怎么看到了，就顺手写一下吧，算法的思想来源于boosting，理解这个也有助于之后对各种树模型的boosting算法的学习。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习算法" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://lukan797.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="boosting" scheme="https://lukan797.github.io/tags/boosting/"/>
    
    <category term="回归算法" scheme="https://lukan797.github.io/tags/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（二）</title>
    <link href="https://lukan797.github.io/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://lukan797.github.io/2020/12/11/%E7%88%AC%E5%8F%96%E5%8E%A6%E9%97%A858%E5%90%8C%E5%9F%8E%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2020-12-10T16:00:00.000Z</published>
    <updated>2021-06-30T18:36:21.370Z</updated>
    
    <content type="html"><![CDATA[<p>书接上文：<a href="https://zhuanlan.zhihu.com/p/329185040">爬取厦门58同城二手房数据进行数据分析（一）</a> 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。</p><h1 id="数据分析及可视化">数据分析及可视化</h1><h2 id="数据预处理">数据预处理</h2><p>首先导包，由于<code>seaborn</code>画图不支持中文显示，因此还需要加几行代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 中文字体设置-黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line">sns.<span class="built_in">set</span>(font=<span class="string">&#x27;SimHei&#x27;</span>)  <span class="comment"># 解决Seaborn中文显示问题</span></span><br></pre></td></tr></table></figure> 读入数据，删除不需要分析的字段，以及删除存在缺失值的数据： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">data = data.drop(columns=[<span class="string">&#x27;Unnamed: 0&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;产权年限&#x27;</span>, <span class="string">&#x27;location2&#x27;</span>])</span><br><span class="line">data = data[data[<span class="string">&#x27;location1&#x27;</span>] != <span class="string">&#x27;厦门周边&#x27;</span>] <span class="comment"># 删除厦门周边的数据</span></span><br><span class="line">data = data.dropna()</span><br><span class="line">data</span><br></pre></td></tr></table></figure> 最终得到的数据像这样子，去除缺失值后一共749行*16列： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620860545-327f70ad-5f36-4bb7-bb43-d88b1d34f9b1.png" /></p><p>为了方便后续的工作，我们在将数据做一些简单的处理： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;室&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;厅&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;卫&#x27;</span>] = data[<span class="string">&#x27;房屋户型&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>]))</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>] = data[<span class="string">&#x27;均价&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;房本面积&#x27;</span>] = data[<span class="string">&#x27;房本面积&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;建筑年代&#x27;</span>] = data[<span class="string">&#x27;建筑年代&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[:-<span class="number">1</span>]))</span><br><span class="line">data[<span class="string">&#x27;总楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x[<span class="number">4</span>:-<span class="number">2</span>]))</span><br><span class="line">data[<span class="string">&#x27;所在楼层&#x27;</span>] = data[<span class="string">&#x27;所在楼层&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">&#x27;小区均价&#x27;</span>] = data[<span class="string">&#x27;小区均价&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;物业费&#x27;</span>] = data[<span class="string">&#x27;物业费&#x27;</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.split(<span class="string">&#x27;元&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">data[<span class="string">&#x27;绿化率&#x27;</span>] = data[<span class="string">&#x27;绿化率&#x27;</span>].apply(<span class="built_in">float</span>)</span><br><span class="line">data[<span class="string">&#x27;车位信息&#x27;</span>] = data[<span class="string">&#x27;车位信息&#x27;</span>].apply(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure></p><h2 id="单变量可视化">单变量可视化</h2><p><strong>价格分布</strong></p><p>厦门市的房价总体来说还是非常贵的，一平方米平均要四万多，一套下来得四百多万，买不起买不起 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">&#x27;均价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;均价&#x27;</span>].mean()</span><br><span class="line">sns.distplot(data[<span class="string">&#x27;总价&#x27;</span>])</span><br><span class="line">data[<span class="string">&#x27;总价&#x27;</span>].mean()</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607617912033-306d3e0a-325a-4ae7-bbff-1d91d715aa58.png" /></p><p><strong>房屋区域分布</strong></p><p>有将近一半的二手房都在岛内（思明和湖里)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;位置1&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607619988435-7f18f6b6-fa96-4fa9-b889-e04cedf4e344.png" /> <strong>房屋朝向分布</strong></p><p>选取前五种最受欢迎的房屋朝向，可以看出，有2/3的房子都是南北朝向：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋朝向&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620480819-0821a116-4e07-45f8-90cb-dcbe6a31ce44.png" /></p><p><strong>房屋户型分布</strong></p><p>同样选取前五种最受欢迎的房屋朝向，可以发现3室2厅2卫的户型最受欢迎：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;房屋户型&#x27;</span>].value_counts().head(<span class="number">5</span>).plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607620697989-2b958314-3805-4be7-8094-2e0237bc6f5d.png" /></p><p><strong>装修情况分布</strong></p><p>二手房基本上都是装修好了的，只有不到10%的是毛坯（为啥二手房还有毛坯的？）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;装修情况&#x27;</span>].value_counts().plot.pie(autopct=<span class="string">&#x27;%.2f%%&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621101526-c6d1f02d-3bc1-4008-a9f0-7dbc1dbba179.png" /></p><h2 id="多变量间关系及可视化">多变量间关系及可视化</h2><p><strong>地域与房价</strong></p><p>画出各个区域的每平方米价格的箱型图，果然，岛内的房价更可怕了，思明区接近6万/平米，更有12万/平米的天价房，湖里区也接近5万/平米，就算在同安和翔安这两个鸟不拉屎的地方一平米也要两万多了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(data=data, x=<span class="string">&#x27;位置1&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621274809-4304e87f-f8bf-4720-b874-7cf6192373e5.png" /> 地域与其他变量 将数据做一个聚合，取平均，可以发现，岛内的房子都比较老，大概都在2000年上下（因为没地方可建了吧)，而岛外基本上都在2010年左右，而且岛内的房子就只有十三四层，而岛外的房子有二十层左右，面积也相对来说比岛内的小一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=[<span class="string">&#x27;位置1&#x27;</span>])[<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;房本面积&#x27;</span>,<span class="string">&#x27;建筑年代&#x27;</span>,<span class="string">&#x27;总楼层&#x27;</span>].mean()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607621696738-bc35f9ce-7418-460b-a873-a5bf5001c76c.png" /></p><p><strong>建筑年代与房价</strong></p><p>看上去好像越老的房子越贵，上世纪末建的房子最值钱，而最近几年建的房子都不怎么值钱，当然这也跟我们之前分析的区域有关，因为最近建的房子基本都在岛外，所以当然不怎么值钱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;建筑年代&#x27;</span>)[<span class="string">&#x27;均价&#x27;</span>].mean().plot()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607623479793-fe610cab-0954-4669-afff-3824eabb0d12.png" /></p><p><strong>所在楼层与房价</strong></p><p>一般来说，大家都不太喜欢低楼层的房子，因为太吵了，当然太高也不行，这种关系，也反映在房价中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;所在楼层&#x27;</span>, y=<span class="string">&#x27;均价&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607622112702-1862764d-a020-4303-8938-244724f22518.png" /></p><p>再来看看厦门哪个小区的房子最贵吧，这里选取小区均价最高的15个小区：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(by=<span class="string">&#x27;小区名&#x27;</span>)[<span class="string">&#x27;小区均价&#x27;</span>].mean().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">15</span>).plot(kind=<span class="string">&#x27;barh&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607651743576-9563d729-5c92-4f5c-a10d-1d85574a67fe.png" /></p><h2 id="地理可视化">地理可视化</h2><p>前阵子刚好接触到百度地图的API，非常强大，就顺手做个地图可视化吧！ 首先需要去百度地图开发者官网（ <a href="https://link.jianshu.com/?t=http://lbsyun.baidu.com/">http://lbsyun.baidu.com/</a>）注册一个密钥，然后创建两个应用，一个是服务端的，用来使用Python获取小区坐标，一个是浏览器端的，用来通过修改html源代码创建热力图，具体实现可以参考这篇文章：<a href="https://blog.csdn.net/ebzxw/article/details/80265796">Python使用百度地图API实现地点信息转换及房价指数热力地图</a> 最后生成的效果如下图所示，可以看出，厦门市最贵的地段基本上就在火车站周围那一块： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607655308085-a39295fa-8bad-495b-80b8-8fedada0be79.png" /> <em>ps: 这里可视化原本想使用 folium，但是存在 folium包存在两个问题，一个是热力图存在 bug，没有渐变效果，另外一个是因为我坐标采用的是百度的坐标，百度的坐标是经过加密的，用在 folium上会存在坐标偏移的情况，故弃用</em></p><h1 id="预测模型">预测模型</h1><p>以每平方米价格为因变量，其余变量为自变量，并将分类变量使用 LabelEncoder 编码，将测试集与训练集以2：8的比例分割： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=data.drop(columns=[<span class="string">&#x27;小区均价&#x27;</span>,<span class="string">&#x27;总价&#x27;</span>,<span class="string">&#x27;均价&#x27;</span>,<span class="string">&#x27;房屋户型&#x27;</span>,<span class="string">&#x27;小区名&#x27;</span>])</span><br><span class="line">y=data[<span class="string">&#x27;均价&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">&#x27;位置1&#x27;</span>,<span class="string">&#x27;房屋朝向&#x27;</span>,<span class="string">&#x27;一手房源&#x27;</span>,<span class="string">&#x27;所在楼层&#x27;</span>,<span class="string">&#x27;装修情况&#x27;</span>]:</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    x[col]=le.fit_transform(x[col])</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure> 由于自变量中存在很多分类变量，因此考虑使用树模型进行预测，由于树模型本身就有着特征选择的功能，因此，不做特征选择，直接跑模型</p><p><strong>决策树</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeRegressor()</span><br><span class="line">dt.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;决策树绝对值误差：<span class="subst">&#123;mean_absolute_error(dt.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>随机森林</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">2000</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">rf.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;随机森林绝对值误差：<span class="subst">&#123;mean_absolute_error(rf.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>Catboost</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cb=CatBoostRegressor()</span><br><span class="line">cb.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Catboost绝对值误差：<span class="subst">&#123;mean_absolute_error(cb.predict(x_test),y_test)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>结果对比</strong></p><table><thead><tr class="header"><th></th><th>决策树</th><th>随机森林</th><th>catboost</th></tr></thead><tbody><tr class="odd"><td>绝对值误差</td><td>2885.81</td><td>2286.76</td><td>2347.04</td></tr></tbody></table><p><strong>特征重要性</strong></p><p>用随机森林输出特征重要性看看： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fi = pd.DataFrame(</span><br><span class="line">    &#123;<span class="string">&#x27;x&#x27;</span>: x.columns, <span class="string">&#x27;feature_importance&#x27;</span>: rf.feature_importances_&#125;)</span><br><span class="line">fi = fi.sort_values(by=<span class="string">&#x27;feature_importance&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;feature_importance&#x27;</span>, y=<span class="string">&#x27;x&#x27;</span>, data=fi)</span><br></pre></td></tr></table></figure> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607667452772-a2ae6636-935b-411c-b9fa-864472bf30a5.png" /> 啊这，小区均价一枝独秀，解释力度太大了，把其他特征的信息都全部吃下去了，为了更好的解释其他特征与每平方米价格的关系，我们考虑把它排除在外，再输出一次特征重要性： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607668343722-a06abc5d-4305-4e67-a2ea-ee97a3a68a0b.png" /> 这次就好点了，预测的绝对值误差虽然变成了四千，预测效果变差了，但是解释力度提高了，对房价影响最大的前五个特征为：位置1（区域）、物业费（反映小区的质量）、容积率（反映小区的居住的舒适度)、总楼层、建筑年代，而房屋朝向、所在楼层和装修情况这些特征居然没有想象中的那么重要，看来在厦门，<strong>决定一套房子价格的是房子所在小区的属性，而不是你这套房子本身的属性</strong>。</p><h1 id="小结">小结</h1><p>好了，又一篇文章水完了，这篇文章还是花了我不少时间的，尤其是在研究怎么画图上，看来可视化这方面还是得继续学习一下啊！这个月总体来说还是比较忙的，希望能够坚持每周写一篇吧，下周可能会开始写一些算法的学习笔记。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;书接上文：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/329185040&quot;&gt;爬取厦门58同城二手房数据进行数据分析（一）&lt;/a&gt; 这一篇主要对上一篇文章爬取下来的数据进行一些探索性分析和可视化，并且建立一个简单的预测模型进行房价预测。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="数据分析" scheme="https://lukan797.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>爬取58同城厦门二手房数据进行数据分析（一）</title>
    <link href="https://lukan797.github.io/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://lukan797.github.io/2020/12/04/%E7%88%AC%E5%8F%9658%E5%90%8C%E5%9F%8E%E5%8E%A6%E9%97%A8%E4%BA%8C%E6%89%8B%E6%88%BF%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2020-12-03T16:00:00.000Z</published>
    <updated>2021-06-30T18:35:44.731Z</updated>
    
    <content type="html"><![CDATA[<p>最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。</p><h1 id="网页分析">网页分析</h1><p>首先，点进去首页，是一行行的信息，一页有120条： <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010708512-8e3cc62c-fd97-4df6-a329-014b77fc3cf8.png#align=left&amp;display=inline&amp;height=419" alt="image.png" /> 点进去链接后是详情页的信息，主要分为两部分信息：房子属性和小区信息 <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011106024-d85dbb2a-08eb-47dd-b954-c89b1c253faa.png#align=left&amp;display=inline&amp;height=388" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010951590-61f97fe9-5be9-40f0-9eb2-47ef2046d244.png#align=left&amp;display=inline&amp;height=251" alt="image.png" /> <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607010981428-3ead9cef-a4d1-4950-a749-ea116896ad27.png#align=left&amp;display=inline&amp;height=310" alt="image.png" /> 这里需要注意的是，虽然这些信息都可以直接用xpath获取，但是详情页里面的价格居然用了字体加密！！！ <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1607011288804-77aaeae6-90d6-4295-8486-30459b7c8642.png" alt="image.png" /> 虽然网上有很多这种破解办法，但其实根本不需要那么复杂，回到我们的首页，那边不也有价格嘛，而且，这个价格是没有加密的哦！（58同城程序员故意留的后门？？？)，所以我们爬虫思路大概就分三步走：</p><ol type="1"><li>先爬取首页一行行的信息，用xpath获取标题，价格，链接，一条信息用一个字典存储，然后将这条字典并存储在一个列表里面</li><li>第一步爬完后，然后开始遍历我们的列表，进入详情页的链接，把详情页的信息也用xpath一个个扒到我们的字典里，最终返回一个具有完整信息的列表</li><li>使用pandas的Dataframe函数就可以将第二部返回的列表直接转化为一个Dataframe，就能直接导出csv了。 原以为这样就能直接收工了，没想到小看58同城了，还没爬几条就给你来了个人机验证，访问再频繁点就需要登录了，然后就啥信息都爬不到了，因为自己之前爬的都是比较简单的网站，从来就没出现过这个问题，看来还是自己naive了啊，所以前几天也花了一整天的时间来解决这个问题，接下来就看看如何解决这个问题吧！</li></ol><h1 id="ip访问频繁问题">ip访问频繁问题</h1><p>一般这种问题的解决方案有两种：</p><ol type="1"><li>使用代理ip访问。</li><li>设置程序休眠时间和断点续传机制。</li></ol><p>第一种解决方案的话，一般来说是需要自己花钱去买付费ip的，因为大多数免费ip都被人用烂了，考虑到穷和需要花时间去学习怎么构建代理池的问题，于是第一种方案就被我放弃了。 第二种方案算是一种土办法吧，只要我访问足够慢，跟正常用户一样，那么它就不会封我ip（所以这次我爬500条数据都用了两个小时），但是，为了避免还是弹出人机验证的问题，我们需要一个断点续传机制，即当我知道自己的ip被封了的时候，就马上停止访问，并且把已经爬取的数据全部保存下来，然后手动用浏览器去网站上点一下验证码，然后再从之前停止的地方继续开始，这就需要我们给每条信息编个号，当检测到被封ip的时候输出这个编号，手动验证之后继续从这个编号处开始。 说了这么多都是废话，接下来直接上代码吧！ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="comment"># 输入链接，返回解析后的html</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36 Edg/86.0.622.63&quot;</span>&#125;</span><br><span class="line">    response = requests.get(url=url, headers=headers)</span><br><span class="line">    content = response.content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    html = etree.HTML(content)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_base_info</span>(<span class="params">page_url</span>):</span></span><br><span class="line">    <span class="comment"># 获取基础信息</span></span><br><span class="line">    html = parse_url(page_url)</span><br><span class="line">    titles = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/text()&#x27;</span>)  <span class="comment"># 标题</span></span><br><span class="line">    urls = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;list-info&quot;]/h2[@class=&quot;title&quot;]/a/@href&#x27;</span>)  <span class="comment"># 链接</span></span><br><span class="line">    total_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;sum&quot;]/b/text()&#x27;</span>)  <span class="comment"># 总价</span></span><br><span class="line">    unit_prices = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//div[@class=&quot;price&quot;]/p[@class=&quot;unit&quot;]/text()&#x27;</span>)  <span class="comment"># 均价</span></span><br><span class="line">    base_infos = []  <span class="comment"># 使用一个列表存储所有信息</span></span><br><span class="line">    <span class="keyword">for</span> title, url, total_price, unit_price <span class="keyword">in</span> <span class="built_in">zip</span>(titles, urls, total_prices, unit_prices):</span><br><span class="line">        <span class="comment"># 将信息写入一个字典中</span></span><br><span class="line">        info = &#123;&#125;</span><br><span class="line">        info[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">        <span class="keyword">if</span> url[<span class="number">0</span>:<span class="number">5</span>] != <span class="string">&#x27;https&#x27;</span>:  <span class="comment"># 有的链接不是https开头的，手动加上</span></span><br><span class="line">            url = <span class="string">&#x27;https:&#x27;</span>+url</span><br><span class="line">        info[<span class="string">&#x27;url&#x27;</span>] = url.split(<span class="string">&#x27;?&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># 删掉链接后面跟的cookie参数什么乱七八糟的东西</span></span><br><span class="line">        info[<span class="string">&#x27;total_price&#x27;</span>] = total_price</span><br><span class="line">        info[<span class="string">&#x27;unit_price&#x27;</span>] = unit_price</span><br><span class="line">        base_infos.append(info)</span><br><span class="line">    <span class="keyword">return</span> base_infos</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_extra_info</span>(<span class="params">info</span>):</span></span><br><span class="line">    <span class="comment"># 进入详情页获取更多信息</span></span><br><span class="line">    info_url = info[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">    html = parse_url(info_url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[1]/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location1&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li[2]/span[2]/a[2]/text()&#x27;</span>)[<span class="number">0</span>].replace(<span class="string">&#x27;－&#x27;</span>, <span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;location2&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 获取详情页表格中的信息</span></span><br><span class="line">    info_keys = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;mr_25 c_999&quot;]/text()&#x27;</span>)[<span class="number">1</span>:]</span><br><span class="line">    info_values = html.xpath(</span><br><span class="line">        <span class="string">&#x27;//*[@id=&quot;generalSituation&quot;]//span[@class=&quot;c_000&quot;]&#x27;</span>)</span><br><span class="line">    info_values = [v.text <span class="keyword">for</span> v <span class="keyword">in</span> info_values]</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> <span class="built_in">zip</span>(info_keys, info_values):</span><br><span class="line">        info[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取小区及周边信息</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/h3/a/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_name&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[1]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;xiaoqu_price&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[3]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;property_costs&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[4]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;area_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[5]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;green_ratio&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = html.xpath(</span><br><span class="line">            <span class="string">&#x27;//*[@id=&quot;xiaoWrap&quot;]/div/div[2]/ul/li[6]/span[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        info[<span class="string">&#x27;车位信息&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> info</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://xm.58.com/ershoufang/pn&#x27;</span></span><br><span class="line">infos = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))  <span class="comment"># 设置休息时间应对反爬</span></span><br><span class="line">    page_url = base_url+<span class="built_in">str</span>(i)</span><br><span class="line">    results = get_base_info(page_url)</span><br><span class="line">    infos.extend(results)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;爬取页面<span class="subst">&#123;i&#125;</span>的基础信息成功！&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(infos)):</span><br><span class="line">    time.sleep(random.randint(<span class="number">10</span>, <span class="number">20</span>))</span><br><span class="line">    infos[i] = get_extra_info(infos[i])</span><br><span class="line">    <span class="keyword">if</span> infos[i][<span class="string">&#x27;location1&#x27;</span>] == <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> infos[i][<span class="string">&#x27;xiaoqu_name&#x27;</span>] == <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 如果这两个值都为空值，说明开始人机验证了</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;爬取第<span class="subst">&#123;i&#125;</span>条信息失败,请进行人机验证! &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(infos[i][<span class="string">&#x27;url&#x27;</span>])</span><br><span class="line">        <span class="comment"># 及时保存数据</span></span><br><span class="line">        data = pd.DataFrame(infos)</span><br><span class="line">        data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取第&#123;&#125;条信息成功：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, infos[i][<span class="string">&#x27;title&#x27;</span>]))</span><br><span class="line">        </span><br><span class="line">data = pd.DataFrame(infos)</span><br><span class="line">data.to_csv(<span class="string">&#x27;data.csv&#x27;</span>)  <span class="comment"># 导出到csv文件</span></span><br></pre></td></tr></table></figure></p><h1 id="总结">总结</h1><p>这次爬虫主要学了三个东西吧：</p><ol type="1"><li>字典是个很有用的数据类型，不仅存储效率高，而且将多个字典放在列表里可以直接转化为pandas的Dataframe，还就能直接导出，不需要读写文件那么麻烦。</li><li>xpath的异常处理机制很重要。有些信息可能会在某个页面上神奇的消失了，所以最好事先做好异常处理机制，爬不到就置为空值，不然程序一报错之前爬的数据全没掉了。</li><li>断点续传机制也很重要。天知道网站会在什么时候给你跳出验证码，所以最好有断点续传机制，防止你要从头来过。</li></ol><p>好了，第一篇技术性的文章就这样水成了，第二篇过几天有空写。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近发现自己的输出能力实在太菜了，写东西经常要憋很久才憋出来，而且写的东西逻辑也不太清楚，所以就想着多写点东西来提升自己的写作能力，同时也加深下自己对于一些问题的理解吧，另外一个原因就是发现自己好多东西都是学完就忘，必须找个平台记录下自己学过的东西！刚好最近有个课程作业，要求爬取58同城上面的厦门二手房信息进行数据分析，就拿这个来小试牛刀吧，预计写两篇，第一篇是爬虫，第二篇是数据分析。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习实践" scheme="https://lukan797.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="爬虫" scheme="https://lukan797.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>2020美赛参赛经验</title>
    <link href="https://lukan797.github.io/2020/05/03/2020%E7%BE%8E%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    <id>https://lukan797.github.io/2020/05/03/2020%E7%BE%8E%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/</id>
    <published>2020-05-02T16:00:00.000Z</published>
    <updated>2021-06-30T18:34:59.168Z</updated>
    
    <content type="html"><![CDATA[<p>前几天美赛出成绩了，有幸能够获得M奖，虽然说在知乎这种人均F奖、M奖的环境下看似乎算不了什么，但是对于我个人而言，和两位队友四天四夜投入所有时间精力来解决一个陌生的问题在我的大学生活中也算是比较珍贵的一段经历吧！ 鉴于参赛前参考了很多前辈们的参赛经验帖，受益颇深，因此，我也在此将自己本次参赛的一些心得分享给之后参赛的同学们，但凡能够从某个角度上帮助到某些人，那么我写的这些东西也都是值得的了。 文章提到的资料和我本次参赛的作品均已上传到GitHub，好的正文开始：</p><h1 id="参赛经验">参赛经验</h1><h2 id="关于组队">关于组队</h2><p>因为一开始我的想法就是奔着C题大数据去的，因此就直接找了两个经济统计专业的好朋友组队，我们三人都有参加过之前国赛的经历，而且我知道他们两个都很靠谱，所以在组队上我并没有遇到什么太大的问题，我想这也是我们能够拿奖很重要的原因之一吧，但是我知道这次有很多队伍因为组队不慎的原因造成了两人建模甚至一人建模的尴尬处境，所以组队还是得慎重，我的建议就是：优先找自己熟悉的、靠谱的人，而不是一味追求队友要跨专业背景（比如计算机+数学+文科），像我们之前的国赛，我这次的两个队友当时就是为了追求专业背景多样性都遇到了坑队友，队友A找了个数学妹子，结果数学妹子对于建模并没有什么优势，而且当时中秋节还跑出去和她的男票过节了...队友B找了个计算机同学，结果发现好像他编程能力也不怎么样，而且比赛的时候一直在看NBA...所以我觉得组队的一个原则就是首先找的队友要靠谱，就算能力目前不是很强，但是起码会认真对待而且不会半途甩锅走人的那种，在此基础上再去追求队友优势的互补。</p><h2 id="赛前准备">赛前准备</h2><p>他山之石可攻玉，所以我建议在赛前尽可能多的看别人分享的参赛经验贴，然后从中总结出适用于自己的方法和套路，阅读往年的O奖论文也挺重要的（虽然我一篇也没完整的看下去，不过也还是从中学到了一点套路），可以学习别人的行文逻辑等。 队友的分工最好赛前就确定好，可以针对性的去做准备，而且最好是一人负责学习两块这样子，个人觉得建模编程和写作不应该完全割裂开来，比如在这次比赛中，我就是负责编程和建模，然后其他两个队友负责建模和写作，以建模为核心，一人负责两块，这样会减轻沟通成本，就不至于模型都建好了，但是写作的人不理解无法将模型的思想表达到文章中，或者编程的人根本不理解这个模型导致写不出来程序。接下来我会根据这三个模块分享下自己赛前的准备经验。</p><p><strong>关于编程</strong></p><p>有的人可能会纠结用哪种编程语言，比如用MATLAB还是Python，如果你还没学的话，我推荐学Python吧，由于有着各种第三方库的支持，Python在处理尤其是像C题这种大数据类型的题目简直有着得天独厚的优势，很多模型或者算法都能直接调包解决，比如这次C题的情感分析，不用Python我还真不知道要怎么做。 这里推荐两本书籍，如果你想选C题的话或者以后想用Python做数据分析的话，可以考虑好好阅读一下，寒假时我就粗略看了下，比赛时的我的代码编写效率提高了很多很多：</p><ol type="1"><li><a href="https://seancheney.gitbook.io/python-for-data-analysis-2nd/">《利用Python进行数据分析第二版》</a>：主要讲的是数据处理，pandas，numpy，matplotlib的使用方法，较为详细</li><li>《Python数据科学手册》：一部分和上面那本内容重叠，后面也有讲一些机器学习模型的介绍和使用</li></ol><p>由于好像目前建模主流的编程语言都是MATLAB，所以网上能找到很多大佬或者培训机构整理的MATLAB实现算法代码，但是Python好像还很少有人做这样的工作，这里推荐一篇自己赛前收藏的文章，作者整理的还是比较详细的，覆盖了大部分的机器学习模型和启发式算法：<a href="https://zhuanlan.zhihu.com/p/105605953">数学建模部分算法整理</a></p><p><strong>关于建模</strong></p><p>因为在去年暑假的时候参加过学校组织的国赛培训，当初就了解了不少模型，因此在这美赛中我并没有投入太多时间来学习模型。在模型这一块，如果时间足够的话可以去系统的学习下以下的经典的模型，基本上是能够覆盖大部分题型的要求，如果没时间的话，其实大概看一下就行了，知道这个模型的主要思想是什么，能用来解决什么问题，到时候比赛的时候就只需要现学现卖就行了：  <img src="https://cdn.nlark.com/yuque/0/2020/png/764062/1580662526815-7abbd040-dda3-4a9a-928f-197227a4c8c0.png" /> 这里也推荐两本书籍（虽然我都没怎么看，但看了还是很有好处的）：</p><ol type="1"><li>《数学建模算法与应用》--司守奎</li><li>《数学模型》--姜启源</li></ol><p><strong>关于写作</strong></p><p>写作工具推荐使用latex，当然用word也不是不可以，我看往年的很多O奖文章也都是用word写出来的，使用latex的好处在于能够免去排版的烦恼，如果word的话还得去慢慢调一些字体字号行距页眉页脚等。而且其实latex上手也不是很难，但是需要找到系统的入门教程，比如《Ishort》，该教程可以在<a href="https://www.latexstudio.net/">LaTeX工作室</a>找到，他家也有提供各种latex模板，我的美赛和国赛的模板都使用他们的，相对于很多网上随便搜索找到的模板制作要好很多，这里也要说一下，如果使用latex 的话一定要用专门的模板，不然的话还不如用word。找到了模板和入门教程，差不多折腾一个晚上也就能上手latex写作了。这里推荐几个latex在线的网站，一定程度上能够提高写作的效率：</p><ol type="1"><li><a href="https://cn.overleaf.com/">Overleaf, 在线LaTeX编辑器</a>：可以在线协同写作，多人编辑需要花钱，不想花钱可以考虑一周试用，原本想用这个的，但是队友没有科学上网条件访问会很慢，遂放弃。</li><li><a href="http://www.tablesgenerator.com/#">http://www.tablesgenerator.com/#</a>：在线生成表格代码，因为latex弄表格实在太麻烦了，所以我都是现在Excel里面打好表格，然后复制到这个网站里面生成代码，再粘贴到latex编辑器里面。</li><li><a href="http://latex.codecogs.com/eqneditor/editor.php">http://latex.codecogs.com/eqneditor/editor.php</a>：在线生成公式代码，没怎么用过，打公式不熟的话用这个应该可以提高效率</li></ol><p>美赛需要英文写作的，专门负责写作的同学不妨看下这本美赛官方出品的《正确写作美国大学生数学建模竞赛论文》，当然我的队友写作都是直接用机翻，写一句中文然后翻一句英文，写出来非常的Chinglish哈哈哈。</p><h2 id="比赛时">比赛时</h2><p>有效的查找资料，无疑是打开思路最重要的一环，像上次的国赛的出租车问题，我就在IEEE上找到一篇文献，为我们那时候提供了很重要的思路，这次的美赛也是，虽然并没有找到什么有价值的文献，于是我就往kaggle上面找，结果还真发现了很多做电商评论挖掘的，于是带着惊喜认真的看了几个大佬开源的notebook，从中也收获了不少。这里推荐自己查找资料的一些方法：</p><ol type="1"><li>谷歌学术+sci-hub：科研狗必备，选择合适的关键词检索往往能够找到不错的文章，就是英文文档读起来比较费劲，可以用谷歌的文档翻译或者更为专业的翻译狗先翻译成中文粗略看一遍如果觉得可以再看原文</li><li>知网上名牌大学发表的期刊论文或者硕博士论文：虽然知网上很多期刊论文都很水，但是这些名牌大学发表的论文硕博士论文质量还是挺有保障的</li><li>kaggle或者天池等数据竞赛平台：针对大数据题目有奇效，我去年国赛的数据和这次美赛的思路就是在这里找到的</li></ol><p>这次比赛整体来说我和队友还是相处的很愉快的，没有太多冲突，时不时还能互相吹下牛逼，但是在要提交论文的那天晚上，我却因为一个标点符号加不加空格的问题和一个队友吵起来了。。。我们都知道英文每写完一句标点符号后面是要加一个空格的，但是我写作的队友很多地方都没加，不过用的是机翻，从翻译复制过来的那部分就会自己加，于是我就提出了要把所有标点都加上空格，但那个队友嫌麻烦，觉得这种细节的问题并不重要，而且也比较晚了，我们当时就想收工的了，我对于细节要求比较高，所以当时就和那个队友吵起来了，当时情绪都比较激动，所以我就做了让步干脆不加了直接交上去，后面发现其实问题也不是很大嘛。所以我建议比赛时要是和队友发生冲突，先思考下冲突的点是不是问题的本质，如果只是细节问题的话，我就觉得没必要吵了，因为对整体影响不大，没必要因此进行不必要的争吵浪费时间并且破坏大家的心情。</p><h1 id="c题思路">C题思路</h1><p>比赛已经过去快两个月了，我现在对当时的思路印象也不是很深了，而且这个对于后面的人参考意义也不是很大，但还是简单写一下吧！</p><p><strong>数据的预处理</strong></p><p>仔细看下这些数据，其实是存在很多垃圾数据的，比如奶嘴里面就有很多行商品其实根本不是奶嘴，可能是因为爬虫的原因吧，所以第一步是把那些垃圾数据剔除掉，这里我采取的方法比较简单粗暴，就只是对商品名做了一个判断，比如对于microwave，我会判断他的商品标题里面是不是含有microwave，如果没有microwave这个词，那就给这行数据删掉，这样至少能剔除大部分的垃圾数据。 然后就是评论数据的处理，很关键的一点就是怎么将这些评论数据的情感值量化，一开始我觉得这个很难，因为没做过这种，甚至当时都有点想放弃C题了，不过后面发现其实也很简单，用Python的NLTK包里面就有训练好的情感量化模型，剔除禁用词后直接调用即可，然后就可以将每一行的评论数据都转化成一个情感值，还蛮准的其实，但是在上手NLTK时踩了几个坑（一个是语料库的下载，不知道为什么有的调用代码会下载失败，所以最后我索性直接把的6G多的全部语料库手动下载下来在导进去），所以这里花了挺多时间的。</p><p><strong>模型构建</strong></p><p>这次我们用到的模型其实都比较简单，题目有一个是需要从评论数据中挖掘出有价值的信息，一开始我们是想用LDA主题建模的，但是发现效果很差，分出来的那几个主题很难找到一个逻辑，遂放弃。后来就直接用了一个简单的逻辑回归模型，将评论分为两类，好的和差的，然后将那列评论数据用Tf-Idf算法向量化，跑个回归，根据逻辑回归的得出的系数大小和显著性就可以筛选出每一个商品最正面的词和最负面的词，这些词就可以反映顾客最关注的产品的特性，然后一通bb即可。 再后面题目有要求构建一个根据时间的评价模型，这里队友想出了一个比较“随意”的模型，把那几列变量用某种方式相加相乘组合得出了一个评分模型，然后按照月份画个图，看看这些商品的评分有没有存在季节性，记得还有一个要求是看这个商品先前的评价是否会影响之后的评价，这个我们就简单测试了一下这个评分是否存在自相关，结论是没有，即不会影响。然后最后就是一通bb。</p><h1 id="总结">总结</h1><p>这次的美赛对于我来说还是比较爽的一段经历，和两位很靠谱的队友在四天之内投入所有时间精力解决了一个自己陌生的问题，说实话也挺累的，不过还是从中收获了很多东西，我之后应该不会再参加什么建模比赛了，所以这篇文章就献给那些之后参赛的朋友，希望能够对你们有所帮助，同时，也用来记录下自己这段比较珍贵的经历。 第一次写这种经验贴，如有不足，还请指正，也欢迎私信和我交流！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前几天美赛出成绩了，有幸能够获得M奖，虽然说在知乎这种人均F奖、M奖的环境下看似乎算不了什么，但是对于我个人而言，和两位队友四天四夜投入所有时间精力来解决一个陌生的问题在我的大学生活中也算是比较珍贵的一段经历吧！ 鉴于参赛前参考了很多前辈们的参赛经验帖，受益颇深，因此，我也在此将自己本次参赛的一些心得分享给之后参赛的同学们，但凡能够从某个角度上帮助到某些人，那么我写的这些东西也都是值得的了。 文章提到的资料和我本次参赛的作品均已上传到GitHub，好的正文开始：&lt;/p&gt;</summary>
    
    
    
    <category term="经验" scheme="https://lukan797.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
    <category term="数学建模" scheme="https://lukan797.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="美赛" scheme="https://lukan797.github.io/tags/%E7%BE%8E%E8%B5%9B/"/>
    
  </entry>
  
</feed>
